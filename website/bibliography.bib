
@phdthesis{kohli_identifying_2015,
	title = {Identifying and classifying slum areas using remote sensing},
	url = {https://www.researchgate.net/profile/Divyani-Kohli/publication/305790722_Identifying_and_classifying_slum_areas_using_remote_sensing/links/57a1f38908aeef8f311da0c9/Identifying-and-classifying-slum-areas-using-remote-sensing.pdf?origin=publication_detail},
	author = {Kohli, Divyani},
	month = nov,
	year = {2015},
	keywords = {donnees satellites, slum detection},
}

@article{chen_simple_nodate,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
	language = {en},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	keywords = {donnees satellites, slum detection},
}

@article{lv_remote_2021,
	title = {Remote {Sensing} {Data} {Augmentation} {Through} {Adversarial} {Training}},
	volume = {14},
	issn = {2151-1535},
	doi = {10.1109/JSTARS.2021.3110842},
	abstract = {The lack of remote sensing images and poor quality limit the performance improvement of follow-up research such as remote sensing interpretation. In this article, a generative adversarial network (GAN) is proposed for data augmentation of remote sensing images abstracted from Jiangxi and Anhui Provinces in China, i.e., deeply supervised GAN (D-sGAN). D-sGAN can generate high-quality images that are rich in changes, greatly shorten the generation time, and provide data support for applications such as semantic interpretation of remote sensing images. First, to modulate the layer activations, a downsampling scheme is designed based on the segmentation map. Then, the architecture of the generator is Unet++ with the proposed downsampling module. Next, the generator of this net is deeply supervised by the discriminator using deep convolutional neural network. This article further proved that the proposed downsampling module and the dense connection characteristics of UNet++ are significantly beneficial to the retention of semantic information of remote sensing images. Numerical results demonstrated that the images generated by D-sGAN could be used to improve accuracy of the segmentation network, with the faster generation speed compared to the CoGAN, SimGAN, and CycleGAN models. Furthermore, the remote sensing data generated by the model helped the interpretation network to increase the accuracy by 9\%, meeting actual generation requirements.},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Lv, Ning and Ma, Hongxiang and Chen, Chen and Pei, Qingqi and Zhou, Yang and Xiao, Fenglin and Li, Ji},
	year = {2021},
	note = {Conference Name: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	keywords = {donnees satellites, Data augmentation, deep supervision, downsampling, GAN, Generative adversarial networks, Generators, Image synthesis, Remote sensing, Semantics, Task analysis, Training},
	pages = {9318--9333},
}

@misc{lam_xview_2018,
	title = {{xView}: {Objects} in {Context} in {Overhead} {Imagery}},
	shorttitle = {{xView}},
	url = {http://arxiv.org/abs/1802.07856},
	doi = {10.48550/arXiv.1802.07856},
	abstract = {We introduce a new large-scale dataset for the advancement of object detection techniques and overhead object detection research. This satellite imagery dataset enables research progress pertaining to four key computer vision frontiers. We utilize a novel process for geospatial category detection and bounding box annotation with three stages of quality control. Our data is collected from WorldView-3 satellites at 0.3m ground sample distance, providing higher resolution imagery than most public satellite imagery datasets. We compare xView to other object detection datasets in both natural and overhead imagery domains and then provide a baseline analysis using the Single Shot MultiBox Detector. xView is one of the largest and most diverse publicly available object-detection datasets to date, with over 1 million objects across 60 classes in over 1,400 km{\textasciicircum}2 of imagery.},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Lam, Darius and Kuzma, Richard and McGee, Kevin and Dooley, Samuel and Laielli, Michael and Klaric, Matthew and Bulatov, Yaroslav and McCord, Brendan},
	month = feb,
	year = {2018},
	note = {arXiv:1802.07856 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Initial submission},
}

@misc{uzkent_learning_2020,
	title = {Learning {When} and {Where} to {Zoom} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2003.00425},
	doi = {10.48550/arXiv.2003.00425},
	abstract = {While high resolution images contain semantically more useful information than their lower resolution counterparts, processing them is computationally more expensive, and in some applications, e.g. remote sensing, they can be much more expensive to acquire. For these reasons, it is desirable to develop an automatic method to selectively use high resolution data when necessary while maintaining accuracy and reducing acquisition/run-time cost. In this direction, we propose PatchDrop a reinforcement learning approach to dynamically identify when and where to use/acquire high resolution data conditioned on the paired, cheap, low resolution images. We conduct experiments on CIFAR10, CIFAR100, ImageNet and fMoW datasets where we use significantly less high resolution data while maintaining similar accuracy to models which use full high resolution images.},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Uzkent, Burak and Ermon, Stefano},
	month = apr,
	year = {2020},
	note = {arXiv:2003.00425 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in CVPR 2020 as an Oral Presentation. The code can be found at https://github.com/ermongroup/PatchDrop},
}

@inproceedings{al_mansoori_effective_2021,
	title = {Effective {Airplane} {Detection} in {High} {Resolution} {Satellite} {Images} using {YOLOv3} {Model}},
	doi = {10.1109/ICSPIS53734.2021.9652416},
	abstract = {Airplane detection is an artificial intelligence problem that can be solved either by the classical machine learning methods or by the cutting-edge deep learning processes. Generally, object detection is a highly challenging task as it requires recognizing the desired objects in the image in addition to localizing their positions by drawing a rectangular box with a confidence percentage. To obtain a high level of detection accuracy, the developed models are to be trained with hundreds or even thousands of satellite images; however, these images are limited, with low spatial resolution and high expenses. In recent days, deep learning algorithms have proved to excel in numerous object detection tasks due to the availability of huge amount of data along with the continuous advances in computers' power technology. This study investigates the problem of airplane detection from satellite imagery by proposing a simple, but an effective tool to achieve excellent detection rates. Our proposed method is mainly based on You Only Look Only (YOLOv3) model, a cutting-edge object detection technique, which is faster and more accurate than most of the traditional and well-known approaches such as Region-based Convolutional Neural Network (R-CNN). According to the experimental results, the overall accuracy of airplanes' detection, using KhalifaSat images, is 97.64\% with a reasonable computational time. Our findings prove that the proposed YOLOv3 model is robust and efficient in detecting airplanes of different dimensions in high resolution satellite imagery.},
	booktitle = {2021 4th {International} {Conference} on {Signal} {Processing} and {Information} {Security} ({ICSPIS})},
	author = {Al Mansoori, Saeed and Kunhu, Alavi and AlHammadi, Abdulla},
	month = nov,
	year = {2021},
	keywords = {airplane detection, Airplanes, Atmospheric modeling, Computational modeling, deep learning, Deep learning, Object detection, satellite imagery, Satellites, Signal processing algorithms, YOLOv3},
	pages = {57--60},
}

@article{ayush_efficient_2021,
	title = {Efficient {Poverty} {Mapping} from {High} {Resolution} {Remote} {Sensing} {Images}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16072},
	doi = {10.1609/aaai.v35i1.16072},
	abstract = {The combination of high-resolution satellite imagery and machine learning have proven useful in many sustainability-related tasks, including poverty prediction, infrastructure measurement, and forest monitoring. However, the accuracy afforded by high-resolution imagery comes at a cost, as such imagery is extremely expensive to purchase at scale. This creates a substantial hurdle to the efficient scaling and widespread adoption of high-resolution-based approaches.  To reduce acquisition costs while maintaining accuracy, we propose a reinforcement learning approach in which free low-resolution imagery is used to dynamically identify where to acquire costly high-resolution images, prior to performing a deep learning task on the high-resolution images. We apply this approach to the task of poverty prediction in Uganda, building on an earlier approach that used object detection to count objects and use these counts to predict poverty. Our approach exceeds previous performance benchmarks on this task while using 80\% fewer high-resolution images, and could be useful in many domains that require high-resolution imagery.},
	language = {en},
	number = {1},
	urldate = {2023-01-31},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ayush, Kumar and Uzkent, Burak and Tanmay, Kumar and Burke, Marshall and Lobell, David and Ermon, Stefano},
	month = may,
	year = {2021},
	note = {Number: 1},
	keywords = {Reinforcement Learning},
	pages = {12--20},
}

@misc{noauthor_object_nodate,
	title = {Object detection in pleiades images using deep features {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/7729396},
	urldate = {2023-01-31},
}

@article{jean_combining_2016,
	title = {Combining satellite imagery and machine learning to predict poverty},
	volume = {353},
	doi = {10.1126/science.aaf7894},
	abstract = {Measuring consumption and wealth remotely
Nighttime lighting is a rough proxy for economic wealth, and nighttime maps of the world show that many developing countries are sparsely illuminated. Jean et al. combined nighttime maps with high-resolution daytime satellite images (see the Perspective by Blumenstock). With a bit of machine-learning wizardry, the combined images can be converted into accurate estimates of household consumption and assets, both of which are hard to measure in poorer countries. Furthermore, the night- and day-time data are publicly available and nonproprietary.
Science , this issue p. 790 ; see also p. 753},
	journal = {Science},
	author = {Jean, Neal and Burke, Marshall and Xie, Michael and Davis, W. and Lobell, David and Ermon, Stefano},
	month = aug,
	year = {2016},
	keywords = {donnees satellites},
	pages = {790--794},
	annote = {slum
},
}

@article{garcia-garcia_review_2017,
	title = {A {Review} on {Deep} {Learning} {Techniques} {Applied} to {Semantic} {Segmentation}},
	url = {https://arxiv.org/pdf/1704.06857.pdf},
	abstract = {Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efﬁcient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every ﬁeld or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this ﬁeld as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their signiﬁcance in the ﬁeld. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.},
	language = {en},
	urldate = {2021-10-18},
	journal = {arXiv:1704.06857 [cs]},
	author = {Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Garcia-Rodriguez, Jose},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.06857},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Submitted to TPAMI on Apr. 22, 2017},
}

@misc{noauthor_accurate_nodate,
	title = {Accurate {Road} {Detection} from {Satellite} {Images} {Using} {Modified} {U}-net {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Accurate-Road-Detection-from-Satellite-Images-Using-Constantin-Ding/4f65550902aebed04487ab0c4b97c5de2fdcfcb4},
	urldate = {2021-10-18},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2021-10-19},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
}

@misc{noauthor_combining_nodate,
	title = {Combining satellite imagery and machine learning to predict poverty},
	url = {https://www.science.org/doi/abs/10.1126/science.aaf7894},
	urldate = {2021-10-28},
}

@article{ienco_combining_2019,
	title = {Combining {Sentinel}-1 and {Sentinel}-2 {Satellite} {Image} {Time} {Series} for land cover mapping via a multi-source deep learning architecture},
	volume = {158},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619302278},
	doi = {10.1016/j.isprsjprs.2019.09.016},
	abstract = {The huge amount of data currently produced by modern Earth Observation (EO) missions has allowed for the design of advanced machine learning techniques able to support complex Land Use/Land Cover (LULC) mapping tasks. The Copernicus programme developed by the European Space Agency provides, with missions such as Sentinel-1 (S1) and Sentinel-2 (S2), radar and optical (multi-spectral) imagery, respectively, at 10 m spatial resolution with revisit time around 5 days. Such high temporal resolution allows to collect Satellite Image Time Series (SITS) that support a plethora of Earth surface monitoring tasks. How to effectively combine the complementary information provided by such sensors remains an open problem in the remote sensing field. In this work, we propose a deep learning architecture to combine information coming from S1 and S2 time series, namely TWINNS (TWIn Neural Networks for Sentinel data), able to discover spatial and temporal dependencies in both types of SITS. The proposed architecture is devised to boost the land cover classification task by leveraging two levels of complementarity, i.e., the interplay between radar and optical SITS as well as the synergy between spatial and temporal dependencies. Experiments carried out on two study sites characterized by different land cover characteristics (i.e., the Koumbia site in Burkina Faso and Reunion Island, a overseas department of France in the Indian Ocean), demonstrate the significance of our proposal.},
	language = {en},
	urldate = {2022-09-21},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Ienco, Dino and Interdonato, Roberto and Gaetano, Raffaele and Ho Tong Minh, Dinh},
	month = dec,
	year = {2019},
	keywords = {Deep learning, Data fusion, Land cover classification, Satellite Image Time Series, Sentinel-1, Sentinel-2},
	pages = {11--22},
}

@article{wang_understanding_2018,
	title = {Understanding {Convolution} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1702.08502},
	abstract = {Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have led to signiﬁcant improvement over previous semantic segmentation systems. Here we show how to improve pixel-wise semantic segmentation by manipulating convolution-related operations that are of both theoretical and practical value. First, we design dense upsampling convolution (DUC) to generate pixel-level prediction, which is able to capture and decode more detailed information that is generally missing in bilinear upsampling. Second, we propose a hybrid dilated convolution (HDC) framework in the encoding phase. This framework 1) effectively enlarges the receptive ﬁelds (RF) of the network to aggregate global information; 2) alleviates what we call the “gridding issue”caused by the standard dilated convolution operation. We evaluate our approaches thoroughly on the Cityscapes dataset, and achieve a state-of-art result of 80.1\% mIOU in the test set at the time of submission. We also have achieved state-of-theart overall on the KITTI road estimation benchmark and the PASCAL VOC2012 segmentation task. Our source code can be found at https://github.com/TuSimple/ TuSimple-DUC .},
	language = {en},
	urldate = {2021-10-18},
	journal = {arXiv:1702.08502 [cs]},
	author = {Wang, Panqu and Chen, Pengfei and Yuan, Ye and Liu, Ding and Huang, Zehua and Hou, Xiaodi and Cottrell, Garrison},
	month = may,
	year = {2018},
	note = {arXiv: 1702.08502},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: WACV 2018. Updated acknowledgements. Source code: https://github.com/TuSimple/TuSimple-DUC},
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2021-10-15},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Unet},
	annote = {Comment: conditionally accepted at MICCAI 2015},
}

@article{wagner_u-net-id_2020,
	title = {U-{Net}-{Id}, an {Instance} {Segmentation} {Model} for {Building} {Extraction} from {Satellite} {Images}—{Case} {Study} in the {Joanópolis} {City}, {Brazil}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/12/10/1544},
	doi = {10.3390/rs12101544},
	abstract = {Currently, there exists a growing demand for individual building mapping in regions of rapid urban growth in less-developed countries. Most existing methods can segment buildings but cannot discriminate adjacent buildings. Here, we present a new convolutional neural network architecture (CNN) called U-net-id that performs building instance segmentation. The proposed network is trained with WorldView-3 satellite RGB images (0.3 m) and three different labeled masks. The first is the building mask; the second is the border mask, which is the border of the building segment with 4 pixels added outside and 3 pixels inside; and the third is the inner segment mask, which is the segment of the building diminished by 2 pixels. The architecture consists of three parallel paths, one for each mask, all starting with a U-net model. To accurately capture the overlap between the masks, all activation layers of the U-nets are copied and concatenated on each path and sent to two additional convolutional layers before the output activation layers. The method was tested with a dataset of 7563 manually delineated individual buildings of the city of Joanópolis-SP, Brazil. On this dataset, the semantic segmentation showed an overall accuracy of 97.67\% and an F1-Score of 0.937 and the building individual instance segmentation showed good performance with a mean intersection over union (IoU) of 0.582 (median IoU = 0.694).},
	language = {en},
	number = {10},
	urldate = {2021-10-18},
	journal = {Remote Sensing},
	author = {Wagner, Fabien H. and Dalagnol, Ricardo and Tarabalka, Yuliya and Segantine, Tassiana Y. F. and Thomé, Rogério and Hirye, Mayumi C. M.},
	month = jan,
	year = {2020},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {building detection, instance segmentation, U-net, urban landscape},
	pages = {1544},
}

@article{forkuor_landsat-8_2018,
	title = {Landsat-8 vs. {Sentinel}-2: examining the added value of sentinel-2’s red-edge bands to land-use and land-cover mapping in {Burkina} {Faso}},
	volume = {55},
	issn = {1548-1603, 1943-7226},
	shorttitle = {Landsat-8 vs. {Sentinel}-2},
	url = {https://www.tandfonline.com/doi/full/10.1080/15481603.2017.1370169},
	doi = {10.1080/15481603.2017.1370169},
	language = {en},
	number = {3},
	urldate = {2022-11-29},
	journal = {GIScience \& Remote Sensing},
	author = {Forkuor, Gerald and Dimobe, Kangbeni and Serme, Idriss and Tondoh, Jerome Ebagnerin},
	month = may,
	year = {2018},
	keywords = {donnees satellites},
	pages = {331--354},
}

@misc{zhang_unsupervised_2022,
	title = {Unsupervised {Wildfire} {Change} {Detection} based on {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2211.14654},
	abstract = {The accurate characterization of the severity of the wildﬁre event strongly contributes to the characterization of the fuel conditions in ﬁre-prone areas, and provides valuable information for disaster response. The aim of this study is to develop an autonomous system built on top of high-resolution multispectral satellite imagery, with an advanced deep learning method for detecting burned area change. This work proposes an initial exploration of using an unsupervised model for feature extraction in wildﬁre scenarios. It is based on the contrastive learning technique SimCLR, which is trained to minimize the cosine distance between augmentations of images. The distance between encoded images can also be used for change detection. We propose changes to this method that allows it to be used for unsupervised burned area detection and following downstream tasks. We show that our proposed method outperforms the tested baseline approaches.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Zhang, Beichen and Wang, Huiqi and Alabri, Amani and Bot, Karol and McCall, Cole and Hamilton, Dale and Růžička, Vít},
	month = nov,
	year = {2022},
	note = {arXiv:2211.14654 [cs]},
	keywords = {Computer Science - Machine Learning, donnees satellites, slum detection, Computer Science - Computer Vision and Pattern Recognition, change detection, annotations},
}

@article{verma_transfer_2019,
	title = {Transfer learning approach to map urban slums using high and medium resolution satellite imagery},
	volume = {88},
	issn = {0197-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S0197397518306982},
	doi = {10.1016/j.habitatint.2019.04.008},
	abstract = {Slums provide cheaper workforce and informal services which contribute substantially towards GDP. However, such areas, due to the high population density, sub-standard housing and lack of essential services are urban risks. The socio-physical development of such settlements has often been neglected due to poor laws and provisions in urban management and policies. One of the primary reasons for negligence has been the unavailability of slum maps to study the evolution of slums and to actively manage and contain them. Various remote sensing techniques have been utilized to answer the problem but have not produced universal solutions. In recent years, Deep Learning (DL) techniques with remote sensing have been found beneficial in comprehending the underlying structure of physical features present in the satellite imageries. This study deals with one of the Deep Learning techniques which use pre-trained convolutional networks for slum detection in Very High Resolution (VHR) and Medium Resolution (MR) satellite imagery. We created a training dataset which comprises of four classes including slums, built, green and water. We further trained the model to detect these classes in the entire city. Classification performance was evaluated for Very high and Medium Resolution imagery with the help of manually delineated slum boundaries gathered from urban local authorities of Mumbai. The Overall accuracy of 94.2 and 90.2 and kappa of 0.70 and 0.55 is obtained from VHR and MR imagery respectively. We provide a comprehensive technique for the detection of informal settlements which can be tailored and applied to any city to detect various landforms.},
	language = {en},
	urldate = {2023-01-25},
	journal = {Habitat International},
	author = {Verma, Deepank and Jana, Arnab and Ramamritham, Krithi},
	month = jun,
	year = {2019},
	keywords = {donnees satellites, slum detection},
	pages = {101981},
}

@article{kuffer_slums_2016,
	title = {Slums from {Space}—15 {Years} of {Slum} {Mapping} {Using} {Remote} {Sensing}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/8/6/455},
	doi = {10.3390/rs8060455},
	abstract = {The body of scientific literature on slum mapping employing remote sensing methods has increased since the availability of more very-high-resolution (VHR) sensors. This improves the ability to produce information for pro-poor policy development and to build methods capable of supporting systematic global slum monitoring required for international policy development such as the Sustainable Development Goals. This review provides an overview of slum mapping-related remote sensing publications over the period of 2000–2015 regarding four dimensions: contextual factors, physical slum characteristics, data and requirements, and slum extraction methods. The review has shown the following results. First, our contextual knowledge on the diversity of slums across the globe is limited, and slum dynamics are not well captured. Second, a more systematic exploration of physical slum characteristics is required for the development of robust image-based proxies. Third, although the latest commercial sensor technologies provide image data of less than 0.5 m spatial resolution, thereby improving object recognition in slums, the complex and diverse morphology of slums makes extraction through standard methods difficult. Fourth, successful approaches show diversity in terms of extracted information levels (area or object based), implemented indicator sets (single or large sets) and methods employed (e.g., object-based image analysis (OBIA) or machine learning). In the context of a global slum inventory, texture-based methods show good robustness across cities and imagery. Machine-learning algorithms have the highest reported accuracies and allow working with large indicator sets in a computationally efficient manner, while the upscaling of pixel-level information requires further research. For local slum mapping, OBIA approaches show good capabilities of extracting both area- and object-based information. Ultimately, establishing a more systematic relationship between higher-level image elements and slum characteristics is essential to train algorithms able to analyze variations in slum morphologies to facilitate global slum monitoring.},
	language = {en},
	number = {6},
	urldate = {2022-12-03},
	journal = {Remote Sensing},
	author = {Kuffer, Monika and Pfeffer, Karin and Sliuzas, Richard},
	month = jun,
	year = {2016},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {donnees satellites, slum detection, Global South, informal areas, slums, urban remote sensing, VHR imagery},
	pages = {455},
}

@article{kit_texture-based_2012,
	title = {Texture-based identification of urban slums in {Hyderabad}, {India} using remote sensing data},
	volume = {32},
	issn = {0143-6228},
	url = {https://www.sciencedirect.com/science/article/pii/S0143622811001512},
	doi = {10.1016/j.apgeog.2011.07.016},
	abstract = {This paper outlines a methodology to identify informal settlements out of high resolution satellite imagery using the concept of lacunarity. Principal component analysis and line detection algorithms were applied alternatively to obtain a high resolution binary representation of the city of Hyderabad, India and used to calculate lacunarity values over a 60 × 60 m grid. A number of ground truthing areas were used to classify the resulting datasets and to identify lacunarity ranges which are typical for settlement types that combine high density housing and small dwelling size – features characteristic for urban slums in India. It was discovered that the line detection algorithm is advantageous over principal component analysis in providing suitable binary datasets for lacunarity analysis as it is less sensitive to spectral variability within mosaicked imagery. The resulting slum location map constitutes an efficient tool in identifying particularly overcrowded areas of the city and can be used as a reliable source in vulnerability and resilience assessments at a later stage. The proposed methodology allows for rapid analysis and comparison of multi-temporal data and can be applied on many developing urban agglomerations around the world.},
	language = {en},
	number = {2},
	urldate = {2022-12-03},
	journal = {Applied Geography},
	author = {Kit, Oleksandr and Lüdeke, Matthias and Reckien, Diana},
	month = mar,
	year = {2012},
	keywords = {donnees satellites, slum detection, Remote sensing, Hyderabad/India, Image classification, Informal settlements, Lacunarity, Line detection, Principal component analysis, Slums},
	pages = {660--667},
}

@misc{noauthor_satellite_2020,
	title = {Satellite imagery sources: {What} do you need to know},
	shorttitle = {Satellite imagery sources},
	url = {https://picterra.ch/blog/satellite-imagery-sources/},
	abstract = {Do you want to understand the key concepts of Earth Observation data? Check this free ultimate guide to satellite imagery sources.},
	language = {en-GB},
	urldate = {2023-01-25},
	journal = {Picterra},
	month = dec,
	year = {2020},
	keywords = {donnees satellites},
}

@article{uddin_forest_2015,
	title = {Forest {Condition} {Monitoring} {Using} {Very}-{High}-{Resolution} {Satellite} {Imagery} in a {Remote} {Mountain} {Watershed} in {Nepal}},
	volume = {35},
	issn = {0276-4741, 1994-7151},
	url = {https://bioone.org/journals/mountain-research-and-development/volume-35/issue-3/MRD-JOURNAL-D-14-00074.1/Forest-Condition-Monitoring-Using-Very-High-Resolution-Satellite-Imagery-in/10.1659/MRD-JOURNAL-D-14-00074.1.full},
	doi = {10.1659/MRD-JOURNAL-D-14-00074.1},
	abstract = {Satellite imagery has proven extremely useful for repetitive timeline-based data collection, because it offers a synoptic view and enables fast processing of large quantities of data. The changes in tree crown number and land cover in a very remote watershed (area 1305 ha) in Nepal were analyzed using a QuickBird image from 2006 and an IKONOS image from 2011. A geographic object-based image analysis (GEOBIA) was carried out using the region-growing technique for tree crown detection, delineation, and change assessment, and a multiresolution technique was used for land cover mapping and change analysis. The coefficient of determination for tree crown detection and delineation was 0.97 for QuickBird and 0.99 for IKONOS, calculated using a line-intercept transect method with 10 randomly selected windows (1×1 ha). The number of tree crowns decreased from 47,121 in 2006 to 41,689 in 2011, a loss of approximately 90 trees per month on average; the area of needle-leaved forest was reduced by 140 ha (23\%) over the same period. Analysis of widely available very-high-resolution satellite images using GEOBIA techniques offers a cost-effective method for detecting changes in tree crown number and land cover in remote mountain valleys; the results provide the information needed to support improved local-level planning and forest management in such areas.},
	number = {3},
	urldate = {2023-01-25},
	journal = {Mountain Research and Development},
	author = {Uddin, Kabir and Gilani, Hammad and Murthy, M. S. R. and Kotru, Rajan and Qamer, Faisal Mueen},
	month = aug,
	year = {2015},
	note = {Publisher: International Mountain Society},
	keywords = {donnees satellites},
	pages = {264--277},
}

@misc{noauthor_deep_2020,
	title = {Deep learning approach for building detection},
	url = {https://picterra.ch/blog/deep-learning-approach-for-building-detection/},
	abstract = {Detecting buildings in satellite or drone imagery? Read how to save 90\% of the cost and time thanks to the deep learning approach for building detection.},
	language = {en-GB},
	urldate = {2023-01-25},
	journal = {Picterra},
	month = jun,
	year = {2020},
	keywords = {donnees satellites},
}

@inproceedings{ivanovsky_building_2019,
	title = {Building {Detection} on {Aerial} {Images} {Using} {U}-{NET} {Neural} {Networks}},
	doi = {10.23919/FRUCT.2019.8711930},
	abstract = {This article presents research results of two convolutional neural networks for building detection on satellite images of Planet database. To analyze the quality of developed algorithms, there was used Sorensen-Dice coefficient of similarity which compares results of algorithms with tagged masks. The masks were generated from json files and sliced on smaller parts together with respective images before the training of algorithms. This approach allows to cope with the problem of segmentation for aerial high-resolution images efficiently and effectively. The problem of building detection on satellite images can be put into practice for urban planning, building control, etc.},
	booktitle = {2019 24th {Conference} of {Open} {Innovations} {Association} ({FRUCT})},
	author = {Ivanovsky, Leonid and Khryashchev, Vladimir and Pavlov, Vladimir and Ostrovskaya, Anna},
	month = apr,
	year = {2019},
	note = {ISSN: 2305-7254},
	keywords = {donnees satellites},
	pages = {116--122},
}

@techreport{witjes_spatiotemporal_2021,
	title = {A spatiotemporal ensemble machine learning framework for generating land use / land cover time-series maps for {Europe} (2000 – 2019) based on {LUCAS}, {CORINE} and {GLAD} {Landsat}},
	url = {https://www.researchsquare.com/article/rs-561383/v1/direct-download.pdf?c=1679043412614},
	abstract = {A seamless spatiotemporal machine learning framework for automated prediction, uncertainty assessment, and analysis of land use / land cover (LULC) dynamics is presented. The framework includes: (1) harmonization and preprocessing of high-resolution spatial and spatiotemporal covariate datasets (GLAD Landsat, NPP/VIIRS) including 5 million harmonized LUCAS and CORINE Land Cover-derived training samples, (2) model building based on spatial k-fold cross-validation and hyper-parameter optimization, (3) prediction of the most probable class, class probabilities and uncertainty per pixel, (4) LULC change analysis on time-series of produced maps. The spatiotemporal ensemble model was fitted by combining random forest, gradient boosted trees, and artificial neural network, with logistic regressor as meta-learner. The results show that the most important covariates for mapping LULC in Europe are: seasonal aggregates of Landsat green and near-infrared bands, multiple Landsat-derived spectral indices, and elevation. Spatial cross-validation of the model indicates consistent performance across multiple years with 62\%, 70\%, and 87\% accuracy when predicting 33 (level-3), 14 (level-2), and 5 classes (level-1); with artificial surface classes such as \&\#039;airports\&\#039; and \&\#039;railroads\&\#039; showing the lowest match with validation points. The spatiotemporal model outperforms spatial models on known-year classification by 2.7\% and unknown-year classification by 3.5\%. Results of the accuracy assessment using 48,365 independent test samples shows 87\% match with the validation points. Results of time-series analysis (time-series of LULC probabilities and NDVI images) suggest gradual deforestation trends in large parts of Sweden, the Alps, and Scotland. An advantage of using spatiotemporal ML is that the fitted model can be used to predict LULC in years that were not included in its training dataset, allowing generalization to past and future periods, e.g. to predict land cover for years prior to 2000 and beyond 2020. The generated land cover time-series data stack (ODSE-LULC), including the training points, is publicly available via the Open Data Science (ODS)-Europe Viewer.},
	urldate = {2021-10-06},
	author = {Witjes, Martijn and Parente, Leandro and Diemen, Chris J. van and Hengl, Tomislav and Landa, Martin and Brodsky, Lukas and Halounova, Lena and Krizan, Josip and Antonic, Luka and Ilie, Codrina M. and Craciunescu, Vasile and Kilibarda, Milan and Antonijevic, Ognjen and Glusica, Luka},
	month = oct,
	year = {2021},
	doi = {10.21203/rs.3.rs-561383/v1},
	note = {ISSN: 2693-5015
Type: article},
	keywords = {donnees satellites, stacking},
	annote = {Très en surface
-{\textgreater} stacking RF reseau artificiel  etc.. + ajout de vraibles auxiliaires (lat long) + interpolation des points non renseignés},
}

@article{mahabir_critical_2018,
	title = {A {Critical} {Review} of {High} and {Very} {High}-{Resolution} {Remote} {Sensing} {Approaches} for {Detecting} and {Mapping} {Slums}: {Trends}, {Challenges} and {Emerging} {Opportunities}},
	volume = {2},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2413-8851},
	shorttitle = {A {Critical} {Review} of {High} and {Very} {High}-{Resolution} {Remote} {Sensing} {Approaches} for {Detecting} and {Mapping} {Slums}},
	url = {https://www.mdpi.com/2413-8851/2/1/8},
	doi = {10.3390/urbansci2010008},
	abstract = {Slums are a global urban challenge, with less developed countries being particularly impacted. To adequately detect and map them, data is needed on their location, spatial extent and evolution. High- and very high-resolution remote sensing imagery has emerged as an important source of data in this regard. The purpose of this paper is to critically review studies that have used such data to detect and map slums. Our analysis shows that while such studies have been increasing over time, they tend to be concentrated to a few geographical areas and often focus on the use of a single approach (e.g., image texture and object-based image analysis), thus limiting generalizability to understand slums, their population, and evolution within the global context. We argue that to develop a more comprehensive framework that can be used to detect and map slums, other emerging sourcing of geospatial data should be considered (e.g., volunteer geographic information) in conjunction with growing trends and advancements in technology (e.g., geosensor networks). Through such data integration and analysis we can then create a benchmark for determining the most suitable methods for mapping slums in a given locality, thus fostering the creation of new approaches to address this challenge.},
	language = {en},
	number = {1},
	urldate = {2022-12-03},
	journal = {Urban Science},
	author = {Mahabir, Ron and Croitoru, Arie and Crooks, Andrew T. and Agouris, Peggy and Stefanidis, Anthony},
	month = mar,
	year = {2018},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {donnees satellites, slum detection, slums, évolution des bidonvilles, geosensor networks, high-and very high-resolution imagery, image analysis, remote sensing, volunteer geographic information},
	pages = {8},
	annote = {Les endroits où les bidonvilles se créent/se situent ne sont pas aléatoires, ils dépendent de plusieurs facteurs comme l’offre d’emploi et la disponibilité d’espace.

Évolution des bidonvilles :
L’évolution d’un bidonville se caractérise par 3 stades distincts.


1er stade - l'enfance : peu d’habitations.


2ème stade - la consolidation : introduction de services au sein du bidonville, création d’une frontière de peuplement avec la ville, amélioration des constructions.


3ème stade - la maturité : densification non durable, bidonville encombré, voire densification verticale (des étages sont créés), démolition.



Même si la majorité des bidonvilles sont de base des constructions informelles, des parties d’une ville avec des bâtiments formels peuvent se transformer en bidonvilles. Ce n’est donc pas parce qu'une construction est formelle qu’elle n’a aucun risque de devenir un bidonville. Ce sont donc également des localisations à surveiller dans le temps.

Face à la difficulté de détecter ces changements par détection satellite, il est important d’avoir des données locales pour compléter ces données au fil du temps.
Au préalable, il faut bien définir ce qu’est un bidonville pour savoir ce qu’on cherche à détecter.

Cartographie des bidonvilles en 3 étapes : 


Détection de bidonville : classification d’image.


Délimitation : identification de l'étendue spatiale des bidonvilles.


Caractérisation du bidonville.


Détection de bidonvilles en 7 approches : 


Multi-échelles : Mesure fractale (faible pour un bidonville car les structures sont désordonnées et chaotiques) et lacunarité (élevée lorsqu’on rencontre un bidonville car il y a une discontinuité avec la partie bâtiments formels de la commune).


Texture des images : Déterminer le profil morphologique pour les bâtiments de bidonville et le comparer au profil d’un bâtiment formel. Ex : le profil morphologique est plus petit pour le bâtiment d’un bidonville qu’un bâtiment formel à Nairobi au Kenya (presque 3x plus petit). Images avec des pixels en niveau de gris : les bidonvilles sont les parties de la ville les plus concentrées donc avec des niveau de gris les plus élevés, de plus, à Mumbai en Inde, les bidonvilles ont des variances de niveau de gris inférieures aux zones formelles de la ville. Problème de généralisation (pas les mêmes règles pour les bidonvilles de différents pays/villes). Applicable lorsqu’on voit la limite de peuplement, c’est à dire au stade 2 et 3 d’un bidonville.


Analyse du paysage : Des indices pour calculer la densité par exemple (plus élevé pour les bidonvilles). Aussi, il a été démontré que la végétation était beaucoup plus compacte dans les bidonvilles que dans d'autres zones formelles. Mesures du paysage en fonction de l’adéquation du paysage. Adapté aux bidonvilles de stade 2 et 3.


Analyse basée sur les objets : Considérer l’image comme une composition d’objets. Méthode spécifique aux données donc difficilement généralisable. Peu d’études sur la segmentation d’images pour détecter les bidonvilles.


Caractéristiques des bâtiments : Modèles numériques de surface (3D), hauteur du bâtiment (densification verticale).


Apprentissage automatique : performances élevées, besoin de données d’apprentissage.


Données socio-économiques : utiliser des données disponibles pour détecter les bidonvilles et les évolutions des bidonvilles.


},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {Detecting} {Building} {Changes} with {Off}-{Nadir} {Aerial} {Images}},
	url = {https://paperswithcode.com/paper/detecting-building-changes-with-off-nadir},
	abstract = {Implemented in one code library.},
	language = {en},
	urldate = {2023-03-17},
	keywords = {change detection, annotations},
	annote = {Encore un beau dataset annoté disponible  ici
https://github.com/fitzpchao/BANDON
:BaiDu Netdisk
},
}

@article{chen_object-based_2012,
	title = {Object-based change detection},
	volume = {33},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431161.2011.648285},
	doi = {10.1080/01431161.2011.648285},
	abstract = {Characterizations of land-cover dynamics are among the most important applications of Earth observation data, providing insights into management, policy and science. Recent progress in remote sensing and associated digital image processing offers unprecedented opportunities to detect changes in land cover more accurately over increasingly large areas, with diminishing costs and processing time. The advent of high-spatial-resolution remote-sensing imagery further provides opportunities to apply change detection with object-based image analysis (OBIA), that is, object-based change detection (OBCD). When compared with the traditional pixel-based change paradigm, OBCD has the ability to improve the identification of changes for the geographic entities found over a given landscape. In this article, we present an overview of the main issues in change detection, followed by the motivations for using OBCD as compared to pixel-based approaches. We also discuss the challenges caused by the use of objects in change detection and provide a conceptual overview of solutions, which are followed by a detailed review of current OBCD algorithms. In particular, OBCD offers unique approaches and methods for exploiting high-spatial-resolution imagery, to capture meaningful detailed change information in a systematic and repeatable manner, corresponding to a wide range of information needs.},
	number = {14},
	urldate = {2023-03-17},
	journal = {International Journal of Remote Sensing},
	author = {Chen, Gang and Hay, Geoffrey J. and Carvalho, Luis M. T. and Wulder, Michael A.},
	month = jul,
	year = {2012},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01431161.2011.648285},
	keywords = {change detection, images haute résolution},
	pages = {4434--4457},
}

@article{lebedev_change_2018,
	title = {Change {Detection} in {Remote} {Sensing} {Images} {Using} {Conditional} {Adversarial} {Networks}},
	volume = {422},
	issn = {2194-9034       The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	url = {https://ui.adsabs.harvard.edu/abs/2018ISPAr.422..565L},
	doi = {10.5194/isprs-archives-XLII-2-565-2018},
	abstract = {We present a method for change detection in images using Conditional Adversarial Network approach. The original network architecture based on pix2pix is proposed and evaluated for difference map creation. The paper address three types of experiments: change detection in synthetic images without objects relative shift, change detection in synthetic images with small relative shift of objects, and change detection in real season-varying remote sensing images.},
	urldate = {2023-03-17},
	journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Lebedev, M. A. and Vizilter, Y. V. and Vygolov, O. V. and Knyaz, V. A. and Rubis, A. Y.},
	month = may,
	year = {2018},
	note = {ADS Bibcode: 2018ISPAr.422..565L},
	keywords = {change detection},
	pages = {565--571},
	annote = {Clement : pointe vers un dataset annoté de change detection
},
}

@article{celik_unsupervised_2009,
	title = {Unsupervised {Change} {Detection} in {Satellite} {Images} {Using} {Principal} {Component} {Analysis} and \$k\$-{Means} {Clustering}},
	volume = {6},
	issn = {1558-0571},
	doi = {10.1109/LGRS.2009.2025059},
	abstract = {In this letter, we propose a novel technique for unsupervised change detection in multitemporal satellite images using principal component analysis (PCA) and k-means clustering. The difference image is partitioned into h times h nonoverlapping blocks. S, S les h2, orthonormal eigenvectors are extracted through PCA of h times h nonoverlapping block set to create an eigenvector space. Each pixel in the difference image is represented with an S-dimensional feature vector which is the projection of h times h difference image data onto the generated eigenvector space. The change detection is achieved by partitioning the feature vector space into two clusters using k-means clustering with k = 2 and then assigning each pixel to the one of the two clusters by using the minimum Euclidean distance between the pixel's feature vector and mean feature vector of clusters. Experimental results confirm the effectiveness of the proposed approach.},
	number = {4},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Celik, Turgay},
	month = oct,
	year = {2009},
	note = {Conference Name: IEEE Geoscience and Remote Sensing Letters},
	keywords = {Satellites, change detection, Principal component analysis, remote sensing, \$k\$-means clustering, Change detection algorithms, Data mining, Euclidean distance, Image analysis, multitemporal satellite images, Optical devices, optical images, Parameter estimation, Pixel, principal component analysis (PCA), Radar detection},
	pages = {772--776},
	annote = {Clement : très intéressant aussi bien que daté (2012). 
Idée génrale :
On prend 2 images couvrant la même zone géographique en t et t+1
On calcule l’image “différence” Xd = {\textbar}Xt+1-Xt{\textbar}
On partitionne X\_d en patchs de même taille on a donc des patchs X(x,y) 1{\textless}= x {\textless}= h et  1 {\textless}= y {\textless}= h, donc h{\textasciicircum}2 patchs
On transforme ces patchs (matrices de pixels) en vecteurs, ce qui nous fait h**2 vecteurs.
On réalise une ACP sur ces vecteurs (ou tsne ou n’importe quoi d’autres) et on projette nos vecteurs patchs sur cet espace (revient à faire un embedding avec un neural net)
Ceci étant fait on fait un clustering k means à 2 classes. la classe 1 sera la classe des unchanged patch et la classe 2 celle des patchs qui ont changé.
Pour reconnaitre quelle classe est changed ou unchanged -{\textgreater} on regarde la valeur des patchs de l’image différence appartenant à telle ou telle classe) la classe qui a les plus grosses différences est la classe des patchs qui ont changés très naturellement.
Ce qui est intéressant : technique non supervisée  ={\textgreater} pas de labels.
Peut se pratiquer usur une seule image.
={\textgreater} facile à mettre en place pour benchmarker ! 
},
}

@article{peng_end--end_2019,
	title = {End-to-{End} {Change} {Detection} for {High} {Resolution} {Satellite} {Images} {Using} {Improved} {UNet}++},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/11/11/1382},
	doi = {10.3390/rs11111382},
	abstract = {Change detection (CD) is essential to the accurate understanding of land surface changes using available Earth observation data. Due to the great advantages in deep feature representation and nonlinear problem modeling, deep learning is becoming increasingly popular to solve CD tasks in remote-sensing community. However, most existing deep learning-based CD methods are implemented by either generating difference images using deep features or learning change relations between pixel patches, which leads to error accumulation problems since many intermediate processing steps are needed to obtain final change maps. To address the above-mentioned issues, a novel end-to-end CD method is proposed based on an effective encoder-decoder architecture for semantic segmentation named UNet++, where change maps could be learned from scratch using available annotated datasets. Firstly, co-registered image pairs are concatenated as an input for the improved UNet++ network, where both global and fine-grained information can be utilized to generate feature maps with high spatial accuracy. Then, the fusion strategy of multiple side outputs is adopted to combine change maps from different semantic levels, thereby generating a final change map with high accuracy. The effectiveness and reliability of our proposed CD method are verified on very-high-resolution (VHR) satellite image datasets. Extensive experimental results have shown that our proposed approach outperforms the other state-of-the-art CD methods.},
	language = {en},
	number = {11},
	urldate = {2023-03-16},
	journal = {Remote Sensing},
	author = {Peng, Daifeng and Zhang, Yongjun and Guan, Haiyan},
	month = jan,
	year = {2019},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, change detection, encoder-decoder architecture, end-to-end, feature maps, multiple side-outputs fusion},
	pages = {1382},
	annote = {Clément : Super article !
Stratégie de change detection avec modèles de segmentation de type Unet
Stratégie d’entrainement : on s’entraîne sur des triplets (images t image t+1 change\_detection\_segmentation\_map) et on a un algo en sortie qui prédit le masque de différence. 
renvoi sur l’article Change Detection in Remote Sensing Images using conditionnal Adversarial Networks dont les chercheur mettent publiquement disponible un jeu de données Im1+Im2 -{\textgreater} change segmentation map {\textless}3 https://drive.google.com/file/d/1GX656JqqOyBi\_Ef0w65kDGVto-nHrNs9
Fort impact de la data augmentation
Unet à multiples output, on calcule une loss sur tous les output et on somme + chaque sous loss est une combinaison linéaire e la cross entropy, du F1 score etc..
},
}

@article{mboga_detection_2017,
	title = {Detection of {Informal} {Settlements} from {VHR} {Images} {Using} {Convolutional} {Neural} {Networks}},
	volume = {9},
	issn = {2072-4292},
	url = {http://www.mdpi.com/2072-4292/9/11/1106},
	doi = {10.3390/rs9111106},
	language = {en},
	number = {11},
	urldate = {2023-03-20},
	journal = {Remote Sensing},
	author = {Mboga, Nicholus and Persello, Claudio and Bergado, John and Stein, Alfred},
	month = oct,
	year = {2017},
	keywords = {slum detection},
	pages = {1106},
	annote = {Article intéressant dans le sens où il s’agit de la même problématique que nous, avec des données très similaires (VHR, 0.6m).
Publié en 2017 et utilise des données de 2007 (et masques de 2004). 
CNN les plus performants
},
}

@article{duque_exploring_2017,
	title = {Exploring the {Potential} of {Machine} {Learning} for {Automatic} {Slum} {Identification} from {VHR} {Imagery}},
	volume = {9},
	issn = {2072-4292},
	url = {http://www.mdpi.com/2072-4292/9/9/895},
	doi = {10.3390/rs9090895},
	language = {en},
	number = {9},
	urldate = {2023-03-20},
	journal = {Remote Sensing},
	author = {Duque, Juan and Patino, Jorge and Betancourt, Alejandro},
	month = aug,
	year = {2017},
	keywords = {slum detection},
	pages = {895},
	annote = {Même problématique que nous. 
Utilise données de Google Earth et considère ca comme VHR (Pléaides devrait être bien mieux)
Intéressant car ils s’intéresse à la détection de changement même si ca marche pas chez eux (mais ils utilisent SVM et données Google Earth)
},
}

@article{noauthor_mfvnet_nodate,
	edition = {2022},
	title = {{MFVNet}: {Deep} {Adaptive} {Fusion} {Network} with {Multiple} {Field}-of-{Views} for {Remote} {Sensing} {Image} {Semantic} {Segmentation}},
	journal = {SCIENCE CHINA Information Sciences},
}
