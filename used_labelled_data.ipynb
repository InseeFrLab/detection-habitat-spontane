{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0758f67b-7497-4d0f-be0f-a000fb6da6c8",
   "metadata": {},
   "source": [
    "## Utilisation du jeu de données Xview2, qui intitialement présebnte des exemples de territoire avant et après dommage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73560aa1-5db5-4cfb-a536-c84403c33dc6",
   "metadata": {},
   "source": [
    "Je m'écarte ici un peu de l'idée initiale qui était de produire un dataset à l'aide du RIl et de la BDTOPO et je me concentre + sur les datasets labellisés préexistants. J'entraine un modèle de segmentation de suus ou un modèle de déttection d'objet et je vois comment ça réagit sur donnée spleiades. \n",
    "Dans tousd les cas le travail sur le RIL et la BD TOPO est à conserver puisque ces derniers servent de vzlidation !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b10de5-1360-4cda-ac46-9dec8016c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import s3fs\n",
    "import tarfile\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader,  random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},key ='HPC7PNZMF3SC28Q5FYY3', secret = '3DfluFzC0ZqQlCk2ifJLNy4Xra0D4tbwzoraa2ET', token = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJIUEM3UE5aTUYzU0MyOFE1RllZMyIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNjc1MzM4MzkyLCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6ImNsZW1lbnQuZ3VpbGxvQGluc2VlLmZyIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImV4cCI6MTY3NTQ0ODc5MSwiZmFtaWx5X25hbWUiOiJHdWlsbG8iLCJnaXZlbl9uYW1lIjoiQ2zDqW1lbnQiLCJncm91cHMiOlsiY2hhbGxlbmdlZGF0YS1lbnMiLCJmdW5hdGhvbiIsInNsdW1zLWRldGVjdGlvbiJdLCJpYXQiOjE2NzUzMzk2NTIsImlzcyI6Imh0dHBzOi8vYXV0aC5sYWIuc3NwY2xvdWQuZnIvYXV0aC9yZWFsbXMvc3NwY2xvdWQiLCJqdGkiOiI2NWVhMDA1Zi1hOTg0LTQ1ODctYWRhMS01ZWI5MTI4Mzg5NDMiLCJsb2NhbGUiOiJlbiIsIm5hbWUiOiJDbMOpbWVudCBHdWlsbG8iLCJub25jZSI6ImUwNDY0Zjc3LTU0OTctNGVmMi04NGRkLTI1OTdhNGIwNzI5OSIsInBvbGljeSI6InN0c29ubHkiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJjZ3VpbGxvIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNlc3Npb25Qb2xpY3kiOiJleUpXWlhKemFXOXVJam9pTWpBeE1pMHhNQzB4TnlJc0lsTjBZWFJsYldWdWRDSTZXM3NpUldabVpXTjBJam9pUVd4c2IzY2lMQ0pCWTNScGIyNGlPbHNpY3pNNktpSmRMQ0pTWlhOdmRYSmpaU0k2V3lKaGNtNDZZWGR6T25Nek9qbzZjSEp2YW1WMExYTnNkVzF6TFdSbGRHVmpkR2x2YmlJc0ltRnlianBoZDNNNmN6TTZPanB3Y205cVpYUXRjMngxYlhNdFpHVjBaV04wYVc5dUx5b2lYWDBzZXlKRlptWmxZM1FpT2lKQmJHeHZkeUlzSWtGamRHbHZiaUk2V3lKek16cE1hWE4wUW5WamEyVjBJbDBzSWxKbGMyOTFjbU5sSWpwYkltRnlianBoZDNNNmN6TTZPam9xSWwwc0lrTnZibVJwZEdsdmJpSTZleUpUZEhKcGJtZE1hV3RsSWpwN0luTXpPbkJ5WldacGVDSTZJbVJwWm1aMWMybHZiaThxSW4xOWZTeDdJa1ZtWm1WamRDSTZJa0ZzYkc5M0lpd2lRV04wYVc5dUlqcGJJbk16T2tkbGRFOWlhbVZqZENKZExDSlNaWE52ZFhKalpTSTZXeUpoY200NllYZHpPbk16T2pvNktpOWthV1ptZFhOcGIyNHZLaUpkZlYxOSIsInNlc3Npb25fc3RhdGUiOiI3YzFjMWIzNi0xNDM4LTQ2OWItOWYwNi1lOGY1ODFkMWM4MzkiLCJzaWQiOiI3YzFjMWIzNi0xNDM4LTQ2OWItOWYwNi1lOGY1ODFkMWM4MzkiLCJzdWIiOiIzYjA2ZWZhNC01OWZlLTQzYzgtYTAyYi1hOTRkOWI0YjU0NGUiLCJ0eXAiOiJCZWFyZXIifQ.ob4PwOHQerwgrx_es8u_nIHph-Iqt_1RLz4CvR_Bn8D67q8XJoGsHULDKvV19i6dB9TPZA-qi2oYIzVGOD3SJA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa7c3a3-c558-4f44-a1e5-243db46a0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# je travaille sur une version minimale du dataset xview, le vrai fait 50 giga.. mais ça devrait déjà faire l'affaire pour travailler\n",
    "fs.get('projet-slums-detection/Donnees/data_xBD.tar', 'data_xBD.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec31159-1d5f-4bf6-a688-67b7d85a65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"data_xBD.tar\", \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb79d8-72e1-4c6a-a450-c32ed061b1fd",
   "metadata": {},
   "source": [
    "### chargement/observation en place des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a199b3d3-21f9-4533-8af9-a99568a52211",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_image  = sorted(os.listdir(\"train/images/\"))\n",
    "liste_label  = sorted(os.listdir(\"train/labels/\")) # boundingbox et polygones !!\n",
    "liste_target  = sorted(os.listdir(\"train/targets/\")) # le masque de segmentation !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8367e53f-e724-4025-b202-204dd2df10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = 'train/labels/hurricane-florence_00000024_pre_disaster.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac233ac-45d9-4e5f-968e-f0eda92cb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "selec_pre_disaster = [nom_image.split(\"_\")[2] == \"pre\" for nom_image in liste_image]\n",
    "liste_image = np.array(liste_image)[selec_pre_disaster]\n",
    "liste_label = np.array(liste_label)[selec_pre_disaster]\n",
    "liste_target = np.array(liste_target)[selec_pre_disaster]\n",
    "\n",
    "print(np.sum(selec_pre_disaster)) # 3000 images pour s'entrainer avec des exemples entourés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35017c79-30e5-431b-8bb1-ddee599d83f3",
   "metadata": {},
   "source": [
    "## Observation du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43cf49-6ea6-41a3-87cb-4c7477a97748",
   "metadata": {},
   "source": [
    "- Images de dimension 1024-1024 à découper en 4 * 250 pour avoir un diviseur de 2000 (pour les données pleiades) (donc en 4)\n",
    "- Dans la classe data set splitter l'image en 4 et prendre un bout aléatoirement à chaque fois\n",
    "- image à 3 channels, pas de RGB ici..\n",
    "- est ce vraiment la mêlme résolution que pleiade ? résistance à la résolution ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d940dca-39fc-4e38-adc1-fa9fe0210722",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"train/images/guatemala-volcano_00000007_pre_disaster.png\")\n",
    "img = img.resize((1000,1000))\n",
    "\n",
    "masque = Image.open(\"train/targets/guatemala-volcano_00000007_pre_disaster_target.png\")\n",
    "masque = np.array(masque)\n",
    "show_mask = np.empty((*masque.shape, 3))\n",
    "show_mask[masque == 1, :] = [255,255,255]\n",
    "show_mask = show_mask.astype(np.uint8)\n",
    "\n",
    "# On traçe\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "ax1.imshow(img)\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(show_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2e06c-671b-447c-9348-dc14abe58f57",
   "metadata": {},
   "source": [
    "A mettre dans la classe dataset ! sélection d'un pa(tch aléatoire parmi les 16  possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90abee8a-1c54-413e-adb8-ec1b609f533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"train/images/guatemala-volcano_00000007_pre_disaster.png\")\n",
    "\n",
    "img = img.crop((0,0,1000,1000))# je dégomme les derniers pixels..\n",
    "img\n",
    "\n",
    "facteur_div = 250\n",
    "width, height = img.size\n",
    "\n",
    "num_subparts_x = width//facteur_div\n",
    "num_subparts_y =  height//facteur_div\n",
    "\n",
    "# sélection aléatoire d'une aprtie de l'image pour le dataset\n",
    "i = np.random.randint(num_subparts_x)\n",
    "j = np.random.randint(num_subparts_y)\n",
    "\n",
    "print(i,j)\n",
    "\n",
    "left = j * facteur_div\n",
    "right = (j+1) * facteur_div\n",
    "top = i * facteur_div\n",
    "bottom =(i+1)*facteur_div\n",
    "\n",
    "out = img.crop((left,top,right,bottom))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48d7a7-0330-400a-bd48-ea46a2b6fc10",
   "metadata": {},
   "source": [
    "Polygones associés au bati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9a1e928f-ea1e-4d0f-b25b-0d75738da0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4d252a4-2e20-4111-89b1-7921a01d3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    " class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths,mask_paths, facteur_div = 250):   # initial logic happens like transform\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.facteur_div = 250\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        with Image.open(self.image_paths[idx]) as img :\n",
    "            \n",
    "            width, height = img.size\n",
    "\n",
    "            num_subparts_x = width//facteur_div\n",
    "            num_subparts_y =  height//facteur_div\n",
    "            # sélection aléatoire d'une aprtie de l'image pour le dataset\n",
    "            i = np.random.randint(num_subparts_x)\n",
    "            j = np.random.randint(num_subparts_y)\n",
    "\n",
    "            left = j * facteur_div\n",
    "            right = (j+1) * facteur_div\n",
    "            top = i * facteur_div\n",
    "            bottom =(i+1)*facteur_div\n",
    "\n",
    "            img = img.crop((left,top,right,bottom))\n",
    "            img = img.convert(\"RGB\")\n",
    "            \n",
    "        with Image.open(self.mask_paths[idx]) as masque :\n",
    "            masque = masque.crop((left,top,right,bottom))\n",
    "            masque = np.array(masque)\n",
    "         \n",
    "        \n",
    "        masque = torch.tensor(masque,dtype = torch.long)\n",
    "        img = torch.tensor(np.array(img,dtype = float), dtype =torch.float).permute(2,0,1)\n",
    "        \n",
    "        ID = str(self.image_paths[idx])\n",
    "     \n",
    "        return {\"image\": img, \"masque\" : masque, \"id\" : ID} \n",
    "        \n",
    "    def __len__(self):  \n",
    "        return len(self.mask_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb1dc43c-0809-425e-830e-db19fcf12f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_paths = [\"train/images/\" + elt for elt in liste_image]\n",
    "train_masks_paths = [\"train/targets/\" + elt for elt in liste_target]\n",
    "\n",
    "all_dataset = CustomDataset(train_images_paths,train_masks_paths)\n",
    "one_element = next(iter(all_dataset))\n",
    "\n",
    "one_element[\"image\"].shape\n",
    "#torch.max(one_element[\"masque\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94d2c498-4227-4d1e-88bd-26a0825a97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loader = DataLoader(all_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "train_size = 2000\n",
    "val_size = len(all_dataset.mask_paths) - train_size\n",
    "#dans la liste donner la taille du train et la taille deu test\n",
    "train_dataset, valid_dataset = random_split(all_dataset,[train_size,val_size], generator=torch.Generator().manual_seed(42))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, \n",
    "                          shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "next(iter(valid_loader))[\"image\"].shape # parfait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67917b8-b69a-46d5-af2c-3ba34259598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True) # 233 Mega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf16d3-370b-4127-aafb-088e5ef8f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(\"load batch\")\n",
    "data = next(iter(train_loader)) \n",
    "images = data[\"image\"]\n",
    "labels = data[\"masque\"]\n",
    "optimizer.zero_grad()\n",
    "print(\"apply model\")\n",
    "outputs = model(images)['out']\n",
    "print(\"calculate loss\")\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(\"loss\")\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bd9d15e1-c64f-477a-94c9-aeb1013755ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autre idée : pour un batch donné \n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "loss = 0\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        print(loss)\n",
    "        images = data[\"image\"]\n",
    "        labels = data[\"masque\"]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "# Save the trained model\n",
    "#torch.save(model.state_dict(), 'path/to/model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ab30e-78ec-468d-bd67-9bf9a6016f0f",
   "metadata": {},
   "source": [
    "### Les contours géométriques en Json si besoin !! bounding box etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3edf84-a61a-462a-87e3-3be5d035a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(label_path)\n",
    "dico = jason.load(f)\n",
    "  \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b480e01-e129-467e-be78-2e92e58e8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico.keys()\n",
    "dico[\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab590c6b-684c-468f-b2fd-980217a9858e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
