{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e064d8b4-2af2-4263-800f-713875386be0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fonction to Raster\n",
    "Prend en entrée une Satellite Image, un dossier et un nom et la sauve en JP2 dans le dossier considéré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b683696-509c-466e-bdee-73d3aba6e3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install pyarrow -q -q -q \n",
    "! pip install rasterio -q -q -q \n",
    "! pip install geopandas -q -q -q\n",
    "! pip install matplotlib -q -q -q\n",
    "! pip install albumentations -q -q -q\n",
    "! pip install pytorch_lightning -q -q -q\n",
    "!pip install mlflow -q -q -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524c5b49-9f48-4092-a43a-ad47710a6f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.satellite_image import SatelliteImage\n",
    "from utils.utils import *\n",
    "from utils.plot_utils import *\n",
    "\n",
    "import yaml\n",
    "import re\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2\n",
    "from PIL import Image as im\n",
    "\n",
    "from datetime import date\n",
    "import re\n",
    "import pyproj\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils.labeler import RILLabeler\n",
    "from utils.filter import is_too_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45654cf2-d2ef-4ae5-bf12-5a008ff640c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/onyxia/work/detection-habitat-spontane/notebooks/..')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = get_root_path()\n",
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d90c35-321b-44d0-a9ea-a0b8084ade17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_storage_access()\n",
    "environment = get_environment()\n",
    "\n",
    "root_path = get_root_path()\n",
    "bucket = environment[\"bucket\"]\n",
    "path_s3_cayenne_data = environment[\"sources\"][\"PLEIADES\"][2022][\"guyane\"]\n",
    "path_local_cayenne_data = os.path.join(root_path, environment[\"local-path\"][\"PLEIADES\"][2022][\"guyane\"])\n",
    "\n",
    "path_s3_pleiades_data_2022_martinique = environment[\"sources\"][\"PLEIADES\"][2022][\"martinique\"]\n",
    "path_local_pleiades_data_2022_martinique = environment[\"local-path\"][\"PLEIADES\"][2022][\"martinique\"]\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c92228ba-5dd3-4755-83e5-afa24a5b7e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.download(\n",
    "        rpath=f\"{bucket}/{path_s3_cayenne_data}\",\n",
    "        lpath=f\"{path_local_cayenne_data}\",\n",
    "        recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab4c183-2d06-4364-89da-94633ca0757a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_raster(satellite_image,directory_name,file_name):\n",
    "    \"\"\"\n",
    "    save a SatelliteImage Object into a raster file (.tif)\n",
    "\n",
    "    Args:\n",
    "        satellite_image: a SatelliteImage object representing the input image to be saved as a raster file.\n",
    "        directory_name: a string representing the name of the directory where the output file should be saved.\n",
    "        file_name: a string representing the name of the output file.\n",
    "    \"\"\"\n",
    "\n",
    "    data = satellite_image.array\n",
    "    crs  = satellite_image.crs\n",
    "    transform = satellite_image.transform\n",
    "    n_bands = satellite_image.n_bands\n",
    "\n",
    "    metadata = {\n",
    "        'dtype': data.dtype,\n",
    "        'count': n_bands,\n",
    "        'width': data.shape[2],\n",
    "        'height': data.shape[1],\n",
    "        'crs': crs,\n",
    "        'transform': transform\n",
    "    }\n",
    "    \n",
    "    #print(os.path.exists(directory_name))\n",
    "    if not os.path.exists(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "\n",
    "    # Save the array as a raster file in jp2 format\n",
    "    with rasterio.open(directory_name + \"/\" + file_name, 'w', **metadata) as dst:\n",
    "        dst.write(data, indexes = np.arange(n_bands)+1)\n",
    "\n",
    "\n",
    "def write_splitted_images_masks(file_path,output_directory_name,labeler,tile_size,n_bands, dep):\n",
    "    \n",
    "    \"\"\"\n",
    "    write the couple images mask into a specific folder\n",
    "\n",
    "    Args:\n",
    "        file_path: a string representing the path to the directory containing the input image files.\n",
    "        output_directory_name: a string representing the name of the output directory where the split images and masks should be saved.\n",
    "        labeler: a Labeler object representing the labeler used to create segmentation labels.\n",
    "        tile_size: an integer representing the size of the tiles to split the input image into.\n",
    "        n_bands: an integer representing the number of bands in the input image.\n",
    "        dep: a string representing the department of the input image, or None if not applicable.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_images_path  = output_directory_name + \"/images\"\n",
    "    output_masks_path  = output_directory_name + \"/labels\"\n",
    "    \n",
    "    if not os.path.exists(output_masks_path):\n",
    "        os.makedirs(output_masks_path)\n",
    "        \n",
    "    list_name = os.listdir(file_path)\n",
    "    list_path = [file_path + \"/\" + name for name in list_name]\n",
    "    \n",
    "    for path, file_name in zip(list_path,tqdm(list_name)): # tqdm ici \n",
    "\n",
    "        big_satellite_image = SatelliteImage.from_raster(\n",
    "            file_path = path,\n",
    "            dep = None,\n",
    "            date = None,\n",
    "            n_bands= 3\n",
    "        )\n",
    "\n",
    "        list_satellite_image = big_satellite_image.split(tile_size)\n",
    "        list_satellite_image = [im for im in list_satellite_image if not is_too_black(im)]\n",
    "        # mettre le filtre is too black ici !!!\n",
    "        for i, satellite_image in enumerate(list_satellite_image):\n",
    "                \n",
    "                file_name_i = file_name.split(\".\")[0]+\"_\"+str(i)\n",
    "                to_raster(satellite_image,output_images_path,file_name_i + \".tif\")\n",
    "                \n",
    "                # mask\n",
    "                mask = labeler.create_segmentation_label(satellite_image) \n",
    "                np.save(output_masks_path+\"/\"+file_name_i+\".npy\",mask) # save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b673b6-83d8-4a97-9c25-9af4458d077c",
   "metadata": {},
   "source": [
    "### Test sur une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bcba4f9-9ffb-49c6-91b7-4bf83514ad67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "satellite_image = SatelliteImage.from_raster(\n",
    "        file_path = f\"{path_local_cayenne_data}\"+ \"/ORT_2022072050325085_0352_0545_U22N_16Bits.jp2\",\n",
    "        dep = None,\n",
    "        date = None,\n",
    "        n_bands= 4)\n",
    "\n",
    "print(satellite_image.array.shape)\n",
    "i = 2\n",
    "directory_name = \"../splitted_data\"\n",
    "file_name = \"ORT_2022072050325085_0352_0545_U22N_16Bits\"+\"_\"+str(i)+\".tif\"\n",
    "\n",
    "to_raster(satellite_image,directory_name,file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8440798-f11c-4261-a8e0-ad2c17cbaf58",
   "metadata": {},
   "source": [
    "## Test sur l'ensemble des images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e4631-0148-411f-a1b6-8b80f013458b",
   "metadata": {},
   "source": [
    "- je prends un répertoire en entrée Par exemple Guyane et je lis je split et réécris les images en taille 250\n",
    "- compter le nombre d'images à traiter et le nombre d'images à l'arrivée.\n",
    "- Si plusieurs années dispo généraliser le labeller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8840269a-d846-4962-a4d4-79e8eb0321c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/240 [00:03<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# params \n",
    "file_path = f\"{path_local_cayenne_data}\"\n",
    "tile_size = 250\n",
    "n_bands = 3\n",
    "dep =\"973\"\n",
    "filename = os.listdir(file_path)[0]\n",
    "date = datetime.strptime(re.search(r'ORT_(\\d{8})', filename).group(1), '%Y%m%d') \n",
    "labeler = RILLabeler(date, dep = dep, buffer_size = 10) \n",
    "output_directory_name = \"../splitted_data\"\n",
    "\n",
    "write_splitted_images_masks(file_path,output_directory_name,labeler,tile_size,n_bands,dep)\n",
    "\n",
    "# 1 min pour 250 -> 4min pour 1000, ça se tente un peu lionguet mais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12338605-a045-40f0-9261-6988618b22b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_directory_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43moutput_directory_name\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/labels\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(output_directory_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/images\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_directory_name' is not defined"
     ]
    }
   ],
   "source": [
    "len(os.listdir(output_directory_name+\"/labels\"))\n",
    "len(os.listdir(output_directory_name+\"/images\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea14795-2b15-405e-9996-c6b41776baae",
   "metadata": {},
   "source": [
    "- a priori si je connais le file path : je connais la date et je connais le labeller => créer labeller en amont ?\n",
    "- On crée un labeller par date et par territoire concerné \n",
    "- On créé les masques et on les sauvegarde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a045d-bfa4-498e-b36e-edf16850fc97",
   "metadata": {
    "tags": []
   },
   "source": [
    " ### Train un modèle de segmentation Réadaptation de la classe DeeplaV3module pour la rendre agnostique  au dataset etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d1c3e2-f354-4311-85e5-b6cb34f1d316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%reload_ext autoreload\n",
    "import utils.utils\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import gc\n",
    "import albumentations as album\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "## from src\n",
    "from datas.components.dataset import PleiadeDataset, ChangeDetectionS2LookingDataset\n",
    "from models.components.segmentation_models import DeepLabv3Module\n",
    "from models.segmentation_module import SegmentationModule\n",
    "from datas.datamodule import DataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ead3340-46dc-46c5-ac2b-4a5f65e76d73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFLOW_S3_ENDPOINT_URL=https://minio.lab.sspcloud.fr\n"
     ]
    }
   ],
   "source": [
    "update_storage_access()\n",
    "#%env AWS_ACCESS_KEY_ID=projet-slums-sa\n",
    "#%env AWS_SECRET_ACCESS_KEY=hB2N6hCmp7JoFA6WHKT022WJ9lOc1oOr # chercher ça dans secret ? normalement le update storage access suffit\n",
    "#%env AWS_S3_ENDPOINT=minio.lab.sspcloud.fr\n",
    "%env MLFLOW_S3_ENDPOINT_URL=https://minio.lab.sspcloud.fr\n",
    "# à préciser en +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0836baae-9103-4627-9978-36c1441467d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install mlflow -q -q -q\n",
    "import mlflow\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f24547eb-a00e-43b5-a986-ae47d8c83273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/14 10:55:04 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "#!pip install mlflow\n",
    "import mlflow\n",
    "mlflow.end_run()\n",
    "\n",
    "run_name = \"modele deeplabV354\"\n",
    "remote_server_uri = \"https://projet-slums-detection-386760.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"segmentation\"\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "\n",
    "## Création Dataset\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "dir_data = \"../splitted_data\"\n",
    "list_path_labels =  np.sort([dir_data + \"/labels/\" + name for name in os.listdir(dir_data+\"/labels\")])# os.wlk dans d'autres cas sous des sous arbres de S2Looking\n",
    "list_path_images =  np.sort([dir_data + \"/images/\" + name for name in os.listdir(dir_data+\"/images\")])\n",
    "dataset = PleiadeDataset(list_path_images,list_path_labels)\n",
    "\n",
    "# liste d'images test à tester \n",
    "satellite_image = SatelliteImage.from_raster(\n",
    "        file_path = \"../data/PLEIADES/2020/MAYOTTE/ORT_2020052526670967_0524_8590_U38S_8Bits.jp2\",\n",
    "        dep = None,\n",
    "        date = None,\n",
    "        n_bands= 3) # ../data/PLEIADES/2020/MAYOTTE/ORT_2020052526670967_0524_8590_U38S_8Bits.jp2 fonctions d'affichage de Raya\n",
    "\n",
    "list_test = satellite_image.split(250)\n",
    "directory_image_name = \"../test_data/images/\"\n",
    "\n",
    "for i,simg in enumerate(list_test):\n",
    "    file_name = simg.filename.split(\".\")[0] +\"_\"+str(i)+\".tif\"\n",
    "    to_raster(simg,directory_image_name,file_name)\n",
    "\n",
    "list_path_images_test = [directory_image_name + filename for filename in os.listdir(directory_image_name)]\n",
    "list_path_labels_test =   list_path_labels[:len(list_path_images_test)]# pas propre je metsd une liste de labels de même taille inutilisés\n",
    "\n",
    "dataset_test = PleiadeDataset(list_path_images_test,list_path_labels_test) \n",
    "\n",
    "# à changer avec des images bien spécifiques et une fonction préparer test\n",
    "# faire un mini batch qui ne contient que les patchs d'uune image d'intérêt créer les fichiers iages en local et les utiliser pour reconstituer une full image !\n",
    "# print(list_path_images_test)\n",
    "\n",
    "## Transforms à mettre dans le data module\n",
    "image_size = (250,250)\n",
    "transforms_augmentation = album.Compose(\n",
    "        [\n",
    "            album.Resize(300, 300, always_apply=True),\n",
    "            album.RandomResizedCrop(\n",
    "                *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "            ),\n",
    "            album.HorizontalFlip(),\n",
    "            album.VerticalFlip(),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "       ]\n",
    "    )\n",
    "\n",
    "transforms_preprocessing = album.Compose(\n",
    "        [\n",
    "            album.Resize(*image_size, always_apply=True),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "## Instanciation modèle et paramètres d'entraînement\n",
    "\n",
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\"lr\": 0.0001, \"momentum\": 0.9}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n",
    "\n",
    "model = DeepLabv3Module()\n",
    "\n",
    "\n",
    "\n",
    "##Instanciation des datamodule et plmodule\n",
    "\n",
    "\n",
    "data_module = DataModule(\n",
    "    dataset= dataset,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1, \n",
    "    batch_size=10,\n",
    "    dataset_test = dataset_test\n",
    ")\n",
    "\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"min\", patience=3\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy =\"auto\"\n",
    "list_callbacks = [lr_monitor, checkpoint_callback, early_stop_callback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94a915-cd4a-4819-88ad-45f690e4153f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023/04/14 10:54:25 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/mamba/lib/python3.10/site-packages/mlflow/pytorch/_lightning_autolog.py:352: UserWarning: Autologging is known to be compatible with pytorch-lightning versions between 1.0.5 and 1.9.3 and may not succeed with packages outside this range.\"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | DeepLabv3Module  | 61.0 M\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "61.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "61.0 M    Total params\n",
      "243.965   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f09244c40fa49b09e8afeebd56345a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "    callbacks= list_callbacks,\n",
    "    max_epochs=50,\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    "    )\n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd1b8d-f7d7-4671-ad65-0a6ea8be0244",
   "metadata": {},
   "source": [
    "## Test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c8b68-e33c-450a-a448-987769cc786f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of models.segmentation_module failed: Traceback (most recent call last):\n",
      "  File \"/opt/mamba/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/opt/mamba/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/mamba/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/onyxia/work/detection-habitat-spontane/notebooks/../src/models/segmentation_module.py\", line 130\n",
      "    plot_file = \"img/\"+pthimg+idx\".png\"\n",
      "                                 ^^^^^^\n",
      "SyntaxError: invalid syntax\n",
      "]\n",
      "2023/04/14 11:01:01 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coucou\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc789b51e60412aa5599d8a4b7232ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#trainer.checkpoint_callback.best_model_path\n",
    "mlflow.end_run()\n",
    "\n",
    "run_name = \"modele deeplabV35\"\n",
    "remote_server_uri = \"https://projet-slums-detection-386760.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"segmentation\"\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "output_ligthning = '~/work/detection-habitat-spontane/notebooks/lightning_logs/'\n",
    "\n",
    "with mlflow.start_run(run_id=\"eadf304d0f3e400fbe9e7919dd4bc698\"):\n",
    "    model = DeepLabv3Module()\n",
    "    lightning_module_checkpoint = lightning_module.load_from_checkpoint(\n",
    "    checkpoint_path=output_ligthning+'version_11/checkpoints/epoch=35-step=38196.ckpt',\n",
    "    model= model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval\n",
    "                                         )\n",
    "    #artifact_path = \"modele_segmentation_deeplabv3\"\n",
    "    #mlflow.pytorch.log_model(lightning_module_checkpoint.model, artifact_path)\n",
    "\n",
    "    print(\"coucou\")\n",
    "    trainer = pl.Trainer(\n",
    "    callbacks= list_callbacks,\n",
    "    max_epochs=50,\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    "    )\n",
    "    trainer.test(lightning_module_checkpoint, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4a8f4-33c5-4968-bd68-7eab9cde38a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.test(lightning_module, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757ef55-0d17-4141-8c99-eac1960b9c62",
   "metadata": {},
   "source": [
    "### Test du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165ff6d-969d-4343-9fc2-53b2891c3071",
   "metadata": {},
   "source": [
    "TO DO :\n",
    "- dégager aussi les RIL vides ?\n",
    "- test de lancement via invit de comande train\n",
    "- généraliser la création de liste de file path selon le dataset souhaité\n",
    "- créer un yaml de config et le logger dans mlflow\n",
    "- contrîole sur le modèle qui est loggé dans ml flow ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88ee46-2bb2-4031-ae82-feaf056955ea",
   "metadata": {},
   "source": [
    "## S2Looking training !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a8e0e-8e3f-412a-b0ff-1b4adc834bda",
   "metadata": {},
   "source": [
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec8dae-414f-4551-bee2-8138815235c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_storage_access()\n",
    "environment = get_environment()\n",
    "root_path = get_root_path()\n",
    "bucket = environment[\"bucket\"]\n",
    "path_s3_s2looking = environment[\"sources\"][\"PAPERS\"][\"S2Looking\"]\n",
    "path_local_s2looking = environment[\"local-path\"][\"PAPERS\"]\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "\n",
    "fs.download(\n",
    "        rpath=f\"{bucket}/{path_s3_s2looking}\",\n",
    "        lpath=f\"../{path_local_s2looking}\",\n",
    "        recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1dc5c-b4a2-4dba-b669-595b7673c0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(f\"../{path_local_s2looking}/S2Looking.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(f\"../{path_local_s2looking}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c371e3-b977-4e74-8891-b87ac1d28a14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/06 18:30:45 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | DeepLabv3Module  | 61.0 M\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "61.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "61.0 M    Total params\n",
      "244.003   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da72064af79d4173806673efbe58e2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install mlflow\n",
    "import mlflow\n",
    "\n",
    "run_name = \"s2looking\" # config\n",
    "remote_server_uri = \"https://projet-slums-detection-2439.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"segmentation\" # config\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "# mlflow.pytorch.autolog() # logger la config\n",
    "\n",
    "\n",
    "## Création Dataset\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "## En faire une fonction ? : Idée faire une classe de préparation des données pour chaque data set chargé qui in fine sortirait une list path ?\n",
    "\n",
    "### config, appeler une fionction de création de dataset ?\n",
    "dir_data = \"../data/paper_dataset/S2Looking/\"\n",
    "img1_train = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "img1_val = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "\n",
    "img1_train = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "img1_val = [dir_data + \"val/Image1/\"+name for name in os.listdir(dir_data + \"/val/Image1\")]\n",
    "img1_test = [dir_data + \"test/Image1/\"+name for name in os.listdir(dir_data + \"/test/Image1\")]\n",
    "\n",
    "img2_train = [dir_data + \"train/Image2/\"+name for name in os.listdir(dir_data + \"/train/Image2\")]\n",
    "img2_val = [dir_data + \"val/Image2/\"+name for name in os.listdir(dir_data + \"/val/Image2\")]\n",
    "img2_test = [dir_data + \"test/Image2/\"+name for name in os.listdir(dir_data + \"/test/Image2\")]\n",
    "\n",
    "label_train = [dir_data + \"train/label/\"+name for name in os.listdir(dir_data + \"/train/label\")]\n",
    "label_val = [dir_data + \"val/label/\"+name for name in os.listdir(dir_data + \"/val/label\")]\n",
    "label_test = [dir_data + \"test/label/\"+name for name in os.listdir(dir_data + \"/test/label\")]\n",
    "\n",
    "img1_path = np.concatenate([np.sort(img1_train),np.sort(img1_val),np.sort(img1_test)])\n",
    "img2_path = np.concatenate([np.sort(img2_train),np.sort(img2_val),np.sort(img2_test)])\n",
    "label_path = np.concatenate([np.sort(label_train),np.sort(label_val),np.sort(label_test)])\n",
    "\n",
    "mono_image_dataset = ChangeDetectionS2LookingDataset(img1_path,img2_path,label_path)\n",
    "\n",
    "# transforms incorporé dans les dataset a posteriori\n",
    "image_size = (256,256) # cf la classe dataset\n",
    "transforms_augmentation = album.Compose(\n",
    "        [\n",
    "            album.Resize(300, 300, always_apply=True),\n",
    "            album.RandomResizedCrop(\n",
    "                *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "            ),\n",
    "            album.HorizontalFlip(),\n",
    "            album.VerticalFlip(),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "       ]\n",
    "    )\n",
    "\n",
    "transforms_preprocessing = album.Compose(\n",
    "        [\n",
    "            album.Resize(*image_size, always_apply=True),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    ")\n",
    "\n",
    "\n",
    "## Instanciation modèle et paramètres d'entraînement\n",
    "\n",
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\"lr\": 0.0001, \"momentum\": 0.9}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n",
    "\n",
    "model = DeepLabv3Module(nchannel = 6) # viens du data set qui concatene 2 images à 3 channels\n",
    "\n",
    "##Instanciation des datamodule et plmodule\n",
    "\n",
    "data_module = DataModule(\n",
    "    mono_image_dataset= mono_image_dataset,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1, \n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback_2 = ModelCheckpoint(\n",
    "    monitor=\"train_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"min\", patience=3\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy =\"auto\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "    callbacks=[lr_monitor, checkpoint_callback,checkpoint_callback_2, early_stop_callback],\n",
    "    max_epochs=100,\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    "    )\n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n",
    "    \n",
    "    lightning_module_checkpoint = lightning_module.load_from_checkpoint(\n",
    "    checkpoint_path=trainer.checkpoint_callback.best_model_path\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval\n",
    "                                     )\n",
    "    artifact_path = \"models/modele_change_detection_deeplabv3_on_s2_looking\"\n",
    "    mlflow.pytorch.log_model(lightning_module_checkpoint.model, artifact_path)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
