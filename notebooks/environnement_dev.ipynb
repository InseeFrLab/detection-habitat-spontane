{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e064d8b4-2af2-4263-800f-713875386be0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fonction to Raster\n",
    "Prend en entrée une Satellite Image, un dossier et un nom et la sauve en JP2 dans le dossier considéré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b683696-509c-466e-bdee-73d3aba6e3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install pyarrow -q -q -q \n",
    "! pip install rasterio -q -q -q \n",
    "! pip install geopandas -q -q -q\n",
    "! pip install matplotlib -q -q -q\n",
    "! pip install albumentations -q -q -q\n",
    "! pip install pytorch_lightning -q -q -q\n",
    "!pip install mlflow -q -q -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524c5b49-9f48-4092-a43a-ad47710a6f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.satellite_image import SatelliteImage\n",
    "from utils.utils import *\n",
    "from utils.plot_utils import *\n",
    "\n",
    "import yaml\n",
    "import re\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2\n",
    "from PIL import Image as im\n",
    "\n",
    "from datetime import date\n",
    "import re\n",
    "import pyproj\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils.labeler import RILLabeler\n",
    "from utils.filter import is_too_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45654cf2-d2ef-4ae5-bf12-5a008ff640c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/onyxia/work/detection-habitat-spontane/notebooks/..')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = get_root_path()\n",
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d90c35-321b-44d0-a9ea-a0b8084ade17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_storage_access()\n",
    "environment = get_environment()\n",
    "\n",
    "root_path = get_root_path()\n",
    "bucket = environment[\"bucket\"]\n",
    "path_s3_cayenne_data = environment[\"sources\"][\"PLEIADES\"][2022][\"guyane\"]\n",
    "path_local_cayenne_data = os.path.join(root_path, environment[\"local-path\"][\"PLEIADES\"][2022][\"guyane\"])\n",
    "\n",
    "path_s3_pleiades_data_2022_martinique = environment[\"sources\"][\"PLEIADES\"][2022][\"martinique\"]\n",
    "path_local_pleiades_data_2022_martinique = environment[\"local-path\"][\"PLEIADES\"][2022][\"martinique\"]\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c92228ba-5dd3-4755-83e5-afa24a5b7e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.download(\n",
    "        rpath=f\"{bucket}/{path_s3_cayenne_data}\",\n",
    "        lpath=f\"{path_local_cayenne_data}\",\n",
    "        recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab4c183-2d06-4364-89da-94633ca0757a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_raster(satellite_image,directory_name,file_name):\n",
    "    \"\"\"\n",
    "    save a SatelliteImage Object into a raster file (.tif)\n",
    "\n",
    "    Args:\n",
    "        satellite_image: a SatelliteImage object representing the input image to be saved as a raster file.\n",
    "        directory_name: a string representing the name of the directory where the output file should be saved.\n",
    "        file_name: a string representing the name of the output file.\n",
    "    \"\"\"\n",
    "\n",
    "    data = satellite_image.array\n",
    "    crs  = satellite_image.crs\n",
    "    transform = satellite_image.transform\n",
    "    n_bands = satellite_image.n_bands\n",
    "\n",
    "    metadata = {\n",
    "        'dtype': data.dtype,\n",
    "        'count': n_bands,\n",
    "        'width': data.shape[2],\n",
    "        'height': data.shape[1],\n",
    "        'crs': crs,\n",
    "        'transform': transform\n",
    "    }\n",
    "    \n",
    "    #print(os.path.exists(directory_name))\n",
    "    if not os.path.exists(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "\n",
    "    # Save the array as a raster file in jp2 format\n",
    "    with rasterio.open(directory_name + \"/\" + file_name, 'w', **metadata) as dst:\n",
    "        dst.write(data, indexes = np.arange(n_bands)+1)\n",
    "\n",
    "\n",
    "def write_splitted_images_masks(file_path,output_directory_name,labeler,tile_size,n_bands, dep):\n",
    "    \n",
    "    \"\"\"\n",
    "    write the couple images mask into a specific folder\n",
    "\n",
    "    Args:\n",
    "        file_path: a string representing the path to the directory containing the input image files.\n",
    "        output_directory_name: a string representing the name of the output directory where the split images and masks should be saved.\n",
    "        labeler: a Labeler object representing the labeler used to create segmentation labels.\n",
    "        tile_size: an integer representing the size of the tiles to split the input image into.\n",
    "        n_bands: an integer representing the number of bands in the input image.\n",
    "        dep: a string representing the department of the input image, or None if not applicable.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_images_path  = output_directory_name + \"/images\"\n",
    "    output_masks_path  = output_directory_name + \"/labels\"\n",
    "    \n",
    "    if not os.path.exists(output_masks_path):\n",
    "        os.makedirs(output_masks_path)\n",
    "        \n",
    "    list_name = os.listdir(file_path)\n",
    "    list_path = [file_path + \"/\" + name for name in list_name]\n",
    "    \n",
    "    for path, file_name in zip(list_path,tqdm(list_name)): # tqdm ici \n",
    "\n",
    "        big_satellite_image = SatelliteImage.from_raster(\n",
    "            file_path = path,\n",
    "            dep = None,\n",
    "            date = None,\n",
    "            n_bands= 3\n",
    "        )\n",
    "\n",
    "        list_satellite_image = big_satellite_image.split(tile_size)\n",
    "        list_satellite_image = [im for im in list_satellite_image if not is_too_black(im)]\n",
    "        # mettre le filtre is too black ici !!!\n",
    "        for i, satellite_image in enumerate(list_satellite_image):\n",
    "                \n",
    "                file_name_i = file_name.split(\".\")[0]+\"_\"+str(i)\n",
    "                to_raster(satellite_image,output_images_path,file_name_i + \".tif\")\n",
    "                \n",
    "                # mask\n",
    "                mask = labeler.create_segmentation_label(satellite_image) \n",
    "                np.save(output_masks_path+\"/\"+file_name_i+\".npy\",mask) # save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b673b6-83d8-4a97-9c25-9af4458d077c",
   "metadata": {},
   "source": [
    "### Test sur une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bcba4f9-9ffb-49c6-91b7-4bf83514ad67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "satellite_image = SatelliteImage.from_raster(\n",
    "        file_path = f\"{path_local_cayenne_data}\"+ \"/ORT_2022072050325085_0352_0545_U22N_16Bits.jp2\",\n",
    "        dep = None,\n",
    "        date = None,\n",
    "        n_bands= 4)\n",
    "\n",
    "print(satellite_image.array.shape)\n",
    "i = 2\n",
    "directory_name = \"../splitted_data\"\n",
    "file_name = \"ORT_2022072050325085_0352_0545_U22N_16Bits\"+\"_\"+str(i)+\".tif\"\n",
    "\n",
    "to_raster(satellite_image,directory_name,file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8440798-f11c-4261-a8e0-ad2c17cbaf58",
   "metadata": {},
   "source": [
    "## Test sur l'ensemble des images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e4631-0148-411f-a1b6-8b80f013458b",
   "metadata": {},
   "source": [
    "- je prends un répertoire en entrée Par exemple Guyane et je lis je split et réécris les images en taille 250\n",
    "- compter le nombre d'images à traiter et le nombre d'images à l'arrivée.\n",
    "- Si plusieurs années dispo généraliser le labeller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8840269a-d846-4962-a4d4-79e8eb0321c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/240 [00:03<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# params \n",
    "file_path = f\"{path_local_cayenne_data}\"\n",
    "tile_size = 250\n",
    "n_bands = 3\n",
    "dep =\"973\"\n",
    "filename = os.listdir(file_path)[0]\n",
    "date = datetime.strptime(re.search(r'ORT_(\\d{8})', filename).group(1), '%Y%m%d') \n",
    "labeler = RILLabeler(date, dep = dep, buffer_size = 10) \n",
    "output_directory_name = \"../splitted_data\"\n",
    "\n",
    "write_splitted_images_masks(file_path,output_directory_name,labeler,tile_size,n_bands,dep)\n",
    "\n",
    "# 1 min pour 250 -> 4min pour 1000, ça se tente un peu lionguet mais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12338605-a045-40f0-9261-6988618b22b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_directory_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43moutput_directory_name\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/labels\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(output_directory_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/images\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_directory_name' is not defined"
     ]
    }
   ],
   "source": [
    "len(os.listdir(output_directory_name+\"/labels\"))\n",
    "len(os.listdir(output_directory_name+\"/images\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea14795-2b15-405e-9996-c6b41776baae",
   "metadata": {},
   "source": [
    "- a priori si je connais le file path : je connais la date et je connais le labeller => créer labeller en amont ?\n",
    "- On crée un labeller par date et par territoire concerné \n",
    "- On créé les masques et on les sauvegarde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a045d-bfa4-498e-b36e-edf16850fc97",
   "metadata": {
    "tags": []
   },
   "source": [
    " ### Train un modèle de segmentation Réadaptation de la classe DeeplaV3module pour la rendre agnostique  au dataset etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d1c3e2-f354-4311-85e5-b6cb34f1d316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%reload_ext autoreload\n",
    "import utils.utils\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import gc\n",
    "import albumentations as album\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "## from src\n",
    "from datas.components.dataset import PleiadeDataset, ChangeDetectionS2LookingDataset\n",
    "from models.components.segmentation_models import DeepLabv3Module\n",
    "from models.segmentation_module import SegmentationModule\n",
    "from datas.datamodule import DataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ead3340-46dc-46c5-ac2b-4a5f65e76d73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFLOW_S3_ENDPOINT_URL=https://minio.lab.sspcloud.fr\n"
     ]
    }
   ],
   "source": [
    "update_storage_access()\n",
    "#%env AWS_ACCESS_KEY_ID=projet-slums-sa\n",
    "#%env AWS_SECRET_ACCESS_KEY=hB2N6hCmp7JoFA6WHKT022WJ9lOc1oOr # chercher ça dans secret ? normalement le update storage access suffit\n",
    "#%env AWS_S3_ENDPOINT=minio.lab.sspcloud.fr\n",
    "%env MLFLOW_S3_ENDPOINT_URL=https://minio.lab.sspcloud.fr\n",
    "# à préciser en +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0836baae-9103-4627-9978-36c1441467d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install mlflow -q -q -q\n",
    "import mlflow\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f24547eb-a00e-43b5-a986-ae47d8c83273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/14 10:55:04 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "#!pip install mlflow\n",
    "import mlflow\n",
    "mlflow.end_run()\n",
    "\n",
    "run_name = \"modele deeplabV354\"\n",
    "remote_server_uri = \"https://projet-slums-detection-386760.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"segmentation\"\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "\n",
    "## Création Dataset\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "dir_data = \"../splitted_data\"\n",
    "list_path_labels =  np.sort([dir_data + \"/labels/\" + name for name in os.listdir(dir_data+\"/labels\")])# os.wlk dans d'autres cas sous des sous arbres de S2Looking\n",
    "list_path_images =  np.sort([dir_data + \"/images/\" + name for name in os.listdir(dir_data+\"/images\")])\n",
    "dataset = PleiadeDataset(list_path_images,list_path_labels)\n",
    "\n",
    "# liste d'images test à tester \n",
    "satellite_image = SatelliteImage.from_raster(\n",
    "        file_path = \"../data/PLEIADES/2020/MAYOTTE/ORT_2020052526670967_0524_8590_U38S_8Bits.jp2\",\n",
    "        dep = None,\n",
    "        date = None,\n",
    "        n_bands= 3) # ../data/PLEIADES/2020/MAYOTTE/ORT_2020052526670967_0524_8590_U38S_8Bits.jp2 fonctions d'affichage de Raya\n",
    "\n",
    "list_test = satellite_image.split(250)\n",
    "directory_image_name = \"../test_data/images/\"\n",
    "\n",
    "for i,simg in enumerate(list_test):\n",
    "    file_name = simg.filename.split(\".\")[0] +\"_\"+str(i)+\".tif\"\n",
    "    to_raster(simg,directory_image_name,file_name)\n",
    "\n",
    "list_path_images_test = [directory_image_name + filename for filename in os.listdir(directory_image_name)]\n",
    "list_path_labels_test =   list_path_labels[:len(list_path_images_test)]# pas propre je metsd une liste de labels de même taille inutilisés\n",
    "\n",
    "dataset_test = PleiadeDataset(list_path_images_test,list_path_labels_test) \n",
    "\n",
    "# à changer avec des images bien spécifiques et une fonction préparer test\n",
    "# faire un mini batch qui ne contient que les patchs d'uune image d'intérêt créer les fichiers iages en local et les utiliser pour reconstituer une full image !\n",
    "# print(list_path_images_test)\n",
    "\n",
    "## Transforms à mettre dans le data module\n",
    "image_size = (250,250)\n",
    "transforms_augmentation = album.Compose(\n",
    "        [\n",
    "            album.Resize(300, 300, always_apply=True),\n",
    "            album.RandomResizedCrop(\n",
    "                *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "            ),\n",
    "            album.HorizontalFlip(),\n",
    "            album.VerticalFlip(),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "       ]\n",
    "    )\n",
    "\n",
    "transforms_preprocessing = album.Compose(\n",
    "        [\n",
    "            album.Resize(*image_size, always_apply=True),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "## Instanciation modèle et paramètres d'entraînement\n",
    "\n",
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\"lr\": 0.0001, \"momentum\": 0.9}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n",
    "\n",
    "model = DeepLabv3Module()\n",
    "\n",
    "\n",
    "\n",
    "##Instanciation des datamodule et plmodule\n",
    "\n",
    "\n",
    "data_module = DataModule(\n",
    "    dataset= dataset,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1, \n",
    "    batch_size=10,\n",
    "    dataset_test = dataset_test\n",
    ")\n",
    "\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"min\", patience=3\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy =\"auto\"\n",
    "list_callbacks = [lr_monitor, checkpoint_callback, early_stop_callback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94a915-cd4a-4819-88ad-45f690e4153f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023/04/14 10:54:25 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/mamba/lib/python3.10/site-packages/mlflow/pytorch/_lightning_autolog.py:352: UserWarning: Autologging is known to be compatible with pytorch-lightning versions between 1.0.5 and 1.9.3 and may not succeed with packages outside this range.\"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | DeepLabv3Module  | 61.0 M\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "61.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "61.0 M    Total params\n",
      "243.965   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f09244c40fa49b09e8afeebd56345a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "    callbacks= list_callbacks,\n",
    "    max_epochs=50,\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    "    )\n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd1b8d-f7d7-4671-ad65-0a6ea8be0244",
   "metadata": {},
   "source": [
    "## Test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c32c8b68-e33c-450a-a448-987769cc786f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/14 12:22:12 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n",
      "2023/04/14 12:22:27 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpzl1wnvbu/model/data, flavor: pytorch), fall back to return ['torch==2.0.0', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n",
      "2023/04/14 12:22:30 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under s3://projet-slums-detection/mlflow-artifacts/1/eadf304d0f3e400fbe9e7919dd4bc698/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0464d5c00c3a49fe8f5bf5032dbf7ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "coucou\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 61 into shape (8,8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 35\u001b[0m\n\u001b[1;32m     26\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mlog_model(lightning_module_checkpoint\u001b[38;5;241m.\u001b[39mmodel, artifact_path)\n\u001b[1;32m     28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     29\u001b[0m callbacks\u001b[38;5;241m=\u001b[39m list_callbacks,\n\u001b[1;32m     30\u001b[0m max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlightning_module_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:706\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    704\u001b[0m     model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:749\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(model, test_dataloaders\u001b[38;5;241m=\u001b[39mdataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n\u001b[1;32m    746\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    748\u001b[0m )\n\u001b[0;32m--> 749\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    751\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:971\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-stage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    374\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 375\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    379\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    291\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:387\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtest_step_context():\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, TestStep)\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/detection-habitat-spontane/notebooks/../src/models/segmentation_module.py:135\u001b[0m, in \u001b[0;36mSegmentationModule.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m :\u001b[38;5;66;03m# à paramétrer\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoucou\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m     fig1 \u001b[38;5;241m=\u001b[39m \u001b[43mplot_list_segmentation_labeled_satellite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_labeled_satellite_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m#fig1 = img_label_model.plot([0,1,2])\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/work/detection-habitat-spontane/notebooks/../src/utils/plot_utils.py:111\u001b[0m, in \u001b[0;36mplot_list_segmentation_labeled_satellite_image\u001b[0;34m(list_labeled_image, bands_indices)\u001b[0m\n\u001b[1;32m    108\u001b[0m n_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39marray([bb[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m bb \u001b[38;5;129;01min\u001b[39;00m list_bounding_box])))\n\u001b[1;32m    109\u001b[0m n_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39marray([bb[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m bb \u001b[38;5;129;01min\u001b[39;00m list_bounding_box])))\n\u001b[0;32m--> 111\u001b[0m mat_list_images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_images\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_row\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    112\u001b[0m mat_list_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    113\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(list_labels)\u001b[38;5;241m.\u001b[39mreshape(n_col, n_row, tile_size, tile_size),\n\u001b[1;32m    114\u001b[0m     (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    117\u001b[0m mat_list_images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflip(np\u001b[38;5;241m.\u001b[39mtranspose(mat_list_images), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 61 into shape (8,8)"
     ]
    }
   ],
   "source": [
    "#trainer.checkpoint_callback.best_model_path\n",
    "mlflow.end_run()\n",
    "\n",
    "run_name = \"modele deeplabV35\"\n",
    "remote_server_uri = \"https://projet-slums-detection-386760.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"segmentation\"\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "output_ligthning = '~/work/detection-habitat-spontane/notebooks/lightning_logs/'\n",
    "\n",
    "with mlflow.start_run(run_id=\"eadf304d0f3e400fbe9e7919dd4bc698\"):\n",
    "    model = DeepLabv3Module()\n",
    "    lightning_module_checkpoint = lightning_module.load_from_checkpoint(\n",
    "    checkpoint_path=output_ligthning+'version_11/checkpoints/last.ckpt',\n",
    "    model= model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval\n",
    "                                         )\n",
    "    artifact_path = \"modele_segmentation_deeplabv3\"\n",
    "    mlflow.pytorch.log_model(lightning_module_checkpoint.model, artifact_path)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "    callbacks= list_callbacks,\n",
    "    max_epochs=50,\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    "    )\n",
    "    trainer.test(lightning_module_checkpoint, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4a8f4-33c5-4968-bd68-7eab9cde38a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.test(lightning_module, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757ef55-0d17-4141-8c99-eac1960b9c62",
   "metadata": {},
   "source": [
    "### Test du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165ff6d-969d-4343-9fc2-53b2891c3071",
   "metadata": {},
   "source": [
    "TO DO :\n",
    "- mettre le [0,1,2] par défaut\n",
    "- se servir du if batch idx == 10 recharger tout les fichiers des batchs précédents et en faire une satellite data, stocker également ce qui fera une seule image et donc une seule labelled satellite image ..\n",
    "- dégager aussi les RIL vides ? Oui\n",
    "- dans le filename des images splittée changer avec les bonnes coordonnées dans lenom (trouvable dans la bounding box..)\n",
    "- faire une fonction générer dataset test qui servira pour tous les tests\n",
    "- test de lancement via invit de comande train\n",
    "- généraliser la création de liste de file path selon le dataset souhaité\n",
    "- créer un yaml de config et le logger dans mlflow\n",
    "- contrîole sur le modèle qui est loggé dans ml flow ?\n",
    "- resoudre le bug sur l'ambiguité du nombre d'elemeents dans le batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88ee46-2bb2-4031-ae82-feaf056955ea",
   "metadata": {},
   "source": [
    "## S2Looking training !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a8e0e-8e3f-412a-b0ff-1b4adc834bda",
   "metadata": {},
   "source": [
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec8dae-414f-4551-bee2-8138815235c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_storage_access()\n",
    "environment = get_environment()\n",
    "root_path = get_root_path()\n",
    "bucket = environment[\"bucket\"]\n",
    "path_s3_s2looking = environment[\"sources\"][\"PAPERS\"][\"S2Looking\"]\n",
    "path_local_s2looking = environment[\"local-path\"][\"PAPERS\"]\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "\n",
    "fs.download(\n",
    "        rpath=f\"{bucket}/{path_s3_s2looking}\",\n",
    "        lpath=f\"../{path_local_s2looking}\",\n",
    "        recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1dc5c-b4a2-4dba-b669-595b7673c0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(f\"../{path_local_s2looking}/S2Looking.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(f\"../{path_local_s2looking}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39c371e3-b977-4e74-8891-b87ac1d28a14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (262417337.py, line 122)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 122\u001b[0;36m\u001b[0m\n\u001b[0;31m    checkpoint_path=trainer.checkpoint_callback.best_model_path\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "#!pip install mlflow\n",
    "import mlflow\n",
    "\n",
    "run_name = \"s2looking\" # config\n",
    "remote_server_uri = \"https://projet-slums-detection-2439.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"segmentation\" # config\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "# mlflow.pytorch.autolog() # logger la config\n",
    "\n",
    "\n",
    "## Création Dataset\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "## En faire une fonction ? : Idée faire une classe de préparation des données pour chaque data set chargé qui in fine sortirait une list path ?\n",
    "\n",
    "### config, appeler une fionction de création de dataset ?\n",
    "dir_data = \"../data/paper_dataset/S2Looking/\"\n",
    "img1_train = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "img1_val = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "\n",
    "img1_train = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "img1_val = [dir_data + \"val/Image1/\"+name for name in os.listdir(dir_data + \"/val/Image1\")]\n",
    "img1_test = [dir_data + \"test/Image1/\"+name for name in os.listdir(dir_data + \"/test/Image1\")]\n",
    "\n",
    "img2_train = [dir_data + \"train/Image2/\"+name for name in os.listdir(dir_data + \"/train/Image2\")]\n",
    "img2_val = [dir_data + \"val/Image2/\"+name for name in os.listdir(dir_data + \"/val/Image2\")]\n",
    "img2_test = [dir_data + \"test/Image2/\"+name for name in os.listdir(dir_data + \"/test/Image2\")]\n",
    "\n",
    "label_train = [dir_data + \"train/label/\"+name for name in os.listdir(dir_data + \"/train/label\")]\n",
    "label_val = [dir_data + \"val/label/\"+name for name in os.listdir(dir_data + \"/val/label\")]\n",
    "label_test = [dir_data + \"test/label/\"+name for name in os.listdir(dir_data + \"/test/label\")]\n",
    "\n",
    "img1_path = np.concatenate([np.sort(img1_train),np.sort(img1_val),np.sort(img1_test)])\n",
    "img2_path = np.concatenate([np.sort(img2_train),np.sort(img2_val),np.sort(img2_test)])\n",
    "label_path = np.concatenate([np.sort(label_train),np.sort(label_val),np.sort(label_test)])\n",
    "\n",
    "mono_image_dataset = ChangeDetectionS2LookingDataset(img1_path,img2_path,label_path)\n",
    "\n",
    "# transforms incorporé dans les dataset a posteriori\n",
    "image_size = (256,256) # cf la classe dataset\n",
    "transforms_augmentation = album.Compose(\n",
    "        [\n",
    "            album.Resize(300, 300, always_apply=True),\n",
    "            album.RandomResizedCrop(\n",
    "                *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "            ),\n",
    "            album.HorizontalFlip(),\n",
    "            album.VerticalFlip(),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "       ]\n",
    "    )\n",
    "\n",
    "transforms_preprocessing = album.Compose(\n",
    "        [\n",
    "            album.Resize(*image_size, always_apply=True),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    ")\n",
    "\n",
    "\n",
    "## Instanciation modèle et paramètres d'entraînement\n",
    "\n",
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\"lr\": 0.0001, \"momentum\": 0.9}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n",
    "\n",
    "model = DeepLabv3Module(nchannel = 6) # viens du data set qui concatene 2 images à 3 channels\n",
    "\n",
    "##Instanciation des datamodule et plmodule\n",
    "\n",
    "data_module = DataModule(\n",
    "    mono_image_dataset= mono_image_dataset,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1, \n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback_2 = ModelCheckpoint(\n",
    "    monitor=\"train_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"min\", patience=3\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy =\"auto\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "    callbacks=[lr_monitor, checkpoint_callback,checkpoint_callback_2, early_stop_callback],\n",
    "    max_epochs=100,\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    "    )\n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n",
    "    \n",
    "    lightning_module_checkpoint = lightning_module.load_from_checkpoint(\n",
    "    checkpoint_path=trainer.checkpoint_callback.best_model_path\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval\n",
    "                                     )\n",
    "    artifact_path = \"models/modele_change_detection_deeplabv3_on_s2_looking\"\n",
    "    mlflow.pytorch.log_model(lightning_module_checkpoint.model, artifact_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef23363-3a3c-4b83-8e09-303fce3b4647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
