{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e064d8b4-2af2-4263-800f-713875386be0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fonction to Raster\n",
    "Prend en entrée une Satellite Image, un dossier et un nom et la sauve en JP2 dans le dossier considéré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b683696-509c-466e-bdee-73d3aba6e3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install pyarrow -q -q -q \n",
    "! pip install rasterio -q -q -q \n",
    "! pip install geopandas -q -q -q\n",
    "! pip install matplotlib -q -q -q\n",
    "! pip install albumentations -q -q -q\n",
    "! pip install pytorch_lightning -q -q -q\n",
    "!pip install mlflow -q -q -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f1ba902-f234-49db-b4f6-937d29b0a24d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from utils.gestion_data import load_pleiade_data, write_splitted_images_masks, build_dataset_test, instantiate_module\n",
    "from utils.utils import update_storage_access\n",
    "from utils.labeler import RILLabeler\n",
    "from datas.components.dataset import PleiadeDataset\n",
    "\n",
    "import albumentations as album\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torch\n",
    "\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "from models.segmentation_module import SegmentationModule\n",
    "from datas.datamodule import DataModule\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a1fa2-f0b5-44ee-a2d0-6ed206a025b9",
   "metadata": {},
   "source": [
    "#### Définition fichier config\n",
    "Ectrire ça en yaml une fois figé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2b5bf-1d44-4fff-9885-af2f454a4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = { \n",
    "    \"tile size\" : 250,\n",
    "    \"source train\": \"S2LOOKING\",\n",
    "    \"type labeler\" : \"RIL\", # None if source train != PLEIADE\n",
    "    \"buffer size\" : 10, # None if BDTOPO\n",
    "    \"year\"  : 2022,\n",
    "    \"territory\" : \"martinique\",\n",
    "    \"dep\" : \"972\",\n",
    "    \"n bands\" : 3\n",
    "}\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7c6df94-cce9-4180-a942-78ded018e884",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'run name': 'entrainement martinique deeplab v3', 'remote server uri': 'https://projet-slums-detection-386760.user.lab.sspcloud.fr', 'experiment name': 'segmentation', 'tile size': 250, 'source train': 'PLEIADE', 'type labeler': 'RIL', 'buffer size': 10, 'year': 2022, 'territory': 'martinique', 'dep': '972', 'n bands': 3}\n",
      "{'lr': 0.0001, 'momentum': 0.9, 'module': 'deeplabv3', 'batch size': 9}\n"
     ]
    }
   ],
   "source": [
    "config = { \n",
    "    \"run name\" : \"entrainement martinique deeplab v3\",\n",
    "    \"remote server uri\":\"https://projet-slums-detection-386760.user.lab.sspcloud.fr\",\n",
    "    \"experiment name\":\"segmentation\",\n",
    "    \"tile size\" : 250,\n",
    "    \"source train\": \"PLEIADE\",\n",
    "    \"type labeler\" : \"RIL\", # None if source train != PLEIADE\n",
    "    \"buffer size\" : 10, # None if BDTOPO\n",
    "    \"year\"  : 2022,\n",
    "    \"territory\" : \"martinique\",\n",
    "    \"dep\" : \"972\",\n",
    "    \"n bands\" : 3\n",
    "}\n",
    "\n",
    "config_train = { \n",
    "    \"lr\": 0.0001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"module\" : \"deeplabv3\",\n",
    "    \"batch size\" : 9,\n",
    "    \"max epochs\" : 100\n",
    "}\n",
    "\n",
    "print(config)\n",
    "print(config_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8840269a-d846-4962-a4d4-79e8eb0321c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFLOW_S3_ENDPOINT_URL=https://minio.lab.sspcloud.fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/17 14:55:06 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "# params \n",
    "tile_size = config[\"tile size\"]\n",
    "n_bands = config[\"n bands\"]\n",
    "dep = config[\"dep\"]\n",
    "territory = config[\"territory\"]\n",
    "year = config[\"year\"]\n",
    "buffer_size = config[\"buffer size\"] \n",
    "source_train = config[\"source train\"] \n",
    "type_labeler = config[\"type labeler\"] \n",
    "\n",
    "module = config_train[\"module\"]\n",
    "batch_size = config_train[\"batch size\"]\n",
    "\n",
    "train_directory_name = \"../splitted_data\"\n",
    "\n",
    "update_storage_access()\n",
    "%env MLFLOW_S3_ENDPOINT_URL=https://minio.lab.sspcloud.fr\n",
    "# DL des données du teritoire dont on se sert pour l'entraînement\n",
    "\n",
    "if source_train == \"PLEIADE\":\n",
    "    \n",
    "    # Plus tard décliner avec change detection etc..\n",
    "    dataset_train =  build_dataset_train(year,territory,type_labeler,train_directory_name,PleiadeDataset)\n",
    "    load_pleiade_data(2020,\"mayotte\")\n",
    "    dataset_test = build_dataset_test(\"../data/PLEIADES/2020/MAYOTTE/ORT_2020052526670967_0519_8586_U38S_8Bits.jp2\",3,250,labeler)\n",
    "    image_size = (tile_size,tile_size)\n",
    "    \n",
    "transforms_augmentation = album.Compose(\n",
    "        [\n",
    "            album.Resize(300, 300, always_apply=True),\n",
    "            album.RandomResizedCrop(\n",
    "                *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "            ),\n",
    "            album.HorizontalFlip(),\n",
    "            album.VerticalFlip(),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "       ]\n",
    "    )\n",
    "\n",
    "transforms_preprocessing = album.Compose(\n",
    "        [\n",
    "            album.Resize(*image_size, always_apply=True),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    ")\n",
    "\n",
    "## Instanciation modèle et paramètres d'entraînement\n",
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\"lr\": config_train[\"lr\"], \"momentum\":  config_train[\"momentum\"]}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n",
    "\n",
    "model = instantiate_module(module)\n",
    "\n",
    "data_module = DataModule(\n",
    "    dataset= dataset_train,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1, \n",
    "    batch_size= batch_size,\n",
    "    dataset_test = dataset_test\n",
    ")\n",
    "\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_IOU\", save_top_k=1, save_last=True, mode=\"max\"\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_IOU\", mode=\"min\", patience=3\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy =\"auto\"\n",
    "list_callbacks = [lr_monitor, checkpoint_callback, early_stop_callback]\n",
    "\n",
    "#!pip install mlflow\n",
    "mlflow.end_run()\n",
    "\n",
    "run_name = config[\"run name\"]\n",
    "remote_server_uri = config[\"remote server uri\"]\n",
    "experiment_name = config[\"experiment name\"] \n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "    callbacks= list_callbacks,\n",
    "    max_epochs=config_train[\"max epochs\"],\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    "    )\n",
    "    \n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n",
    "    \n",
    "    print(\"test\")\n",
    "    trainer.test(lightning_module , datamodule= data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd1b8d-f7d7-4671-ad65-0a6ea8be0244",
   "metadata": {},
   "source": [
    "## Test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c8b68-e33c-450a-a448-987769cc786f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trainer.checkpoint_callback.best_model_path\n",
    "import mlflow\n",
    "mlflow.end_run()\n",
    "\n",
    "run_name = \"modele deeplabV38\"\n",
    "remote_server_uri = \"https://projet-slums-detection-386760.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"segmentation\"\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "output_ligthning = 'lightning_logs/'\n",
    "\n",
    "with mlflow.start_run(run_id=\"0c7742ac29844f95ab49b05dc9c4842f\"):\n",
    "    model = DeepLabv3Module()\n",
    "    \n",
    "    lightning_module_checkpoint = lightning_module.load_from_checkpoint(\n",
    "    checkpoint_path=output_ligthning+'version_12/checkpoints/epoch=34-step=24815.ckpt',\n",
    "    model= model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval\n",
    "                                         )\n",
    "    #artifact_path = \"modele_segmentation_deeplabv3\"\n",
    "    #mlflow.pytorch.log_model(lightning_module_checkpoint.model, artifact_path)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "    callbacks= list_callbacks,\n",
    "    max_epochs=50,\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    "    )\n",
    "    trainer.test(lightning_module_checkpoint, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceee4ee-fd85-410b-8faf-c847f49fcd68",
   "metadata": {},
   "source": [
    "\n",
    "- Ici j'évalue un batch à la mano à mettre dans evaluation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2d75f-0f8c-4795-8a69-3ba0d7440b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_quick(datasetparam,model,idx):\n",
    "    imagesat = SatelliteImage.from_raster(datasetparam.list_paths_images[idx],\"976\",n_bands = 3) \n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(\"cuda\")\n",
    "    #image, mask, dic = dataset_test[idx]\n",
    "    image, mask, dic = datasetparam[idx]\n",
    "\n",
    "    # mask réel\n",
    "    plt.imshow(mask, cmap='binary', interpolation='none')\n",
    "    plt.colorbar(ticks=[0, 1])\n",
    "    plt.show()\n",
    "\n",
    "    # image\n",
    "    imagesat.plot([0,1,2])\n",
    "\n",
    "    # model\n",
    "    output = model(image.unsqueeze(0).to(\"cuda\"))\n",
    "    print(output.shape)\n",
    "    mask_model = np.array(torch.argmax(output,axis= 1).squeeze(0).to(\"cpu\"))\n",
    "    print(np.sum(mask_model))\n",
    "    # mask model\n",
    "    plt.imshow(mask_model, cmap='binary', interpolation='none')\n",
    "    plt.colorbar(ticks=[0, 1])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8efd93db-61ff-426f-a1db-8e51f287c9b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_quick' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_quick\u001b[49m(dataset,model,\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      2\u001b[0m evaluate_quick(dataset_test,model,\u001b[38;5;241m49\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_quick' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate_quick(dataset,model,70)\n",
    "evaluate_quick(dataset_test,model,49)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9202a80-55fa-4b10-91fb-37cfceab8f0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "- marrant de voir comment la normalisation detecte les logements..sérendipité\n",
    "- qgis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a7f82f-a290-4432-9b37-605d0d30b6d8",
   "metadata": {},
   "source": [
    "## gestion MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37039398-a9ca-4659-842b-69acc05136bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plots/image_test.png']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow import MlflowClient\n",
    "run = mlflow.get_run(run_id=\"0c7742ac29844f95ab49b05dc9c4842f\")\n",
    "#r.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "artifacts = [f.path for f in MlflowClient().list_artifacts(run.info.run_id, \"plots\")]\n",
    "#print_auto_logged_info()\n",
    "artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6f63b29-2cbd-4bf6-a934-177d99ea5d08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plots/image_test.png'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  MlflowClient().list_artifacts(run.info.run_id) ok pour lister les sous dossiers\n",
    "MlflowClient().list_artifacts(run.info.run_id)\n",
    "artifacts = [f.path for f in MlflowClient().list_artifacts(run.info.run_id, \"plots\")]\n",
    "artifacts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8719aeb8-740a-4328-b210-0e44fb3b5c63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 0c7742ac29844f95ab49b05dc9c4842f\n",
      "params: {'monitor': 'validation_loss', 'mode': 'min', 'patience': '3', 'min_delta': '-0.0', 'stopped_epoch': '0', 'epochs': '100', 'optimizer_name': 'SGD', 'lr': '0.0001', 'momentum': '0.9', 'dampening': '0', 'weight_decay': '0', 'nesterov': 'False', 'maximize': 'False', 'foreach': 'None', 'differentiable': 'False'}\n",
      "metrics: {'train_loss': 0.24770013988018036, 'train_loss_epoch': 0.24770013988018036, 'train_loss_step': 0.28203338384628296, 'validation_IOU': 0.49895596504211426, 'validation_loss': 0.2548619508743286, 'stopped_epoch': 34.0, 'restored_epoch': 31.0, 'best_score': 0.2544165551662445, 'wait_count': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"run_id: {}\".format(run.info.run_id))\n",
    "print(\"params: {}\".format(run.data.params))\n",
    "print(\"metrics: {}\".format(run.data.metrics))\n",
    "\n",
    "path = mlflow.artifacts.download_artifacts(run_id = run.info.run_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0d3781a-4231-443b-9ff0-17c9be0cbfff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpf90n2ev1/'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path # c'est le ^path où  sont téléchargés les artefacts en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f749da8-3249-4e45-8685-f6fae2a1914b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "os.listdir(path+\"plots/\")\n",
    "im = Image.open(path+\"plots/image_test.png\")\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682003e-2535-43c6-8433-0c041cb5abf6",
   "metadata": {},
   "source": [
    "**Save with pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165ff6d-969d-4343-9fc2-53b2891c3071",
   "metadata": {},
   "source": [
    "TO DO :\n",
    "- les images Guyane sont en 16 bits et MAyotte et martinique etc.. en 8 normal que ça deconne\n",
    "- faire en sorte de calculer plusieurs grosse images sur le .test et trouver une autre image que mayotte pour le moment qui est en 255 couleurs... != du reste\n",
    "- mettre une oimage de la validation en jeu\n",
    "- mettre le calcul d'image dans la validation aussi ?\n",
    "- mettre dans data set test une image du jeu de validatio à minima et une image de jeu externe etc..\n",
    "- dégager aussi les RIL vides ? Oui\n",
    "- labelstudio, segment anything model\n",
    "- bien construire un beau jeu de test avec cet outil  label studio, labellisation à la main , pour evaluation pertinente !!\n",
    "- mettre le [0,1,2] par défautdans les plots\n",
    "- se servir du if batch idx == 10 recharger tout les fichiers des batchs précédents et en faire une satellite data, stocker également ce qui fera une seule image et donc une seule labelled satellite image ..\n",
    "- dans le filename des images splittée changer avec les bonnes coordonnées dans lenom (trouvable dans la bounding box..)\n",
    "- faire une fonction générer dataset test qui servira pour tous les tests\n",
    "- test de lancement via invit de comande train\n",
    "- généraliser la création de liste de file path selon le dataset souhaité\n",
    "- créer un yaml de config et le logger dans mlflow\n",
    "- contrîole sur le modèle qui est loggé dans ml flow ?\n",
    "- resoudre le bug sur l'ambiguité du nombre d'elemeents dans le batch\n",
    "- workshop fevrier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc074164-878d-4598-8cea-df6c4bd5ed54",
   "metadata": {
    "tags": []
   },
   "source": [
    "Reunion section\n",
    "- 13 travaux\n",
    "- Production de masques : Juin Juillet,\n",
    "    - Esane normal + Branches\n",
    "    - demographie  # RDV Nathanael + soutien code\n",
    "    - maj EAP (Optionnel)\n",
    "    - IPI ICA\n",
    "    - Antipol, tab multi\n",
    "    \n",
    "rtauargus prez , veux-tu présenter ? nathanael ?\n",
    "fideli ..\n",
    "pour rtauargus mix des différentes présentations déjà faites :\n",
    "que veux-on partager, pas NTTIS\n",
    "différentes prez du worksho en Septembre dernier exemples bien senti\n",
    "\n",
    "Présentation \"paillettes\" -> d'où vient la complexité des données ? décrire avec le graph\n",
    "tau argus outil de référence, différnets types de complexité , une partie résolu liens tableaux, partie non résolue, partie analyse de la demande, + méta données non automatisées\n",
    "-> rtauargus\n",
    "-> \n",
    "-> création liste de tables c'est sympa ça demande une généralisation, une métadonnées nickel permettrait d'automatiser cette analyse\n",
    "-> insee.fr exemple, overlezf, reprise présentation\n",
    "\n",
    "- census bureau litterature reunion Julien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88ee46-2bb2-4031-ae82-feaf056955ea",
   "metadata": {},
   "source": [
    "## S2Looking training !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a8e0e-8e3f-412a-b0ff-1b4adc834bda",
   "metadata": {},
   "source": [
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec8dae-414f-4551-bee2-8138815235c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_storage_access()\n",
    "environment = get_environment()\n",
    "root_path = get_root_path()\n",
    "bucket = environment[\"bucket\"]\n",
    "path_s3_s2looking = environment[\"sources\"][\"PAPERS\"][\"S2Looking\"]\n",
    "path_local_s2looking = environment[\"local-path\"][\"PAPERS\"]\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "\n",
    "fs.download(\n",
    "        rpath=f\"{bucket}/{path_s3_s2looking}\",\n",
    "        lpath=f\"../{path_local_s2looking}\",\n",
    "        recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1dc5c-b4a2-4dba-b669-595b7673c0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(f\"../{path_local_s2looking}/S2Looking.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(f\"../{path_local_s2looking}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c371e3-b977-4e74-8891-b87ac1d28a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install mlflow\n",
    "import mlflow\n",
    "\n",
    "run_name = \"s2looking\" # config\n",
    "remote_server_uri = \"https://projet-slums-detection-2439.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"segmentation\" # config\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "# mlflow.pytorch.autolog() # logger la config\n",
    "\n",
    "\n",
    "## Création Dataset\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "## En faire une fonction ? : Idée faire une classe de préparation des données pour chaque data set chargé qui in fine sortirait une list path ?\n",
    "\n",
    "### config, appeler une fionction de création de dataset ?\n",
    "dir_data = \"../data/paper_dataset/S2Looking/\"\n",
    "img1_train = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "img1_val = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "\n",
    "img1_train = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "img1_val = [dir_data + \"val/Image1/\"+name for name in os.listdir(dir_data + \"/val/Image1\")]\n",
    "img1_test = [dir_data + \"test/Image1/\"+name for name in os.listdir(dir_data + \"/test/Image1\")]\n",
    "\n",
    "img2_train = [dir_data + \"train/Image2/\"+name for name in os.listdir(dir_data + \"/train/Image2\")]\n",
    "img2_val = [dir_data + \"val/Image2/\"+name for name in os.listdir(dir_data + \"/val/Image2\")]\n",
    "img2_test = [dir_data + \"test/Image2/\"+name for name in os.listdir(dir_data + \"/test/Image2\")]\n",
    "\n",
    "label_train = [dir_data + \"train/label/\"+name for name in os.listdir(dir_data + \"/train/label\")]\n",
    "label_val = [dir_data + \"val/label/\"+name for name in os.listdir(dir_data + \"/val/label\")]\n",
    "label_test = [dir_data + \"test/label/\"+name for name in os.listdir(dir_data + \"/test/label\")]\n",
    "\n",
    "img1_path = np.concatenate([np.sort(img1_train),np.sort(img1_val),np.sort(img1_test)])\n",
    "img2_path = np.concatenate([np.sort(img2_train),np.sort(img2_val),np.sort(img2_test)])\n",
    "label_path = np.concatenate([np.sort(label_train),np.sort(label_val),np.sort(label_test)])\n",
    "\n",
    "mono_image_dataset = ChangeDetectionS2LookingDataset(img1_path,img2_path,label_path)\n",
    "\n",
    "# transforms incorporé dans les dataset a posteriori\n",
    "image_size = (256,256) # cf la classe dataset\n",
    "transforms_augmentation = album.Compose(\n",
    "        [\n",
    "            album.Resize(300, 300, always_apply=True),\n",
    "            album.RandomResizedCrop(\n",
    "                *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "            ),\n",
    "            album.HorizontalFlip(),\n",
    "            album.VerticalFlip(),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "       ]\n",
    "    )\n",
    "\n",
    "transforms_preprocessing = album.Compose(\n",
    "        [\n",
    "            album.Resize(*image_size, always_apply=True),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    ")\n",
    "\n",
    "\n",
    "## Instanciation modèle et paramètres d'entraînement\n",
    "\n",
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\"lr\": 0.0001, \"momentum\": 0.9}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n",
    "\n",
    "model = DeepLabv3Module(nchannel = 6) # viens du data set qui concatene 2 images à 3 channels\n",
    "\n",
    "##Instanciation des datamodule et plmodule\n",
    "\n",
    "data_module = DataModule(\n",
    "    mono_image_dataset= mono_image_dataset,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1, \n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback_2 = ModelCheckpoint(\n",
    "    monitor=\"train_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"min\", patience=3\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy =\"auto\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "    callbacks=[lr_monitor, checkpoint_callback,checkpoint_callback_2, early_stop_callback],\n",
    "    max_epochs=100,\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    "    )\n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n",
    "    \n",
    "    lightning_module_checkpoint = lightning_module.load_from_checkpoint(\n",
    "    checkpoint_path=trainer.checkpoint_callback.best_model_path\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval\n",
    "                                     )\n",
    "    artifact_path = \"models/modele_change_detection_deeplabv3_on_s2_looking\"\n",
    "    mlflow.pytorch.log_model(lightning_module_checkpoint.model, artifact_path)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
