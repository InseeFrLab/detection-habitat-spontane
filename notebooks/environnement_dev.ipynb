{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e064d8b4-2af2-4263-800f-713875386be0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fonction to Raster\n",
    "Prend en entrée une Satellite Image, un dossier et un nom et la sauve en JP2 dans le dossier considéré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b683696-509c-466e-bdee-73d3aba6e3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install pyarrow -q -q -q \n",
    "! pip install rasterio -q -q -q \n",
    "! pip install geopandas -q -q -q\n",
    "! pip install matplotlib -q -q -q\n",
    "! pip install albumentations -q -q -q\n",
    "! pip install pytorch_lightning -q -q -q\n",
    "!pip install mlflow -q -q -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "524c5b49-9f48-4092-a43a-ad47710a6f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.satellite_image import SatelliteImage\n",
    "from utils.utils import *\n",
    "from utils.plot_utils import *\n",
    "\n",
    "import yaml\n",
    "import re\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2\n",
    "from PIL import Image as im\n",
    "\n",
    "from datetime import date\n",
    "import re\n",
    "import pyproj\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils.labeler import RILLabeler\n",
    "from utils.filter import is_too_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45654cf2-d2ef-4ae5-bf12-5a008ff640c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_path = get_root_path()\n",
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27d90c35-321b-44d0-a9ea-a0b8084ade17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_storage_access()\n",
    "environment = get_environment()\n",
    "\n",
    "root_path = get_root_path()\n",
    "bucket = environment[\"bucket\"]\n",
    "path_s3_cayenne_data = environment[\"sources\"][\"PLEIADES\"][2022][\"guyane\"]\n",
    "path_local_cayenne_data = os.path.join(root_path, environment[\"local-path\"][\"PLEIADES\"][2022][\"guyane\"])\n",
    "\n",
    "path_s3_pleiades_data_2022_martinique = environment[\"sources\"][\"PLEIADES\"][2022][\"martinique\"]\n",
    "path_local_pleiades_data_2022_martinique = environment[\"local-path\"][\"PLEIADES\"][2022][\"martinique\"]\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c92228ba-5dd3-4755-83e5-afa24a5b7e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.download(\n",
    "        rpath=f\"{bucket}/{path_s3_cayenne_data}\",\n",
    "        lpath=f\"{path_local_cayenne_data}\",\n",
    "        recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dab4c183-2d06-4364-89da-94633ca0757a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_raster(satellite_image,directory_name,file_name):\n",
    "    \"\"\"\n",
    "    save a SatelliteImage Object into a raster file (.tif)\n",
    "\n",
    "    Args:\n",
    "        satellite_image: a SatelliteImage object representing the input image to be saved as a raster file.\n",
    "        directory_name: a string representing the name of the directory where the output file should be saved.\n",
    "        file_name: a string representing the name of the output file.\n",
    "    \"\"\"\n",
    "\n",
    "    data = satellite_image.array\n",
    "    crs  = satellite_image.crs\n",
    "    transform = satellite_image.transform\n",
    "    n_bands = satellite_image.n_bands\n",
    "\n",
    "    metadata = {\n",
    "        'dtype': data.dtype,\n",
    "        'count': n_bands,\n",
    "        'width': data.shape[2],\n",
    "        'height': data.shape[1],\n",
    "        'crs': crs,\n",
    "        'transform': transform\n",
    "    }\n",
    "    \n",
    "    #print(os.path.exists(directory_name))\n",
    "    if not os.path.exists(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "\n",
    "    # Save the array as a raster file in jp2 format\n",
    "    with rasterio.open(directory_name + \"/\" + file_name, 'w', **metadata) as dst:\n",
    "        dst.write(data, indexes = np.arange(n_bands)+1)\n",
    "\n",
    "\n",
    "def write_splitted_images_masks(file_path,output_directory_name,labeler,tile_size,n_bands, dep):\n",
    "    \n",
    "    \"\"\"\n",
    "    write the couple images mask into a specific folder\n",
    "\n",
    "    Args:\n",
    "        file_path: a string representing the path to the directory containing the input image files.\n",
    "        output_directory_name: a string representing the name of the output directory where the split images and masks should be saved.\n",
    "        labeler: a Labeler object representing the labeler used to create segmentation labels.\n",
    "        tile_size: an integer representing the size of the tiles to split the input image into.\n",
    "        n_bands: an integer representing the number of bands in the input image.\n",
    "        dep: a string representing the department of the input image, or None if not applicable.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_images_path  = output_directory_name + \"/images\"\n",
    "    output_masks_path  = output_directory_name + \"/labels\"\n",
    "    \n",
    "    if not os.path.exists(output_masks_path):\n",
    "        os.makedirs(output_masks_path)\n",
    "        \n",
    "    list_name = os.listdir(file_path)\n",
    "    list_path = [file_path + \"/\" + name for name in list_name]\n",
    "    \n",
    "    for path, file_name in zip(list_path,tqdm(list_name)): # tqdm ici \n",
    "\n",
    "        big_satellite_image = SatelliteImage.from_raster(\n",
    "            file_path = path,\n",
    "            dep = None,\n",
    "            date = None,\n",
    "            n_bands= 3\n",
    "        )\n",
    "\n",
    "        list_satellite_image = big_satellite_image.split(tile_size)\n",
    "        list_satellite_image = [im for im in list_satellite_image if not is_too_black(im)]\n",
    "        # mettre le filtre is too black ici !!!\n",
    "        for i, satellite_image in enumerate(list_satellite_image):\n",
    "                \n",
    "                file_name_i = file_name.split(\".\")[0]+\"_\"+str(i)\n",
    "                to_raster(satellite_image,output_images_path,file_name_i + \".tif\")\n",
    "                \n",
    "                # mask\n",
    "                mask = labeler.create_segmentation_label(satellite_image) \n",
    "                np.save(output_masks_path+\"/\"+file_name_i+\".npy\",mask) # save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b673b6-83d8-4a97-9c25-9af4458d077c",
   "metadata": {},
   "source": [
    "### Test sur une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bcba4f9-9ffb-49c6-91b7-4bf83514ad67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "satellite_image = SatelliteImage.from_raster(\n",
    "        file_path = f\"{path_local_cayenne_data}\"+ \"/ORT_2022072050325085_0352_0545_U22N_16Bits.jp2\",\n",
    "        dep = None,\n",
    "        date = None,\n",
    "        n_bands= 4)\n",
    "\n",
    "print(satellite_image.array.shape)\n",
    "i = 2\n",
    "directory_name = \"../splitted_data\"\n",
    "file_name = \"ORT_2022072050325085_0352_0545_U22N_16Bits\"+\"_\"+str(i)+\".tif\"\n",
    "\n",
    "to_raster(satellite_image,directory_name,file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8440798-f11c-4261-a8e0-ad2c17cbaf58",
   "metadata": {},
   "source": [
    "## Test sur l'ensemble des images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e4631-0148-411f-a1b6-8b80f013458b",
   "metadata": {},
   "source": [
    "- je prends un répertoire en entrée Par exemple Guyane et je lis je split et réécris les images en taille 250\n",
    "- compter le nombre d'images à traiter et le nombre d'images à l'arrivée.\n",
    "- Si plusieurs années dispo généraliser le labeller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840269a-d846-4962-a4d4-79e8eb0321c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# params \n",
    "file_path = f\"{path_local_cayenne_data}\"\n",
    "tile_size = 250\n",
    "n_bands = 3\n",
    "dep =\"973\"\n",
    "filename = os.listdir(file_path)[0]\n",
    "date = datetime.strptime(re.search(r'ORT_(\\d{8})', filename).group(1), '%Y%m%d') \n",
    "labeler = RILLabeler(date, dep = dep, buffer_size = 10) \n",
    "output_directory_name = \"../splitted_data\"\n",
    "\n",
    "write_splitted_images_masks(file_path,output_directory_name,labeler,tile_size,n_bands,dep)\n",
    "\n",
    "# 1 min pour 250 -> 4min pour 1000, ça se tente un peu lionguet mais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12338605-a045-40f0-9261-6988618b22b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(os.listdir(output_directory_name+\"/labels\"))\n",
    "len(os.listdir(output_directory_name+\"/images\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea14795-2b15-405e-9996-c6b41776baae",
   "metadata": {},
   "source": [
    "- a priori si je connais le file path : je connais la date et je connais le labeller => créer labeller en amont ?\n",
    "- On crée un labeller par date et par territoire concerné \n",
    "- On créé les masques et on les sauvegarde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a045d-bfa4-498e-b36e-edf16850fc97",
   "metadata": {
    "tags": []
   },
   "source": [
    " ### Train un modèle de segmentation Réadaptation de la classe DeeplaV3module pour la rendre agnostique  au dataset etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6d1c3e2-f354-4311-85e5-b6cb34f1d316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%reload_ext autoreload\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import gc\n",
    "import albumentations as album\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "## from src\n",
    "from datas.components.dataset import PleiadeDataset, ChangeDetectionS2LookingDataset\n",
    "from models.components.segmentation_models import DeepLabv3Module\n",
    "from models.segmentation_module import SegmentationModule\n",
    "from datas.datamodule import DataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85394b9-960e-4ad7-8d31-9ad5abae65db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install mlflow\n",
    "import mlflow\n",
    "mlflow.end_run()\n",
    "\n",
    "run_name = \"modele deeplabV3\"\n",
    "remote_server_uri = \"https://projet-slums-detection-2439.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"segmentation\"\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "\n",
    "## Création Dataset\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "dir_data = \"../splitted_data\"\n",
    "list_path_labels =  np.sort([dir_data + \"/labels/\" + name for name in os.listdir(dir_data+\"/labels\")])# os.wlk dans d'autres cas sous des sous arbres de S2Looking\n",
    "list_path_images =  np.sort([dir_data + \"/images/\" + name for name in os.listdir(dir_data+\"/images\")])\n",
    "\n",
    "\n",
    "image_size = (250,250)\n",
    "transforms_augmentation = album.Compose(\n",
    "        [\n",
    "            album.Resize(300, 300, always_apply=True),\n",
    "            album.RandomResizedCrop(\n",
    "                *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "            ),\n",
    "            album.HorizontalFlip(),\n",
    "            album.VerticalFlip(),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "       ]\n",
    "    )\n",
    "\n",
    "transforms_preprocessing = album.Compose(\n",
    "        [\n",
    "            album.Resize(*image_size, always_apply=True),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    ")\n",
    "\n",
    "mono_image_dataset = PleiadeDataset(list_path_images,list_path_labels)\n",
    "\n",
    "\n",
    "## Instanciation modèle et paramètres d'entraînement\n",
    "\n",
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\"lr\": 0.0001, \"momentum\": 0.9}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n",
    "\n",
    "model = DeepLabv3Module()\n",
    "\n",
    "##Instanciation des datamodule et plmodule\n",
    "\n",
    "\n",
    "data_module = DataModule(\n",
    "    mono_image_dataset= mono_image_dataset,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1, \n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback_2 = ModelCheckpoint(\n",
    "    monitor=\"train_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"min\", patience=3\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy =\"auto\"\n",
    "list_callbacks = [lr_monitor, checkpoint_callback, early_stop_callback]\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "    callbacks= list_callbacks,\n",
    "    max_epochs=100,\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    "    )\n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n",
    "    \n",
    "    lightning_module_checkpoint = lightning_module.load_from_checkpoint(\n",
    "    checkpoint_path=trainer.checkpoint_callback.best_model_path,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval\n",
    "                                     )\n",
    "    \n",
    "    artifact_path = \"models/modele_segmetation_deeplabv3_guyane\"\n",
    "    mlflow.pytorch.log_model(lightning_module_checkpoint.model, artifact_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757ef55-0d17-4141-8c99-eac1960b9c62",
   "metadata": {},
   "source": [
    "### Pour tester :\n",
    "On peut mettre n'importe quel dataloader compatible là dedans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf5748-d613-4dad-8aef-f9df24378f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.test(lightning_module,data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c3e27aee-eeef-43b7-896a-af6f3771b1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image, label, dico =  data_module.mono_image_dataset.__getitem__(10000)\n",
    "            \n",
    "satellite_image = SatelliteImage.from_raster(\n",
    "        file_path = dico[\"pathimage\"],\n",
    "        dep = None,\n",
    "        date = None,\n",
    "        n_bands= 3)\n",
    "\n",
    "#satellite_image.array\n",
    "satellite_image.plot([0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88ee46-2bb2-4031-ae82-feaf056955ea",
   "metadata": {},
   "source": [
    "## S2Looking training !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a8e0e-8e3f-412a-b0ff-1b4adc834bda",
   "metadata": {},
   "source": [
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04ec8dae-414f-4551-bee2-8138815235c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_storage_access()\n",
    "environment = get_environment()\n",
    "root_path = get_root_path()\n",
    "bucket = environment[\"bucket\"]\n",
    "path_s3_s2looking = environment[\"sources\"][\"PAPERS\"][\"S2Looking\"]\n",
    "path_local_s2looking = environment[\"local-path\"][\"PAPERS\"]\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "\n",
    "fs.download(\n",
    "        rpath=f\"{bucket}/{path_s3_s2looking}\",\n",
    "        lpath=f\"../{path_local_s2looking}\",\n",
    "        recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1dc5c-b4a2-4dba-b669-595b7673c0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(f\"../{path_local_s2looking}/S2Looking.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(f\"../{path_local_s2looking}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa934cdd-5362-4d7a-b2c5-6426f55525af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env AWS_ACCESS_KEY_ID=G2QT7HK0AVMP2TSD86WQ\n",
    "%env AWS_SECRET_ACCESS_KEY=kBuiMpHcltBqODfN4eYdtKHkR4OBnM69DPLIUSwU\n",
    "%export AWS_SESSION_TOKEN=eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJHMlFUN0hLMEFWTVAyVFNEODZXUSIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNjgxMjA5NjY2LCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6ImNsZW1lbnQuZ3VpbGxvQGluc2VlLmZyIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImV4cCI6MTY4MTI5NjA3MywiZmFtaWx5X25hbWUiOiJHdWlsbG8iLCJnaXZlbl9uYW1lIjoiQ2zDqW1lbnQiLCJncm91cHMiOlsiY2hhbGxlbmdlZGF0YS1lbnMiLCJmdW5hdGhvbiIsInNsdW1zLWRldGVjdGlvbiJdLCJpYXQiOjE2ODEyMDk2NjcsImlzcyI6Imh0dHBzOi8vYXV0aC5sYWIuc3NwY2xvdWQuZnIvYXV0aC9yZWFsbXMvc3NwY2xvdWQiLCJqdGkiOiJhNjM1NDIxOC01NTA0LTQ3YTktYmE2OS1hODZmODlkNzNjMDAiLCJsb2NhbGUiOiJlbiIsIm5hbWUiOiJDbMOpbWVudCBHdWlsbG8iLCJub25jZSI6IjhmYWU4YmMzLTViNzgtNDdlOC05NWI1LTNjMWY4ZmU5MWU3OSIsInBvbGljeSI6InN0c29ubHkiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJjZ3VpbGxvIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNlc3Npb25Qb2xpY3kiOiJleUpXWlhKemFXOXVJam9pTWpBeE1pMHhNQzB4TnlJc0lsTjBZWFJsYldWdWRDSTZXM3NpUldabVpXTjBJam9pUVd4c2IzY2lMQ0pCWTNScGIyNGlPbHNpY3pNNktpSmRMQ0pTWlhOdmRYSmpaU0k2V3lKaGNtNDZZWGR6T25Nek9qbzZjSEp2YW1WMExYTnNkVzF6TFdSbGRHVmpkR2x2YmlJc0ltRnlianBoZDNNNmN6TTZPanB3Y205cVpYUXRjMngxYlhNdFpHVjBaV04wYVc5dUx5b2lYWDBzZXlKRlptWmxZM1FpT2lKQmJHeHZkeUlzSWtGamRHbHZiaUk2V3lKek16cE1hWE4wUW5WamEyVjBJbDBzSWxKbGMyOTFjbU5sSWpwYkltRnlianBoZDNNNmN6TTZPam9xSWwwc0lrTnZibVJwZEdsdmJpSTZleUpUZEhKcGJtZE1hV3RsSWpwN0luTXpPbkJ5WldacGVDSTZJbVJwWm1aMWMybHZiaThxSW4xOWZTeDdJa1ZtWm1WamRDSTZJa0ZzYkc5M0lpd2lRV04wYVc5dUlqcGJJbk16T2tkbGRFOWlhbVZqZENKZExDSlNaWE52ZFhKalpTSTZXeUpoY200NllYZHpPbk16T2pvNktpOWthV1ptZFhOcGIyNHZLaUpkZlYxOSIsInNlc3Npb25fc3RhdGUiOiJlNDhjZmRkNy03NzdhLTQ3ODUtYWYwYS1iNTgzYzM1YTY5OGQiLCJzaWQiOiJlNDhjZmRkNy03NzdhLTQ3ODUtYWYwYS1iNTgzYzM1YTY5OGQiLCJzdWIiOiIzYjA2ZWZhNC01OWZlLTQzYzgtYTAyYi1hOTRkOWI0YjU0NGUiLCJ0eXAiOiJCZWFyZXIifQ.gpxFVvlwtDROi3NafkozhywwL3qsInH5prWxG_XlbHOIwtgDD6H452k-WRDmWm03Vy8vzklYp7VuoDDxwnQAKw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c371e3-b977-4e74-8891-b87ac1d28a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install mlflow\n",
    "import mlflow\n",
    "\n",
    "run_name = \"s2looking\" # config\n",
    "remote_server_uri = \"https://projet-slums-detection-2439.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"segmentation\" # config\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "# mlflow.pytorch.autolog() # logger la config\n",
    "\n",
    "\n",
    "## Création Dataset\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "## En faire une fonction ? : Idée faire une classe de préparation des données pour chaque data set chargé qui in fine sortirait une list path ?\n",
    "\n",
    "### config, appeler une fionction de création de dataset ?\n",
    "dir_data = \"../data/paper_dataset/S2Looking/\"\n",
    "img1_train = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "img1_val = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "\n",
    "img1_train = [dir_data + \"train/Image1/\"+name for name in os.listdir(dir_data + \"/train/Image1\")]\n",
    "img1_val = [dir_data + \"val/Image1/\"+name for name in os.listdir(dir_data + \"/val/Image1\")]\n",
    "img1_test = [dir_data + \"test/Image1/\"+name for name in os.listdir(dir_data + \"/test/Image1\")]\n",
    "\n",
    "img2_train = [dir_data + \"train/Image2/\"+name for name in os.listdir(dir_data + \"/train/Image2\")]\n",
    "img2_val = [dir_data + \"val/Image2/\"+name for name in os.listdir(dir_data + \"/val/Image2\")]\n",
    "img2_test = [dir_data + \"test/Image2/\"+name for name in os.listdir(dir_data + \"/test/Image2\")]\n",
    "\n",
    "label_train = [dir_data + \"train/label/\"+name for name in os.listdir(dir_data + \"/train/label\")]\n",
    "label_val = [dir_data + \"val/label/\"+name for name in os.listdir(dir_data + \"/val/label\")]\n",
    "label_test = [dir_data + \"test/label/\"+name for name in os.listdir(dir_data + \"/test/label\")]\n",
    "\n",
    "img1_path = np.concatenate([np.sort(img1_train),np.sort(img1_val),np.sort(img1_test)])\n",
    "img2_path = np.concatenate([np.sort(img2_train),np.sort(img2_val),np.sort(img2_test)])\n",
    "label_path = np.concatenate([np.sort(label_train),np.sort(label_val),np.sort(label_test)])\n",
    "\n",
    "mono_image_dataset = ChangeDetectionS2LookingDataset(img1_path,img2_path,label_path)\n",
    "\n",
    "# transforms incorporé dans les dataset a posteriori\n",
    "image_size = (256,256) # cf la classe dataset\n",
    "transforms_augmentation = album.Compose(\n",
    "        [\n",
    "            album.Resize(300, 300, always_apply=True),\n",
    "            album.RandomResizedCrop(\n",
    "                *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "            ),\n",
    "            album.HorizontalFlip(),\n",
    "            album.VerticalFlip(),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "       ]\n",
    "    )\n",
    "\n",
    "transforms_preprocessing = album.Compose(\n",
    "        [\n",
    "            album.Resize(*image_size, always_apply=True),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    ")\n",
    "\n",
    "\n",
    "## Instanciation modèle et paramètres d'entraînement\n",
    "\n",
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\"lr\": 0.0001, \"momentum\": 0.9}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n",
    "\n",
    "model = DeepLabv3Module(nchannel = 6) # viens du data set qui concatene 2 images à 3 channels\n",
    "\n",
    "##Instanciation des datamodule et plmodule\n",
    "\n",
    "data_module = DataModule(\n",
    "    mono_image_dataset= mono_image_dataset,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1, \n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback_2 = ModelCheckpoint(\n",
    "    monitor=\"train_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"min\", patience=3\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy =\"auto\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "    callbacks=[lr_monitor, checkpoint_callback,checkpoint_callback_2, early_stop_callback],\n",
    "    max_epochs=100,\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    "    )\n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n",
    "    \n",
    "    lightning_module_checkpoint = lightning_module.load_from_checkpoint(\n",
    "    checkpoint_path=trainer.checkpoint_callback.best_model_path\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval\n",
    "                                     )\n",
    "    artifact_path = \"models/modele_change_detection_deeplabv3_on_s2_looking\"\n",
    "    mlflow.pytorch.log_model(lightning_module_checkpoint.model, artifact_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22381971-042f-4148-acaf-9679c91f3de4",
   "metadata": {},
   "source": [
    "### Evaluer un modèle entrainé sur des exemples du jeu de test et envoyer à MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ebc057-aaf2-488d-bcc9-62a5a1978b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\"lr\": 0.0001, \"momentum\": 0.9}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n",
    "\n",
    "model = DeepLabv3Module(nchannel = 6) # viens du data set qui concatene 2 images à 3 channels\n",
    "\n",
    "strategy  = \"auto\"\n",
    "\n",
    "lightning_module_checkpoint = SegmentationModule.load_from_checkpoint(\n",
    "    checkpoint_path = \"lightning_logs/version_10/checkpoints/epoch=15-step=28000.ckpt\",\n",
    "    model= model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval\n",
    "                                     )\n",
    "\n",
    "\n",
    "# je prends un dataloader basé sur plusieurs exemples tests bien choisi ?\n",
    "# faire ça après entraînement d'un modèle de segmentation ce sera plus facile!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab3a1e-93b7-4513-af1c-0160c9b9c9c8",
   "metadata": {},
   "source": [
    "### Exporter a posteriori un modèle dans MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dca6e4d8-d6f6-456c-877a-78160e3ec439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env AWS_ACCESS_KEY_ID=projet-slums-sa\n",
    "%env AWS_SECRET_ACCESS_KEY=hB2N6hCmp7JoFA6WHKT022WJ9lOc1oOr # chercher ça dans secret ? normalement le update storage access suffit\n",
    "%env AWS_S3_ENDPOINT=minio.lab.sspcloud.fr\n",
    "%env MLFLOW_S3_ENDPOINT_URL=https://minio.lab.sspcloud.fr # à préciser en +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff30d800-8b63-4916-90aa-1e8bb777e951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "#update_storage_access()\n",
    "# Save the trained model to the target run\n",
    "run_id =\"7dc98255e1f14645843d586e4567517d\"\n",
    "# Log additional artifacts to the target run\n",
    "mlflow.start_run(run_id = run_id)\n",
    "artifact_path = \"models/modele_change_detection_deeplabv3_on_s2_looking\"\n",
    "mlflow.pytorch.log_model(lightning_module_checkpoint.model, artifact_path)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e935e-c885-4d26-aa38-f47a742604aa",
   "metadata": {},
   "source": [
    "## TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef3f7dc-59e0-4db4-b415-e280a16f8b79",
   "metadata": {},
   "source": [
    "- Trouver des exemples à changements dans Mayotte\n",
    "- d&égager aussi les RIL vides ?\n",
    "- test de lancement via invit de comande train\n",
    "- lancer une entrainement de segmentation et mettre une liste d'image en dur dans la list des paths de dataset\n",
    "- Créer un premier dico de config qui appellerait la fonction relative au dataset pour le produire et un seul fichier train !\n",
    " 1) au final le fiochier de config sserait modifé par le lanceur on aurait un seul et unique fchier de train./\n",
    "- corriger istoo black, ddetection de nuages\n",
    "- retrouver des patchs où ça bouge via le RIL N N-1\n",
    "- filtrer les patchs mauvais, avant l'écriture.. ecrire en dur une liste d'images ennuagée\n",
    "- Modif des codes de Tom : #### 1) modification des codes de Tom pour que son train marche quand même !\n",
    "- save la liste des paths fichiers test dans MLFLOW ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
