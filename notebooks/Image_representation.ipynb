{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3259dd6b-62b0-4058-903c-c3d1049759b2",
   "metadata": {},
   "source": [
    "## Construire une représentation pour des images :\n",
    "Idée : Ici on va s'appuyer sur l'article (a simple Framework for Contrastive Learning of Visual Representations) https://arxiv.org/abs/2002.05709\n",
    "Pour construire une bonne representation d'images :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35c634-1936-46fa-86a0-1475a99ab8b0",
   "metadata": {},
   "source": [
    "On s'entraîne sur un jeu d'images oiseau 1 / oiseau 2, l'idée serait de construire une reprsentation des images d'oiseaux non supervisée.\n",
    "Puis de faire une ACP ou tsne sur les vectorisation en sortie en ligne chat ou chien en colonne la représentation et voir si l'espace sépare bien les deux espèces d'oiseau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d0fa54-2585-4fa0-bf0c-4997da79b5d6",
   "metadata": {},
   "source": [
    "Idée de l'entraînement de l'algorithme :\n",
    "\n",
    "On définit 2 réseaux, un convolutif et un \"normaliseur\", f et g (paramètres mouvants) ( g un simple bi couche avec une relu au milieu) ( et f un resnet pretrained ?)\n",
    "\n",
    "Pour une image donnée  on crée 2 images transformées x1 et x2 (rotation découpage etc..)\n",
    "On calcule out1 = f(g(x1)) et out2 = f(g(x2)) et on voudrait que ces 2 quantités soient proches.\n",
    "\n",
    "Pour calculer la proximité on utilise la cosine similarity entre les 2 outputs out1 et out2 (cos(a,b) divisé par les 2 normes)\n",
    "Pour evaluer la proximité on la compare dans une statistique à la similarité de out1 et out2 à la similarité de out1 avec d'autres images tirées aléatoirement\n",
    "\n",
    "En gros l'algo est le suivant pour un batch :\n",
    "\n",
    "1) Tirer  N images \n",
    "2) Appliquer les transformation aléatoires t et t' à ces images  =>  x11..xN1 et x12..xN2\n",
    "3) calculer les loss(xi1,xi2) pour i allant de 1 à N et les loss(xi2,xi1) car  => loss globale\n",
    "4) => backward :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17195d-a188-487b-a2c4-50906f3e2918",
   "metadata": {},
   "source": [
    "Je fais ça avec les images d'oiseau pour pas m'embêter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5875d8-292f-48cd-bfe9-93b674fb2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -q -q tqdm \n",
    "#!pip3 install -q -q -q torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install -q -q -q pandas\n",
    "!pip install -q -q -q imageio\n",
    "!pip install -q -q -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f26e622c-a6ec-4e87-8483-a9b02d3e1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import s3fs\n",
    "import shutil\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import random\n",
    "import imageio.v3 as iio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caad8215-b9ac-4362-8fba-ca73093419c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() : device= torch.device(\"cuda:0\" ) \n",
    "else : device  = \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "if device != \"cpu\" :\n",
    "    print(\"nom du GPU :\", torch.cuda.get_device_name(device=None))\n",
    "    print(\"GPU initialisé : \", torch.cuda.is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "727b4a85-2cd1-4654-8f3a-5689f6fa8684",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\n",
    "fs.get('projet-funathon/diffusion/Sujet9_deep_learning_donnees_satellites/archive.zip', 'oiseau.zip')\n",
    "shutil.unpack_archive('oiseau.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb6c8e2-20a1-4294-841c-2dfe1ed46a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_df = pd.read_csv('birds.csv')\n",
    "NB_CLASSES = 2\n",
    "#On se limite à 20 classes\n",
    "bird_df = bird_df[bird_df['class index'] < NB_CLASSES]\n",
    "\n",
    "train_images_paths = np.array(bird_df['filepaths'])\n",
    "train_images_labels = np.array(bird_df['class index'])\n",
    "\n",
    "# index to nom d'oiseau\n",
    "nom_oiseau = np.array(bird_df.labels.unique())\n",
    "dic_id_to_label = {i : oiseau for i,oiseau in zip(range(NB_CLASSES),nom_oiseau)}\n",
    "dic_id_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5724cd-2721-4183-84b5-59fa63a9dd61",
   "metadata": {},
   "source": [
    "Dans la classe custom Dataset je mets en place la transformation aléatoire et je sors 2 images transformées :\n",
    "le papier préconise l'application successive de :\n",
    "1) random cropping\n",
    "2) random color distortion\n",
    "3) random Gaussian blur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "96aaf016-266d-48df-988a-7f263cb8aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths,labels): \n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = iio.imread(self.image_paths[idx]) \n",
    "        image = torch.tensor(np.array(image,dtype = float)/255, dtype =torch.float).permute(2,1,0)     \n",
    "         \n",
    "        TF = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=(224,224),scale= (0.08,1),ratio= (3/4,4/3)),\n",
    "        transforms.ColorJitter(brightness = (0.8,1),\n",
    "                      contrast = (0.8,1),\n",
    "                     saturation = (0.8,1), \n",
    "                       hue = 0.2),\n",
    "        transforms.GaussianBlur(5, sigma=(0.1, 1)),\n",
    "        ])    \n",
    "    \n",
    "        image1 = TF(image)\n",
    "        image2 = TF(image)\n",
    "            \n",
    "        angle1 = random.choice([0, 90, 180, 270])\n",
    "        angle2 = random.choice([0, 90, 180, 270])\n",
    "        \n",
    "        image1 = transforms.functional.rotate(image1, angle1)\n",
    "        image2 = transforms.functional.rotate(image2, angle2)\n",
    "        \n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return {\"tf1\": image1, \"tf2\" : image2, \"image\" : image, \"label\" : label} \n",
    "        \n",
    "        \n",
    "    def __len__(self):  # return count of sample we have\n",
    "        return len(self.image_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2dc605-20b5-4905-a30a-f9149d49469a",
   "metadata": {},
   "source": [
    "au final, j'enlève le jitter qui rend difficile l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1cf7e797-7774-4dee-af16-90594b9cdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    " def montrer_image(image):\n",
    "    img = image.permute(2,1,0)\n",
    "    img = np.array(img)\n",
    "    plt.imshow(np.array(img))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8cf3b483-882f-4195-a0c5-6411fc9d4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = CustomDataset(train_images_paths,train_images_labels)\n",
    "\n",
    "it = iter(all_dataset)\n",
    "data = next(it)\n",
    "montrer_image(data[\"tf1\"])\n",
    "print(data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dc251fb6-d6b6-4a88-987a-01973b79b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\" :32,\n",
    "         \"lr\" : 0.0002,\n",
    "         \"n_epoch\" : 100}\n",
    "data_loader = DataLoader(all_dataset, batch_size=config[\"batch_size\"],shuffle=True, num_workers=0)\n",
    "\n",
    "batch = next(iter(data_loader))\n",
    "montrer_image(batch[\"tf1\"][0])\n",
    "montrer_image(batch[\"tf2\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d32b3f-a244-4e43-911f-81dc59870273",
   "metadata": {},
   "source": [
    "Je télécharge le resnet pretrained pour créer la rerprésentation puis je créerai le projecteur de toute pièce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3cd85696-0a8a-4c6a-9690-0f185120e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class projectionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1000, 400)\n",
    "        self.fc2 = nn.Linear(400, 50)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.proj = projectionNetwork()\n",
    "        #self.resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True) # réseau pretrained\n",
    "        self.resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=False) # réseau pretrained\n",
    "        \n",
    "        # En fait pour la démo ne pas mettre de prétrained car celui comporte déjà des infos et une bonne représentation des données donc ce n'est pas sopectaculaire\n",
    "        # entrainer from scratch l'est plus car on va voir qu'on arrive à entrainer sdans label (le pretrained est entrtrainé avec label\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = self.resnet(x)\n",
    "        x = self.proj(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d325d47-b633-415e-9887-f31649362c99",
   "metadata": {},
   "source": [
    "### Training !!! \n",
    "Topute la finesse encore une fois se trouve dans la constructiond e la loss il faut faire en sorte de garder le gradient..\n",
    "Le gaussian blur passe !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b9674775-15c9-4a58-96d5-f299d2849811",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() : device= torch.device(\"cuda:0\" ) \n",
    "print(device)\n",
    "\n",
    "net = SimCLR()\n",
    "liste_loss = []\n",
    "net = net.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10403216-f7de-427a-a0c0-0664d3de339f",
   "metadata": {},
   "source": [
    "Ok après quelques exemples, le color jitter empeche le bon entrainement(le supprimer ou l'amoindrir ?)\n",
    "La temperature est capîtale pour bien distinguer les cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "203a9a07-92ed-42c5-8c7b-2b2b3cc6ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\" :64,\n",
    "         \"lr\" : 0.0005,\n",
    "         \"n_epoch\" : 1000,\n",
    "         \"temperature\" :0.006}\n",
    "\n",
    "#optimizer = optim.SGD(net.parameters(), lr=config['lr'])\n",
    "optimizer = optim.Adam(net.parameters(), lr=config['lr'])\n",
    "data_loader = DataLoader(all_dataset, batch_size=config[\"batch_size\"],shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fcfd1e-ed30-46c9-bf06-8cbafd64bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(config['n_epoch']):  \n",
    "        \n",
    "        net = net.to(device)\n",
    "        running_loss = 0.0\n",
    "\n",
    "        t= tqdm(data_loader, desc=\"epoch %i\" % (epoch+1),position = 0, leave=True)\n",
    "        epoch_loop = enumerate(t)\n",
    "\n",
    "        for i, batch in epoch_loop:\n",
    "            \n",
    "            tf1 = batch[\"tf1\"]\n",
    "            tf2 = batch[\"tf2\"]\n",
    "            \n",
    "            tf1, tf2 = tf1.to(device), tf2.to(device)\n",
    "            \n",
    "            res_transfo1 = net(tf1)\n",
    "            res_transfo2 = net(tf2)\n",
    "            \n",
    "            cosineMat = torch.mm(res_transfo1/torch.norm(res_transfo1), # cosine similarity pairwise\n",
    "            torch.transpose(res_transfo2/torch.norm(res_transfo2),0,1)\n",
    "                                )/config[\"temperature\"]\n",
    "            \n",
    "            # cosineMat.shape \n",
    "            # j'ai bien une matrice 32x 32 \n",
    "            # je ne fais pas exactement la même loss que dans l'article mazis en gros je compare la loss du coefficient diagonale avec les losses sur la même ligne et sur la même colonne\n",
    "\n",
    "            loss = 0\n",
    "    \n",
    "            for ind in range(tf1.shape[0]):\n",
    "                \n",
    "                rowi = cosineMat[ind,:]\n",
    "                coli = cosineMat[:,ind]\n",
    "\n",
    "                #sum_row_mi = torch.sum(torch.exp(torch.cat([rowi[:ind], rowi[ind+1:]])))\n",
    "                #loss_row = -torch.log(torch.exp(cosineMat[ind,ind])/sum_row_mi)\n",
    "                loss_row = -torch.log(torch.exp(cosineMat[ind,ind])/torch.sum(torch.exp(rowi)))\n",
    "\n",
    "                #sum_col_mi = torch.sum(torch.exp(torch.cat([coli[:ind], coli[ind+1:]])))\n",
    "                #loss_col = -torch.log(torch.exp(cosineMat[ind,ind])/sum_col_mi)\n",
    "                loss_col = -torch.log(torch.exp(cosineMat[ind,ind])/torch.sum(torch.exp(coli)))\n",
    "\n",
    "                loss += (loss_col + loss_row)\n",
    "\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward() # calculer le gradient\n",
    "            optimizer.step() # avancer dans le sens du gradient calculé\n",
    "\n",
    "            del tf1, tf2, cosineMat\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i+1) % 3 == 0:  \n",
    "                # ici enregistrement de la loss sur le train, sur le validation et envoi des résultats à wnandb\n",
    "                \n",
    "                liste_loss.append(running_loss)\n",
    "\n",
    "                # validation\n",
    "                t.set_description(\"epoch %i, 'mean loss: %.6f'\" % (epoch+1,running_loss/10))\n",
    "                t.refresh()\n",
    "                \n",
    "                running_loss = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632509a5-8549-49a4-beee-92e07c08ee1b",
   "metadata": {},
   "source": [
    "## Tests du modele entrainé !!\n",
    "1) Récupération des représentations pour toutes les images\n",
    "2) Big ACP/t-sne\n",
    "3) On observe la répartition des points sur les premiers axes en les colorants par leurs labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e09c050-f44a-4eed-aa9d-9c17379c6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "058bb8fd-80a3-41fe-aa2f-11b20b4ceb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_eval = DataLoader(all_dataset, batch_size=90,shuffle=True, num_workers=0) # évaluation sur 70 images au pif\n",
    "all_pictures = next(iter(data_loader_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "36a2a8fd-d4ab-4991-a2ee-8c6b3c4ed5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_70_pictures_net = np.array(net(all_pictures[\"image\"].to(device)).to(\"cpu\").detach())\n",
    "res_70_pictures_resnet = np.array(net.resnet(all_pictures[\"image\"].to(device)).to(\"cpu\").detach())\n",
    "\n",
    "labels = all_pictures[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76258492-ebd9-4603-87be-5d68147121d8",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cf57f-0260-4597-9b63-afbebf50ad0b",
   "metadata": {},
   "source": [
    "pca en gardant le net entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "278c662c-63b9-460d-a26e-3eb4cb179543",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "res_70_pca = pca.fit_transform(res_70_pictures_net)\n",
    "plt.scatter(res_70_pca[:,0], res_70_pca[:,1], c = labels)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae575f-e667-4422-aa02-0e75eee93d5e",
   "metadata": {},
   "source": [
    "pca en gardant seulement le resnet comme préconisé dans l'article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a49eb546-2a91-467e-a413-f7d8b33f72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "res_70_pca = pca.fit_transform(res_70_pictures_resnet)\n",
    "\n",
    "plt.scatter(res_70_pca[:,0], res_70_pca[:,1], c = labels)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502eef7-7d84-403c-9359-b3f0f8cf1b67",
   "metadata": {},
   "source": [
    "### tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "356b0c8e-5bed-45c0-8351-c9e6213c1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_70_tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                   init='random', perplexity=3).fit_transform(res_70_pictures_net)\n",
    "\n",
    "plt.scatter(res_70_tsne[:,0], res_70_tsne[:,1], c = labels)\n",
    "plt.xlabel('Ax1')\n",
    "plt.ylabel('Ax2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f9550768-6b7f-4479-8223-56f3c18ab0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_70_tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                   init='random', perplexity=3).fit_transform(res_70_pictures_resnet)\n",
    "\n",
    "plt.scatter(res_70_tsne[:,0], res_70_tsne[:,1], c = labels)\n",
    "plt.xlabel('Ax1')\n",
    "plt.ylabel('Ax2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280d084-13ee-42bd-96e4-fb6773fff5cd",
   "metadata": {},
   "source": [
    "\n",
    "### contre factuel sur mauvais réseau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7a3a2410-fdb4-4609-a335-cead1868ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_non_entraine = SimCLR().to(device)\n",
    "res_70_pictures_net = np.array(net_non_entraine(all_pictures[\"image\"].to(device)).to(\"cpu\").detach())\n",
    "res_70_pictures_resnet = np.array(net_non_entraine.resnet(all_pictures[\"image\"].to(device)).to(\"cpu\").detach())\n",
    "\n",
    "labels = all_pictures[\"label\"]\n",
    "\n",
    "res_70_tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                   init='random', perplexity=3).fit_transform(res_70_pictures_net)\n",
    "\n",
    "plt.scatter(res_70_tsne[:,0], res_70_tsne[:,1], c = labels)\n",
    "plt.xlabel('Ax1')\n",
    "plt.ylabel('Ax2')\n",
    "plt.show()\n",
    "\n",
    "res_70_tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                   init='random', perplexity=3).fit_transform(res_70_pictures_resnet)\n",
    "\n",
    "plt.scatter(res_70_tsne[:,0], res_70_tsne[:,1], c = labels)\n",
    "plt.xlabel('Ax1')\n",
    "plt.ylabel('Ax2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732becb4-41c6-4369-92d4-7380f35aab6e",
   "metadata": {},
   "source": [
    "Sur wandbe mettre l'inertie intra classe dans la TSNE en indicateur par exemple"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
