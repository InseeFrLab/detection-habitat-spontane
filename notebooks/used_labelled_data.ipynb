{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0758f67b-7497-4d0f-be0f-a000fb6da6c8",
   "metadata": {},
   "source": [
    "## Utilisation du jeu de données Xview2, qui intitialement présebnte des exemples de territoire avant et après dommage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73560aa1-5db5-4cfb-a536-c84403c33dc6",
   "metadata": {},
   "source": [
    "Je m'écarte ici un peu de l'idée initiale qui était de produire un dataset à l'aide du RIl et de la BDTOPO et je me concentre + sur les datasets labellisés préexistants. J'entraine un modèle de segmentation de suus ou un modèle de déttection d'objet et je vois comment ça réagit sur donnée spleiades. \n",
    "Dans tousd les cas le travail sur le RIL et la BD TOPO est à conserver puisque ces derniers servent de vzlidation !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a118d9-8fb9-4611-9629-2fdb6a7ced0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install -q -q -q tqdm # progresbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b10de5-1360-4cda-ac46-9dec8016c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import s3fs\n",
    "import tarfile\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader,  random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},key ='XMB3NPLOZDVX0WNE6153', secret = 'sjv23FK1803B0cjV0f4gqj6bFgS80wJvk8Ntv7bm', token = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJYTUIzTlBMT1pEVlgwV05FNjE1MyIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNjc1MjQ5OTY4LCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6ImNsZW1lbnQuZ3VpbGxvQGluc2VlLmZyIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImV4cCI6MTY3NTUxNDEyMiwiZmFtaWx5X25hbWUiOiJHdWlsbG8iLCJnaXZlbl9uYW1lIjoiQ2zDqW1lbnQiLCJncm91cHMiOlsiY2hhbGxlbmdlZGF0YS1lbnMiLCJmdW5hdGhvbiIsInNsdW1zLWRldGVjdGlvbiJdLCJpYXQiOjE2NzUyNDk5NzAsImlzcyI6Imh0dHBzOi8vYXV0aC5sYWIuc3NwY2xvdWQuZnIvYXV0aC9yZWFsbXMvc3NwY2xvdWQiLCJqdGkiOiJjNTJlODhhMS0zNTEyLTQzMTAtYWViYy1kMzAxOTEyZGY3NDAiLCJsb2NhbGUiOiJlbiIsIm5hbWUiOiJDbMOpbWVudCBHdWlsbG8iLCJub25jZSI6ImZiZjJmNTI3LTIxOTYtNGYwNi04NTUzLWMxMzg2NDJiYzE5ZiIsInBvbGljeSI6InN0c29ubHkiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJjZ3VpbGxvIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNlc3Npb25fc3RhdGUiOiI0NDIwMTFhMC02MDk5LTRkOTYtYjIxMy04ZTFiMGFmM2I0MmYiLCJzaWQiOiI0NDIwMTFhMC02MDk5LTRkOTYtYjIxMy04ZTFiMGFmM2I0MmYiLCJzdWIiOiIzYjA2ZWZhNC01OWZlLTQzYzgtYTAyYi1hOTRkOWI0YjU0NGUiLCJ0eXAiOiJCZWFyZXIifQ.UhQ2IGMyFrPOY9edbScANkJd0C3zLpS3E3sKNXoIZQYvFGkLEyyLkT6jNRt04rwyYtK9HgXijkTzEOR4u-gOWA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa7c3a3-c558-4f44-a1e5-243db46a0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# je travaille sur une version minimale du dataset xview, le vrai fait 50 giga.. mais ça devrait déjà faire l'affaire pour travailler\n",
    "fs.get('projet-slums-detection/Donnees/data_xBD.tar', 'data_xBD.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ec31159-1d5f-4bf6-a688-67b7d85a65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"data_xBD.tar\", \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb79d8-72e1-4c6a-a450-c32ed061b1fd",
   "metadata": {},
   "source": [
    "### chargement/observation en place des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a199b3d3-21f9-4533-8af9-a99568a52211",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_image  = sorted(os.listdir(\"train/images/\"))\n",
    "liste_label  = sorted(os.listdir(\"train/labels/\")) # boundingbox et polygones !!\n",
    "liste_target  = sorted(os.listdir(\"train/targets/\")) # le masque de segmentation !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac233ac-45d9-4e5f-968e-f0eda92cb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "selec_pre_disaster = [nom_image.split(\"_\")[2] == \"pre\" for nom_image in liste_image]\n",
    "liste_image = np.array(liste_image)[selec_pre_disaster]\n",
    "liste_label = np.array(liste_label)[selec_pre_disaster]\n",
    "liste_target = np.array(liste_target)[selec_pre_disaster]\n",
    "\n",
    "print(np.sum(selec_pre_disaster)) # 3000 images pour s'entrainer avec des exemples entourés\n",
    "\n",
    "train_images_paths = [\"train/images/\" + elt for elt in liste_image]\n",
    "train_masks_paths = [\"train/targets/\" + elt for elt in liste_target]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35017c79-30e5-431b-8bb1-ddee599d83f3",
   "metadata": {},
   "source": [
    "## Observation du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43cf49-6ea6-41a3-87cb-4c7477a97748",
   "metadata": {},
   "source": [
    "- Images de dimension 1024-1024 à découper en 4 * 250 pour avoir un diviseur de 2000 (pour les données pleiades) (donc en 4)\n",
    "- Dans la classe data set splitter l'image en 4 et prendre un bout aléatoirement à chaque fois\n",
    "- image à 3 channels, pas de RGB ici..\n",
    "- est ce vraiment la mêlme résolution que pleiade ? résistance à la résolution ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d940dca-39fc-4e38-adc1-fa9fe0210722",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(train_images_paths[30]) # 30 ok\n",
    "img = img.resize((1000,1000))\n",
    "\n",
    "masque = Image.open(train_masks_paths[30])\n",
    "masque = np.array(masque)\n",
    "show_mask = np.empty((*masque.shape, 3))\n",
    "show_mask[masque == 1, :] = [255,255,255]\n",
    "show_mask = show_mask.astype(np.uint8)\n",
    "\n",
    "# On traçe\n",
    "fig,(ax1,ax2) = plt.subplots(1,2, figsize = (15,15))\n",
    "ax1.imshow(img)\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(show_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2e06c-671b-447c-9348-dc14abe58f57",
   "metadata": {},
   "source": [
    "A mettre dans la classe dataset ! sélection d'un pa(tch aléatoire parmi les 16  possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90abee8a-1c54-413e-adb8-ec1b609f533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(train_images_paths[35])\n",
    "\n",
    "img = img.crop((0,0,1000,1000))# je dégomme les derniers pixels..\n",
    "img\n",
    "\n",
    "facteur_div = 250\n",
    "width, height = img.size\n",
    "\n",
    "num_subparts_x = width//facteur_div\n",
    "num_subparts_y =  height//facteur_div\n",
    "\n",
    "# sélection aléatoire d'une aprtie de l'image pour le dataset\n",
    "i = np.random.randint(num_subparts_x)\n",
    "j = np.random.randint(num_subparts_y)\n",
    "\n",
    "print(i,j)\n",
    "\n",
    "left = j * facteur_div\n",
    "right = (j+1) * facteur_div\n",
    "top = i * facteur_div\n",
    "bottom =(i+1)*facteur_div\n",
    "\n",
    "out = img.crop((left,top,right,bottom))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48d7a7-0330-400a-bd48-ea46a2b6fc10",
   "metadata": {},
   "source": [
    "Polygones associés au bati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d252a4-2e20-4111-89b1-7921a01d3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    " class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths,mask_paths, facteur_div = 250):   # initial logic happens like transform\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.facteur_div = 250\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        with Image.open(self.image_paths[idx]) as img :\n",
    "            \n",
    "            width, height = img.size\n",
    "\n",
    "            num_subparts_x = width//self.facteur_div\n",
    "            num_subparts_y =  height//self.facteur_div\n",
    "            # sélection aléatoire d'une aprtie de l'image pour le dataset\n",
    "            i = np.random.randint(num_subparts_x)\n",
    "            j = np.random.randint(num_subparts_y)\n",
    "\n",
    "            left = j * self.facteur_div\n",
    "            right = (j+1) * self.facteur_div\n",
    "            top = i * self.facteur_div\n",
    "            bottom =(i+1)*self.facteur_div\n",
    "\n",
    "            img = img.crop((left,top,right,bottom))\n",
    "            img = img.convert(\"RGB\")\n",
    "            img_pil = img\n",
    "            \n",
    "        with Image.open(self.mask_paths[idx]) as masque :\n",
    "            masque = masque.crop((left,top,right,bottom))\n",
    "            masque = np.array(masque)\n",
    "         \n",
    "        \n",
    "        masque = torch.tensor(masque,dtype = torch.long)\n",
    "        img = torch.tensor(np.array(img,dtype = float), dtype =torch.float).permute(2,0,1)\n",
    "        \n",
    "        ID = str(self.image_paths[idx])\n",
    "     \n",
    "        return {\"image\": img, \"masque\" : masque,\"image_pillow\": np.array(img_pil) , \"id\" : ID} \n",
    "        \n",
    "    def __len__(self):  \n",
    "        return len(self.mask_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb1dc43c-0809-425e-830e-db19fcf12f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = CustomDataset(train_images_paths,train_masks_paths)\n",
    "one_element = next(iter(all_dataset))\n",
    "\n",
    "one_element[\"image\"].shape\n",
    "#torch.max(one_element[\"masque\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef0a13f-fe27-4723-a882-a00eea48da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\" : 17,\n",
    "          \"freq monitoring\" : 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d2c498-4227-4d1e-88bd-26a0825a97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loader = DataLoader(all_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=0)\n",
    "\n",
    "train_size = 2000\n",
    "val_size = len(all_dataset.mask_paths) - train_size\n",
    "#dans la liste donner la taille du train et la taille deu test\n",
    "train_dataset, valid_dataset = random_split(all_dataset,[train_size,val_size], generator=torch.Generator().manual_seed(42))\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], \n",
    "                          shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=3, shuffle=True, num_workers=0)\n",
    "\n",
    "next(iter(valid_loader))[\"image\"].shape # parfait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5be83f86-f165-4182-aa8e-766a0de1aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() : device= torch.device(\"cuda:0\" )\n",
    "else : device = \"cpu\"\n",
    "\n",
    "print(\"Using {} device\".format(device))\n",
    "if torch.cuda.is_available() :\n",
    "    print(\"nom du GPU :\", torch.cuda.get_device_name(device=None))\n",
    "    print(\"GPU initialisé : \", torch.cuda.is_initialized())\n",
    "\n",
    "    # Load the pretrained model\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False) # 233 Mega\n",
    "# 1 classe !\n",
    "model.classifier[4] = nn.Conv2d(256,2,kernel_size = (1,1),stride =(1,1))\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da9604ed-bd12-4107-86f0-a49bf79c6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the number of parameters\n",
    "total_params = 0\n",
    "\n",
    "# Loop through the parameters in the model\n",
    "for param in model.parameters():\n",
    "    # Get the size of the parameter tensor\n",
    "    size = param.size()\n",
    "    # Multiply the size of the tensor by the number of elements in it\n",
    "    num_params = torch.prod(torch.tensor(size)).item()\n",
    "    # Add the number of parameters to the total\n",
    "    total_params += num_params\n",
    "\n",
    "# Print the total number of parameters\n",
    "print(\"Total number of parameters: \", total_params) # 60 millions..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d15e1-c64f-477a-94c9-aeb1013755ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autre idée : pour un batch donné \n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    t= tqdm(train_loader, desc=\"epoch %i\" % (epoch+1),position = 0, leave=True)\n",
    "    epoch_loop = enumerate(t)\n",
    "\n",
    "    for i, data in epoch_loop:\n",
    "        \n",
    "        images = data[\"image\"].to(device)\n",
    "        labels = data[\"masque\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)[\"out\"]\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        del images, labels, output\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % config['freq monitoring'] == 0:  \n",
    "                t.set_description(\"epoch %i, 'mean loss: %.6f'\" % (epoch+1,running_loss/config['freq monitoring']))\n",
    "                t.refresh()\n",
    "                running_loss =0\n",
    "    \n",
    "#Save the trained model\n",
    "#torch.save(model.state_dict(), 'path/to/model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3b8a1-9d75-4a67-96d7-4ee104d3d975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0619fbd-7793-4f72-9d01-18096b19931a",
   "metadata": {},
   "source": [
    "Wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e5bdfb-e8c4-48fd-a6d0-15b5a09b508a",
   "metadata": {},
   "source": [
    "- Contacter Raya / dinamis\n",
    "- apurement du jeu de données , trop de patch sans logements.. à enlever et c'est faisable (en filtrant au préalable sur les labels)\n",
    "- 5 min par epoch => 8h pour 100 epochs\n",
    "- la loss sembledimiué\n",
    "- je n'ai pas réadapté la sortie du modèle ici je suis resté en 22 classes.. mais bizarrement ça marche il vamettree 0 sur toutes les autres..\n",
    "- Faire du wand b !!\n",
    "- évaluer le modèle sur macouria !!\n",
    "- inscription données pleiades pour el compte de la dmrg\n",
    "- tenter batch 20 et brancher avec le framework image des gars c'est la limite après ç!a crash\n",
    "- dessiner au brouillon l'architecture du projet avec les entrainemenbt avec la volonté detre agnostique au type de modèle\n",
    "- validation en utilisant les labels RIL !!\n",
    "- transferabilité résolution / couluers\n",
    "- faire un taf en 2 temps test buildoing dans la zone puis segmentation\n",
    "- to do : utiliser les bounding box pour faire autre chose que de la segmentation\n",
    "- faire un énorme schéma\n",
    "- équilibrer jeu de donnée avec les 0 et 1. prendre que des exemple ou il y a des 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f5189-56a4-42ab-ae75-2990cceb9866",
   "metadata": {},
   "source": [
    "## Tests visuels du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f8c1a9c-c9eb-468a-8aca-1141012484cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(output_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d9ca1a3-d778-41d3-af47-e610b4c9f2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "55375398-b57c-4ae7-9258-c7513b91bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250*250 = 62500\n",
    "indice_dans_batch = 0\n",
    "data_val  = next(iter(valid_loader))\n",
    "data_val[\"image\"].shape\n",
    "output = np.array(model(data_val[\"image\"].to(device))[\"out\"].to(\"cpu\").detach())[indice_dans_batch]\n",
    "output_predictions = output.argmax(0)\n",
    "img_init = Image.fromarray(np.array(data_val[\"image_pillow\"][indice_dans_batch]))\n",
    "\n",
    "masque = output_predictions\n",
    "show_mask = np.zeros((*masque.shape, 3))\n",
    "show_mask[masque == 1, :] = [255,255,255]\n",
    "show_mask = show_mask.astype(np.uint8)\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2, figsize = (10,10))\n",
    "ax1.imshow(img_init)\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(show_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63055c-c4f8-41c3-a61b-67e96ddbcadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pleiade !!!!! test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ab30e-78ec-468d-bd67-9bf9a6016f0f",
   "metadata": {},
   "source": [
    "### Les contours géométriques en Json si besoin !! bounding box etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3edf84-a61a-462a-87e3-3be5d035a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(label_path)\n",
    "dico = jason.load(f)\n",
    "  \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b480e01-e129-467e-be78-2e92e58e8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico.keys()\n",
    "dico[\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab590c6b-684c-468f-b2fd-980217a9858e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
