{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0758f67b-7497-4d0f-be0f-a000fb6da6c8",
   "metadata": {},
   "source": [
    "## Utilisation du jeu de données Xview2, qui intitialement présebnte des exemples de territoire avant et après dommage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73560aa1-5db5-4cfb-a536-c84403c33dc6",
   "metadata": {},
   "source": [
    "Je m'écarte ici un peu de l'idée initiale qui était de produire un dataset à l'aide du RIl et de la BDTOPO et je me concentre + sur les datasets labellisés préexistants. J'entraine un modèle de segmentation de suus ou un modèle de déttection d'objet et je vois comment ça réagit sur donnée spleiades. \n",
    "Dans tousd les cas le travail sur le RIL et la BD TOPO est à conserver puisque ces derniers servent de vzlidation !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a118d9-8fb9-4611-9629-2fdb6a7ced0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install -q -q -q tqdm # progresbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b10de5-1360-4cda-ac46-9dec8016c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import s3fs\n",
    "import tarfile\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader,  random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},key ='XMB3NPLOZDVX0WNE6153', secret = 'sjv23FK1803B0cjV0f4gqj6bFgS80wJvk8Ntv7bm', token = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJYTUIzTlBMT1pEVlgwV05FNjE1MyIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNjc1MjQ5OTY4LCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6ImNsZW1lbnQuZ3VpbGxvQGluc2VlLmZyIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImV4cCI6MTY3NTUxNDEyMiwiZmFtaWx5X25hbWUiOiJHdWlsbG8iLCJnaXZlbl9uYW1lIjoiQ2zDqW1lbnQiLCJncm91cHMiOlsiY2hhbGxlbmdlZGF0YS1lbnMiLCJmdW5hdGhvbiIsInNsdW1zLWRldGVjdGlvbiJdLCJpYXQiOjE2NzUyNDk5NzAsImlzcyI6Imh0dHBzOi8vYXV0aC5sYWIuc3NwY2xvdWQuZnIvYXV0aC9yZWFsbXMvc3NwY2xvdWQiLCJqdGkiOiJjNTJlODhhMS0zNTEyLTQzMTAtYWViYy1kMzAxOTEyZGY3NDAiLCJsb2NhbGUiOiJlbiIsIm5hbWUiOiJDbMOpbWVudCBHdWlsbG8iLCJub25jZSI6ImZiZjJmNTI3LTIxOTYtNGYwNi04NTUzLWMxMzg2NDJiYzE5ZiIsInBvbGljeSI6InN0c29ubHkiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJjZ3VpbGxvIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNlc3Npb25fc3RhdGUiOiI0NDIwMTFhMC02MDk5LTRkOTYtYjIxMy04ZTFiMGFmM2I0MmYiLCJzaWQiOiI0NDIwMTFhMC02MDk5LTRkOTYtYjIxMy04ZTFiMGFmM2I0MmYiLCJzdWIiOiIzYjA2ZWZhNC01OWZlLTQzYzgtYTAyYi1hOTRkOWI0YjU0NGUiLCJ0eXAiOiJCZWFyZXIifQ.UhQ2IGMyFrPOY9edbScANkJd0C3zLpS3E3sKNXoIZQYvFGkLEyyLkT6jNRt04rwyYtK9HgXijkTzEOR4u-gOWA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7c3a3-c558-4f44-a1e5-243db46a0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# je travaille sur une version minimale du dataset xview, le vrai fait 50 giga.. mais ça devrait déjà faire l'affaire pour travailler\n",
    "fs.get('projet-slums-detection/Donnees/data_xBD.tar', 'data_xBD.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec31159-1d5f-4bf6-a688-67b7d85a65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"data_xBD.tar\", \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb79d8-72e1-4c6a-a450-c32ed061b1fd",
   "metadata": {},
   "source": [
    "### chargement/observation en place des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a199b3d3-21f9-4533-8af9-a99568a52211",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_image  = sorted(os.listdir(\"train/images/\"))\n",
    "liste_label  = sorted(os.listdir(\"train/labels/\")) # boundingbox et polygones !!\n",
    "liste_target  = sorted(os.listdir(\"train/targets/\")) # le masque de segmentation !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76aa5d2c-8907-4ed7-be4c-4d4d445227b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([nom_image.split(\"_\")[0] for nom_image in liste_image])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac233ac-45d9-4e5f-968e-f0eda92cb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "selec_pre_disaster = [nom_image.split(\"_\")[2] == \"pre\" for nom_image in liste_image]\n",
    "selec_tout_sauf_social_fire = [ not nom_image.split(\"_\")[0] in [\"social-fire\",\"palu-tsunami\"] for nom_image in liste_image]\n",
    "\n",
    "selec_train = np.array(selec_pre_disaster) * selec_tout_sauf_social_fire\n",
    "selec_val = np.array(selec_pre_disaster) * list(map(lambda x: not x, selec_tout_sauf_social_fire))\n",
    "\n",
    "\n",
    "train_images_paths = [\"train/images/\" + elt for elt in np.array(liste_image)[selec_train]]\n",
    "train_masks_paths = [\"train/targets/\" + elt for elt in np.array(liste_target)[selec_train]]\n",
    "\n",
    "valid_images_paths = [\"train/images/\" + elt for elt in np.array(liste_image)[selec_val]]\n",
    "valid_masks_paths = [\"train/targets/\" + elt for elt in np.array(liste_target)[selec_val]]\n",
    "\n",
    "\n",
    "sum(selec_pre_disaster)\n",
    "sum(selec_train)\n",
    "sum(selec_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35017c79-30e5-431b-8bb1-ddee599d83f3",
   "metadata": {},
   "source": [
    "## Observation du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43cf49-6ea6-41a3-87cb-4c7477a97748",
   "metadata": {},
   "source": [
    "- Images de dimension 1024-1024 à découper en 4 * 250 pour avoir un diviseur de 2000 (pour les données pleiades) (donc en 4)\n",
    "- Dans la classe data set splitter l'image en 4 et prendre un bout aléatoirement à chaque fois\n",
    "- image à 3 channels, pas de RGB ici..\n",
    "- est ce vraiment la mêlme résolution que pleiade ? résistance à la résolution ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d940dca-39fc-4e38-adc1-fa9fe0210722",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(train_images_paths[350]) # 30 ok\n",
    "img = img.resize((1000,1000))\n",
    "\n",
    "masque = Image.open(train_masks_paths[350])\n",
    "masque = np.array(masque)\n",
    "show_mask = np.zeros((*masque.shape, 3))\n",
    "show_mask[masque == 1, :] = [255,255,255]\n",
    "show_mask = show_mask.astype(np.uint8)\n",
    "\n",
    "# On traçe\n",
    "fig,(ax1,ax2) = plt.subplots(1,2, figsize = (15,15))\n",
    "ax1.imshow(img)\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(show_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2e06c-671b-447c-9348-dc14abe58f57",
   "metadata": {},
   "source": [
    "A mettre dans la classe dataset ! sélection d'un pa(tch aléatoire parmi les 16  possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90abee8a-1c54-413e-adb8-ec1b609f533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(train_images_paths[35])\n",
    "\n",
    "img = img.crop((0,0,1000,1000))# je dégomme les derniers pixels..\n",
    "img\n",
    "\n",
    "facteur_div = 250\n",
    "width, height = img.size\n",
    "\n",
    "num_subparts_x = width//facteur_div\n",
    "num_subparts_y =  height//facteur_div\n",
    "\n",
    "# sélection aléatoire d'une aprtie de l'image pour le dataset\n",
    "i = np.random.randint(num_subparts_x)\n",
    "j = np.random.randint(num_subparts_y)\n",
    "\n",
    "print(i,j)\n",
    "\n",
    "left = j * facteur_div\n",
    "right = (j+1) * facteur_div\n",
    "top = i * facteur_div\n",
    "bottom =(i+1)*facteur_div\n",
    "\n",
    "out = img.crop((left,top,right,bottom))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48d7a7-0330-400a-bd48-ea46a2b6fc10",
   "metadata": {},
   "source": [
    "Polygones associés au bati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e4d252a4-2e20-4111-89b1-7921a01d3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    " class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths,mask_paths, facteur_div = 250):   # initial logic happens like transform\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.facteur_div = 250\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        with Image.open(self.image_paths[idx]) as img :\n",
    "            \n",
    "            img_pour_mean = torch.tensor(np.array(img,dtype = float), dtype =torch.float).permute(2,0,1)\n",
    "            mean, std = img_pour_mean.mean([1,2]), img_pour_mean.std([1,2]) # moyenne sur toutes l'image avant patch sinon on a des cas dégénérés\n",
    "            \n",
    "            width, height = img.size\n",
    "\n",
    "            num_subparts_x = width//self.facteur_div\n",
    "            num_subparts_y =  height//self.facteur_div\n",
    "            # sélection aléatoire d'une aprtie de l'image pour le dataset\n",
    "            i = np.random.randint(num_subparts_x)\n",
    "            j = np.random.randint(num_subparts_y)\n",
    "\n",
    "            left = j * self.facteur_div\n",
    "            right = (j+1) * self.facteur_div\n",
    "            top = i * self.facteur_div\n",
    "            bottom =(i+1)*self.facteur_div\n",
    "\n",
    "            img = img.crop((left,top,right,bottom))\n",
    "            img = img.convert(\"RGB\")\n",
    "            img_pil = img\n",
    "            \n",
    "            \n",
    "            \n",
    "        with Image.open(self.mask_paths[idx]) as masque :\n",
    "            masque = masque.crop((left,top,right,bottom))\n",
    "            masque = np.array(masque)\n",
    "         \n",
    "        \n",
    "        masque = torch.tensor(masque,dtype = torch.long)\n",
    "        \n",
    "        img = torch.tensor(np.array(img,dtype = float), dtype =torch.float).permute(2,0,1)\n",
    "        \n",
    "        #if any(std == 0):\n",
    "         #   mean = [0,0,0] \n",
    "          #  std = [1,1,1]\n",
    "            \n",
    "        img = transforms.Normalize(mean, std)(img)\n",
    "        \n",
    "        ID = str(self.image_paths[idx])\n",
    "     \n",
    "        return {\"image\": img, \"masque\" : masque,\"image_pillow\": np.array(img_pil) , \"id\" : ID} \n",
    "        \n",
    "    def __len__(self):  \n",
    "        return len(self.mask_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fb1dc43c-0809-425e-830e-db19fcf12f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_images_paths,train_masks_paths)\n",
    "valid_dataset = CustomDataset(valid_images_paths,valid_masks_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f6b7f-2f40-4013-b0a3-53da7229a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour debugger\n",
    "for i in train_dataset:\n",
    "    print(i[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fef0a13f-fe27-4723-a882-a00eea48da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\" : 17,\n",
    "          \"freq monitoring\" : 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "94d2c498-4227-4d1e-88bd-26a0825a97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_size = 2000\n",
    "#val_size = len(all_dataset.mask_paths) - train_size\n",
    "#dans la liste donner la taille du train et la taille deu test\n",
    "#train_dataset, valid_dataset = random_split(all_dataset,[train_size,val_size], generator=torch.Generator().manual_seed(42))\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], \n",
    "                          shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=3, shuffle=True, num_workers=0)\n",
    "\n",
    "next(iter(valid_loader))[\"image\"].shape # parfait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5be83f86-f165-4182-aa8e-766a0de1aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() : device= torch.device(\"cuda:0\" )\n",
    "else : device = \"cpu\"\n",
    "\n",
    "print(\"Using {} device\".format(device))\n",
    "if torch.cuda.is_available() :\n",
    "    print(\"nom du GPU :\", torch.cuda.get_device_name(device=None))\n",
    "    print(\"GPU initialisé : \", torch.cuda.is_initialized())\n",
    "\n",
    "    # Load the pretrained model\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True) # 233 Mega\n",
    "# 1 classe !\n",
    "model.classifier[4] = nn.Conv2d(256,2,kernel_size = (1,1),stride =(1,1))\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "da9604ed-bd12-4107-86f0-a49bf79c6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the number of parameters\n",
    "total_params = 0\n",
    "\n",
    "# Loop through the parameters in the model\n",
    "for param in model.parameters():\n",
    "    # Get the size of the parameter tensor\n",
    "    size = param.size()\n",
    "    # Multiply the size of the tensor by the number of elements in it\n",
    "    num_params = torch.prod(torch.tensor(size)).item()\n",
    "    # Add the number of parameters to the total\n",
    "    total_params += num_params\n",
    "\n",
    "# Print the total number of parameters\n",
    "print(\"Total number of parameters: \", total_params) # 60 millions..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d15e1-c64f-477a-94c9-aeb1013755ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autre idée : pour un batch donné \n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    t= tqdm(train_loader, desc=\"epoch %i\" % (epoch+1),position = 0, leave=True)\n",
    "    epoch_loop = enumerate(t)\n",
    "\n",
    "    for i, data in epoch_loop:\n",
    "        \n",
    "        images = data[\"image\"].to(device)\n",
    "        labels = data[\"masque\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)[\"out\"]\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        del images, labels, output\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % config['freq monitoring'] == 0:  \n",
    "                t.set_description(\"epoch %i, 'mean loss: %.6f'\" % (epoch+1,running_loss/config['freq monitoring']))\n",
    "                t.refresh()\n",
    "                running_loss =0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70199b88-8791-48e6-9ea2-a8b59bd46826",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#Save the trained model\n",
    "torch.save(model.state_dict(), 'path/to/model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0619fbd-7793-4f72-9d01-18096b19931a",
   "metadata": {},
   "source": [
    "Wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e5bdfb-e8c4-48fd-a6d0-15b5a09b508a",
   "metadata": {},
   "source": [
    "- bouynding box de tom\n",
    "- MLFlow + ligthning torch\n",
    "- partir dans l'idée de aire un truc qui marche à peu près\n",
    "- apurement du jeu de données , trop de patch sans logements.. à enlever et c'est faisable (en filtrant au préalable sur les labels)\n",
    "- 5 min par epoch => 8h pour 100 epochs\n",
    "- la loss sembledimiué\n",
    "- Faire du wand b !!\n",
    "- évaluer le modèle sur macouria !!\n",
    "- inscription données pleiades pour el compte de la dmrg\n",
    "- dessiner au brouillon l'architecture du projet avec les entrainemenbt avec la volonté detre agnostique au type de modèle\n",
    "- validation en utilisant les labels RIL !!\n",
    "- transferabilité résolution / couluers\n",
    "- faire un taf en 2 temps test buildoing dans la zone puis segmentation\n",
    "- to do : utiliser les bounding box pour faire autre chose que de la segmentation\n",
    "- faire un énorme schéma\n",
    "- équilibrer jeu de donnée avec les 0 et 1. prendre que des exemple ou il y a des 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f5189-56a4-42ab-ae75-2990cceb9866",
   "metadata": {},
   "source": [
    "## Tests visuels du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55375398-b57c-4ae7-9258-c7513b91bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250*250 = 62500\n",
    "indice_dans_batch = 0\n",
    "data_val  = next(iter(valid_loader))\n",
    "#data_val = next(iter(train_loader))\n",
    "data_val[\"image\"].shape\n",
    "output = np.array(model(data_val[\"image\"].to(device))[\"out\"].to(\"cpu\").detach())[indice_dans_batch]\n",
    "output_predictions = output.argmax(0)\n",
    "img_init = Image.fromarray(np.array(data_val[\"image_pillow\"][indice_dans_batch]))\n",
    "\n",
    "masque = output_predictions\n",
    "show_mask = np.zeros((*masque.shape, 3))\n",
    "show_mask[masque == 1, :] = [255,255,255]\n",
    "show_mask = show_mask.astype(np.uint8)\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2, figsize = (10,10))\n",
    "ax1.imshow(img_init)\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(show_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484cfd67-bc5b-44c1-aff3-f7563fc34495",
   "metadata": {},
   "source": [
    "## pleiade !!!!! test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b479ac-f328-4ce7-a5e4-f1e0c23ecb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a9378-e832-4afd-8717-b5cfe58816d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py7zr  -q -q -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb72e7f-1cba-479e-87dc-cfa1cedd8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py7zr\n",
    "with py7zr.SevenZipFile('Cayenne.7z', mode='r') as z:\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b457a76-8deb-4dd5-95b1-692abd8ca40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasterio # marche indépendemment des instalaltions précédentes !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0c79b9f2-d28e-4ce4-8d37-17042685bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import rasterio.plot as rp\n",
    "im_dir = \"Cayenne_200722/16bits/ORT_2022072050325085_U22N/\"\n",
    "liste  = os.listdir(im_dir)\n",
    "list_path_image = [im_dir+l for l in liste]\n",
    "\n",
    "filepath = list_path_image[230] \n",
    "#filepath = 'Cayenne_200722/16bits/ORT_2022072050325085_U22N/ORT_2022072050325085_0353_0545_U22N_16Bits.jp2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "547ea636-9c94-4ca5-beb9-4a6b825e0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(filepath) as raster:\n",
    "    oviews = raster.overviews(1) # list of overviews from biggest to smallest\n",
    "    print(oviews)\n",
    "    oview = 1 # let's look at the smallest thumbnail\n",
    "\n",
    "    # NOTE this is using a 'decimated read' (http://rasterio.readthedocs.io/en/latest/topics/resampling.html)\n",
    "    B1 = raster.read(1, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "    B2 = raster.read(2, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "    B3 = raster.read(3, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "    B4 = raster.read(4, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "\n",
    "quantile = 0.97\n",
    "B1a = rp.adjust_band(np.clip(B1,0,np.quantile(B1,quantile))) # normalisation min-max simple nécessite un clip d'abord étant donné les valeurs extremes\n",
    "B2a = rp.adjust_band(np.clip(B2,0,np.quantile(B2,quantile)))\n",
    "B3a = rp.adjust_band(np.clip(B3,0,np.quantile(B3,quantile)))\n",
    "B4a = rp.adjust_band(np.clip(B4,0,np.quantile(B4,quantile)))\n",
    "\n",
    "rgb = np.dstack((B1a,B2a,B3a))\n",
    "rgbvegetal = np.dstack((B4a,B1a,B2a))\n",
    "\n",
    "im_pleiade = Image.fromarray(np.array(rgb * 255,dtype = np.uint8))\n",
    "# left top right bottom\n",
    "patch = im_pleiade.crop((0,0,250,250))\n",
    "torch_patch = torch.tensor(np.array(patch).reshape(1,3,250,250), dtype = torch.float).permute(0,3,1,2).reshape(1,3,250,250)\n",
    "\n",
    "torch_patch_2 = torch.cat((torch_patch,torch_patch),dim = 0)# il faut 2 dimensions..\n",
    "\n",
    "out = model(torch_patch_2.to(device))[\"out\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f927e642-40d8-44b7-8dee-d8c2d37bbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250*250 = 62500\n",
    "indice_dans_batch = 0\n",
    "output = np.array(model(torch_patch_2.to(device))[\"out\"].to(\"cpu\").detach())[indice_dans_batch]\n",
    "output_predictions = output.argmax(0)\n",
    "\n",
    "img_init2 = patch\n",
    "\n",
    "masque = output_predictions\n",
    "show_mask = np.zeros((*masque.shape, 3))\n",
    "show_mask[masque == 1, :] = [255,255,255]\n",
    "show_mask = show_mask.astype(np.uint8)\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2, figsize = (10,10))\n",
    "ax1.imshow(img_init2)\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(show_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ec900-7566-4a0d-9db9-32418aabc432",
   "metadata": {},
   "source": [
    "## Satistiques sur les couleurs des images\n",
    "- au programme : observation de la distribution des pixels suyrcertaines ilages pleiades et images xView\n",
    "- normalisation pour centrage réduction\n",
    "- regarderr la distribution des ratios inter pixels , les valeurs extremes etc.. faire de la stat\n",
    "- eventuellement sur plusieurs images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "db2e4579-fd96-413c-aadc-39d6c149e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(img_init2)[:,:,0].ravel(),bins = 50, density = True)\n",
    "plt.hist(np.array(img_init)[:,:,1].ravel(),bins = 50, density = True)\n",
    "plt.hist(np.array(img_init)[:,:,2].ravel(),bins = 50, density = True)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.hist(np.array(im_pleiade)[:,:,0].ravel(),bins = 50, density = True)\n",
    "plt.hist(np.array(im_pleiade)[:,:,1].ravel(),bins = 50, density = True)\n",
    "plt.hist(np.array(im_pleiade)[:,:,2].ravel(),bins = 50, density = True)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6226ccf-fa83-4a9b-8445-e14efb659fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(img_init).ravel(), bins=50, density=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.hist(np.array(im_pleiade).ravel(), bins=50, density=True)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ea0e1-53f2-43f5-b262-f5fb11437b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate mean and std\n",
    "mean, std = img_tr.mean([1,2]), img_tr.std([1,2])\n",
    " \n",
    "# print mean and std\n",
    "print(\"mean and std before normalize:\")\n",
    "print(\"Mean of the image:\", mean)\n",
    "print(\"Std of the image:\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467b4c04-3aa8-43dd-a513-8d5f3cea247d",
   "metadata": {},
   "source": [
    "# Normalser au moment de l'entrainement !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ab30e-78ec-468d-bd67-9bf9a6016f0f",
   "metadata": {},
   "source": [
    "### Les contours géométriques en Json si besoin !! bounding box etc..\n",
    "Mettre en plac eun modèlme type Yolo pour gérer les bounding box !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3edf84-a61a-462a-87e3-3be5d035a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(label_path)\n",
    "dico = jason.load(f)\n",
    "\n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b480e01-e129-467e-be78-2e92e58e8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico.keys()\n",
    "dico[\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab590c6b-684c-468f-b2fd-980217a9858e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
