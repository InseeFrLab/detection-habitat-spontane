{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install rasterio\n",
    "! pip install geopandas \n",
    "! pip install pyarrow\n",
    "! pip install matplotlib\n",
    "! pip install albumentations\n",
    "!pip install pytorch-lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from satellite_image import SatelliteImage\n",
    "from utils import *\n",
    "from plot_utils import *\n",
    "import yaml\n",
    "import re\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import gc\n",
    "import albumentations as album\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from albumentations import Compose\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Dict, Union, Optional, List\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_storage_access()\n",
    "environment = get_environment()\n",
    "root_path = get_root_path()\n",
    "bucket = environment[\"bucket\"]\n",
    "path_s3_s2looking = environment[\"sources\"][\"PAPERS\"][\"S2Looking\"]\n",
    "path_local_s2looking = environment[\"local-path\"][\"PAPERS\"]\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_local_s2looking\n",
    "#root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.download(\n",
    "        rpath=f\"{bucket}/{path_s3_s2looking}\",\n",
    "        lpath=f\"../{path_local_s2looking}\",\n",
    "        recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(f\"../{path_local_s2looking}/S2Looking.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(f\"../{path_local_s2looking}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go Pytorch Lightning ! \n",
    "Le but  que je me fixe ici est de réutiliser la logique de Tom jusqU40 ENTRAÏNEMENT AVE CpYTRCH lIGHTNING\n",
    "Ici je vais sanctuariser l'approche par chemin de fichier où je charge une image et je sélectionne une seule aléatoirement dans l'ensemble ?\n",
    "METTRE EN OPALCE DES METRIQUE SPOUR mlflow t& score etc;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = f\"../{path_local_s2looking}/S2Looking/train/\"\n",
    "list_name = sorted(os.listdir(os.path.join(train_path,\"Image1\")))\n",
    "list_path_image1 =  [ os.path.join(train_path,\"Image1/\")+ name for name  in list_name]\n",
    "list_path_image2 =  [ os.path.join(train_path,\"Image2/\")+ name for name  in list_name]\n",
    "list_path_label =  [ os.path.join(train_path,\"label/\")+ name for name  in list_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import change_detection_triplet\n",
    "idx = 40\n",
    "pthimg1 = list_path_image1[idx]\n",
    "pthimg2 = list_path_image2[idx]\n",
    "pthlabel = list_path_label[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from change_detection_triplet import ChangedetectionTripletS2Looking\n",
    "cdtriplet = ChangedetectionTripletS2Looking(pthimg1,pthimg2,pthlabel)\n",
    "cdtriplet.random_crop(512)\n",
    "cdtriplet.plot()\n",
    "\n",
    "#.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fonction de découpage aléatoire dans utils d'une image Pil (taille 250) ? regarder le crop aléatoire de l'autre notebook, l'intégrer dans la classe Dataset.\n",
    "- Entraîner le Unet++ ?\n",
    "- créer cette classe plutôt pour l'évaluation quali du réseau  \n",
    "- créer une classe triplet Satellites Images \n",
    "- (finalement ces classes sont plus utiles pour représenter et garder l'info géo ?)\n",
    "- fontion  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# création d'une classe Data set pour chopper les images\n",
    "class ChangeDetectionS2LookingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        list_paths_image1: List,\n",
    "        list_paths_image2: List,\n",
    "        list_paths_labels: List,\n",
    "        transforms: Optional[Compose] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Args:\n",
    "            list_paths_image1 (List): list of path of the before state pictures\n",
    "            list_paths_image2 (List): list of paths containing  the \"after\" state pictures\n",
    "            list_paths_labels (List): list of paths containing the labeled differences (mostly segmentation masque showing the differencer between image 1 and image 2) \n",
    "        \"\"\"\n",
    "        self.list_paths_image1 = list_paths_image1\n",
    "        self.list_paths_image2 = list_paths_image2\n",
    "        self.list_paths_labels = list_paths_labels    \n",
    "        self.transforms = transforms\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            idx (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        pathim1 = self.list_paths_image1[idx]\n",
    "        pathim2 = self.list_paths_image2[idx]\n",
    "        pathlabel = self.list_paths_labels[idx]\n",
    "        \n",
    "        label = 0\n",
    "        compteur = 0\n",
    "        while(np.max(label) == 0 and compteur < 15):\n",
    "            cdtriplet = ChangedetectionTripletS2Looking(pthimg1,pthimg2,pthlabel)\n",
    "            cdtriplet.random_crop(256)\n",
    "            label = np.array(cdtriplet.label)\n",
    "            label[label!=0] = 1\n",
    "            compteur += 1\n",
    "        \n",
    "        #img1 = np.transpose(np.array(cdtriplet.image1),(2,0,1))\n",
    "        #img2 = np.transpose(np.array(cdtriplet.image2),(2,0,1))\n",
    "        \n",
    "        img1 = np.array(cdtriplet.image1)\n",
    "        img2 = np.array(cdtriplet.image2)\n",
    "        \n",
    "        #print(img1.shape)\n",
    "        \n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = img1, image2 = img2, mask = label)\n",
    "            img1 = sample['image']\n",
    "            img2 = sample['image2']\n",
    "            label = sample['mask']\n",
    "        else:\n",
    "            img1 = torch.tensor(np.transpose(img1,(2,0,1)))\n",
    "            img2 = torch.tensor(np.transpose(img2,(2,0,1)))\n",
    "        \n",
    "        img_double =torch.concatenate([img1,img2],axis = 0).squeeze()\n",
    "        \n",
    "        img_double = img_double.type(torch.float)\n",
    "        label = label.type(torch.LongTensor)\n",
    "        \n",
    "        return {\"image\" : img_double, \"label\" : label , \"pathim1\" : pathim1, \"pathim2\" : pathim2, \"pathlabel\" : pathlabel }\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.list_paths_image1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_path = f\"../{path_local_s2looking}/S2Looking/train/\"\n",
    "list_name = sorted(os.listdir(os.path.join(train_path,\"Image1\")))\n",
    "train_path_image1 =  [ os.path.join(train_path,\"Image1/\")+ name for name  in list_name]\n",
    "train_path_image2 =  [ os.path.join(train_path,\"Image2/\")+ name for name  in list_name]\n",
    "train_path_label =  [ os.path.join(train_path,\"label/\")+ name for name  in list_name]\n",
    "\n",
    "image_size = (256,256)\n",
    "transforms = album.Compose(\n",
    "        [\n",
    "            album.Resize(300, 300, always_apply=True),\n",
    "            album.RandomResizedCrop(\n",
    "                *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "            ),\n",
    "            album.HorizontalFlip(),\n",
    "            album.VerticalFlip(),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "       ],\n",
    "        additional_targets={'image2': 'image'}\n",
    "    )\n",
    " \n",
    "ds = ChangeDetectionS2LookingDataset(\n",
    "        train_path_image1,\n",
    "        train_path_image2,\n",
    "        train_path_label,\n",
    "        transforms\n",
    " )\n",
    "\n",
    "next(iter(ds))[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class S2LookingDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    Pytorch Lightning Data Module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data: List[str],\n",
    "        valid_data: List[str],\n",
    "        test_data:  List[str],\n",
    "        transforms_preprocessing: Optional[Compose] = None,\n",
    "        transforms_augmentation: Optional[Compose] = None,\n",
    "        batch_size: int = 20,\n",
    "        num_workers: int = 4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Data Module constructor.\n",
    "\n",
    "        Args:\n",
    "            train_data (List): List of training (and validation) instances\n",
    "            test_data (List): List of test instances\n",
    "            transforms_preprocessing (Optional[Compose]): Compose object\n",
    "                from albumentations applied on validation and test datasets.\n",
    "            transforms_augmentation (Optional[Compose]): Compose object\n",
    "                from albumentations applied on training dataset.\n",
    "            batch_size (int): Batch size.\n",
    "            num_workers (int): Number of workers to process data.\n",
    "            bands_indices (List): List of indices of bands to plot.\n",
    "                The indices should be integers between 0 and the\n",
    "                number of bands - 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_data = train_data # list containing image1 path, imaghe 2 paths and label paths\n",
    "        self.valid_data = valid_data\n",
    "        self.test_data = test_data\n",
    "        self.transforms_preprocessing = transforms_preprocessing\n",
    "        self.transforms_augmentation = transforms_augmentation\n",
    "        #self.transforms_preprocessing = False\n",
    "        #self.transforms_augmentation = False\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Start training, validation and test datasets.\n",
    "\n",
    "        Args:\n",
    "            stage (Optional[str]): Used to separate setup logic\n",
    "                for trainer.fit and trainer.test.\n",
    "        \"\"\"\n",
    "        n_samples = len(self.train_data[0])\n",
    "        \n",
    "        self.dataset_train = ChangeDetectionS2LookingDataset(\n",
    "            self.train_data[0],\n",
    "            self.train_data[1],\n",
    "            self.train_data[2],\n",
    "            transforms=self.transforms_augmentation,\n",
    "        )\n",
    "            \n",
    "            \n",
    "         \n",
    "        self.dataset_val = ChangeDetectionS2LookingDataset(\n",
    "            self.valid_data[0],\n",
    "            self.valid_data[1],\n",
    "            self.valid_data[2],\n",
    "            transforms=self.transforms_preprocessing\n",
    "        )\n",
    "        \n",
    "        self.dataset_test = ChangeDetectionS2LookingDataset(\n",
    "            self.test_data[0],\n",
    "            self.test_data[1],\n",
    "            self.test_data[2],\n",
    "            transforms=self.transforms_preprocessing\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Create Dataloader.\n",
    "        Returns: DataLoader\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.dataset_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Create Dataloader.\n",
    "        Returns: DataLoader\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.dataset_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        \"\"\"Create Dataloader.\n",
    "        Returns: DataLoader\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.dataset_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test du S2Looking Data Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le modèle et l'Opti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DeepLabv3Module(nn.Module):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.segmentation.deeplabv3_resnet101(\n",
    "            weights=\"DeepLabV3_ResNet101_Weights.DEFAULT\"\n",
    "        )\n",
    "        # 1 classe !\n",
    "        self.model.classifier[4] = nn.Conv2d(\n",
    "            256, 2, kernel_size=(1, 1), stride=(1, 1)\n",
    "        )\n",
    "        self.model.backbone[\"conv1\"] = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding = (3,3), bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \"\"\"\n",
    "        return self.model(x)[\"out\"]\n",
    "\n",
    "\n",
    "class DeepLabv3LitModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Pytorch Lightning Module for DeepLabv3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: DeepLabv3Module,\n",
    "        optimizer: Union[optim.SGD, optim.Adam],\n",
    "        optimizer_params: Dict,\n",
    "        scheduler: Union[\n",
    "            optim.lr_scheduler.OneCycleLR, optim.lr_scheduler.ReduceLROnPlateau\n",
    "        ],\n",
    "        scheduler_params: Dict,\n",
    "        scheduler_interval: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize TableNet Module.\n",
    "        Args:\n",
    "            model\n",
    "            optimizer\n",
    "            optimizer_params\n",
    "            scheduler\n",
    "            scheduler_params\n",
    "            scheduler_interval\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.scheduler = scheduler\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_interval = scheduler_interval\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Perform forward-pass.\n",
    "        Args:\n",
    "            batch (tensor): Batch of images to perform forward-pass.\n",
    "        Returns (Tuple[tensor, tensor]): Table, Column prediction.\n",
    "        \"\"\"\n",
    "        return self.model(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "        Args:\n",
    "            batch (List[Tensor]): Data for training.\n",
    "            batch_idx (int): batch index.\n",
    "        Returns: Tensor\n",
    "        \"\"\"\n",
    "        samples =  batch[\"image\"].to(self.device)\n",
    "        labels = batch[\"label\"].to(self.device)\n",
    "        \n",
    "        \n",
    "        output = self.forward(samples)\n",
    "\n",
    "        loss = self.loss(output, labels)\n",
    "        self.log(\"loss\", loss)\n",
    "        \n",
    "        tqdm_dict = {'train_loss': loss}\n",
    "        outputs = {\n",
    "            'loss': loss,\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': tqdm_dict\n",
    "        }\n",
    "        del samples, labels\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "        Args:\n",
    "            batch (List[Tensor]): Data for training.\n",
    "            batch_idx (int): batch index.\n",
    "        Returns: Tensor\n",
    "        \"\"\"\n",
    "  \n",
    "        \n",
    "        samples =  batch[\"image\"].to(self.device)\n",
    "        labels = batch[\"label\"].to(self.device)\n",
    "        \n",
    "        output = self.forward(samples)\n",
    "\n",
    "        loss = self.loss(output, labels)\n",
    "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "        \n",
    "        del samples, labels\n",
    "       \n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Test step.\n",
    "        Args:\n",
    "            batch (List[Tensor]): Data for training.\n",
    "            batch_idx (int): batch index.\n",
    "        Returns: Tensor\n",
    "        \"\"\"\n",
    "        samples =  batch[\"image\"].to(self.device)\n",
    "        labels = batch[\"label\"].to(self.device)\n",
    "        \n",
    " \n",
    "        output = self.forward(samples)[\"out\"]\n",
    "\n",
    "        loss = self.loss(output, labels)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizer for pytorch lighting.\n",
    "        Returns: optimizer and scheduler for pytorch lighting.\n",
    "        \"\"\"\n",
    "        optimizer = self.optimizer(self.parameters(), **self.optimizer_params)\n",
    "        scheduler = self.scheduler(optimizer, **self.scheduler_params)\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"monitor\": \"validation_loss\",\n",
    "            \"interval\": self.scheduler_interval,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Loading images\n",
    "    # DataModule definition\n",
    "    # Some additional normalization is done here\n",
    "image_size = (256, 256)\n",
    "transforms_augmentation = album.Compose(\n",
    "        [\n",
    "            album.Resize(300, 300, always_apply=True),\n",
    "            album.RandomResizedCrop(\n",
    "                *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "            ),\n",
    "            album.HorizontalFlip(),\n",
    "            album.VerticalFlip(),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "       ],\n",
    "        additional_targets={'image2': 'image'}\n",
    "    )\n",
    "\n",
    "\n",
    "transforms_preprocessing = album.Compose(\n",
    "        [\n",
    "            album.Resize(*image_size, always_apply=True),\n",
    "            album.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        additional_targets={'image2': 'image'} # pour pouvoir appliquer la même transfo à 2 images pour un seul masque (cd detection)\n",
    ")\n",
    "\n",
    "\n",
    "train_path = f\"../{path_local_s2looking}/S2Looking/train/\"\n",
    "list_name = sorted(os.listdir(os.path.join(train_path,\"Image1\")))\n",
    "train_path_image1 =  [ os.path.join(train_path,\"Image1/\")+ name for name  in list_name]\n",
    "train_path_image2 =  [ os.path.join(train_path,\"Image2/\")+ name for name  in list_name]\n",
    "train_path_label =  [ os.path.join(train_path,\"label/\")+ name for name  in list_name]\n",
    "\n",
    "\n",
    "valid_path = f\"../{path_local_s2looking}/S2Looking/val/\"\n",
    "list_name = sorted(os.listdir(os.path.join(valid_path,\"Image1\")))\n",
    "valid_path_image1 =  [ os.path.join(valid_path,\"Image1/\")+ name for name  in list_name]\n",
    "valid_path_image2 =  [ os.path.join(valid_path,\"Image2/\")+ name for name  in list_name]\n",
    "valid_path_label =  [ os.path.join(valid_path,\"label/\")+ name for name  in list_name]\n",
    "\n",
    "test_path = f\"../{path_local_s2looking}/S2Looking/test/\"\n",
    "list_name = sorted(os.listdir(os.path.join(test_path,\"Image1\")))\n",
    "test_path_image1 =  [ os.path.join(test_path,\"Image1/\")+ name for name  in list_name]\n",
    "test_path_image2 =  [ os.path.join(test_path,\"Image2/\")+ name for name  in list_name]\n",
    "test_path_label =  [ os.path.join(test_path,\"label/\")+ name for name  in list_name]\n",
    "\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\"lr\": 0.0001, \"momentum\": 0.9}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n",
    "\n",
    "model = DeepLabv3Module()\n",
    "\n",
    "data_module = S2LookingDataModule(\n",
    "    train_data=[train_path_image1, train_path_image2, train_path_label],\n",
    "    valid_data =[valid_path_image1, valid_path_image2, valid_path_label],\n",
    "    test_data =[test_path_image1, test_path_image2, test_path_label],\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1, # 15 coeurs déjà\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "\n",
    "lightning_module = DeepLabv3LitModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"min\"\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"min\", patience=3\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "gpus = 1\n",
    "strategy = \"ddp\" if gpus > 1 else None\n",
    "strategy =\"auto\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[lr_monitor, checkpoint_callback, early_stop_callback],\n",
    "    max_epochs=100,\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2\n",
    ")\n",
    "trainer.fit(lightning_module, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reprendre l'entrainement \n",
    "lightning_module_checkpoint = lightning_module.load_from_checkpoint(\n",
    "    checkpoint_path=trainer.checkpoint_callback.best_model_path, # je créé un module qui charge\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval\n",
    "                                     )\n",
    "trainer.fit(lightning_module, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation graphique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère les modèles entraînés par callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lightning_module_checkpoint = lightning_module.load_from_checkpoint(\n",
    "    checkpoint_path=trainer.checkpoint_callback.best_model_path, # je créé un module qui charge\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval\n",
    "                                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iterateur = iter(data_module.dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iterateur)\n",
    "sample = batch[\"image\"]\n",
    "label = np.array(batch[\"label\"])\n",
    "sample = torch.tensor(sample).unsqueeze(0).type(torch.float)\n",
    "model = lightning_module_checkpoint.model.to(\"cpu\")\n",
    "model.eval()\n",
    "res = model(sample)\n",
    "print(res.shape)\n",
    "mask = torch.argmax(res,axis = 1)\n",
    "\n",
    "\n",
    "print(np.sum(label))\n",
    "print(torch.max(batch[\"label\"]),torch.max(mask))\n",
    "\n",
    "\n",
    "#np.array(mask)\n",
    "#print(np.array(batch[\"image\"][:3]))\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "ax1.imshow(label, cmap='binary')\n",
    "ax2.imshow(np.array(mask.squeeze()), cmap='binary')\n",
    "plt.show()\n",
    "cdtriplet = ChangedetectionTripletS2Looking(batch[\"pathim1\"],batch[\"pathim2\"],batch[\"pathlabel\"])\n",
    "cdtriplet.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment représenter au mieux l'image en entrée ?  gard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilan \n",
    "- Comment éviter la machine à Gaz ?\n",
    "- trop de patch avec 0 change  -> le modèle s'entraine à prédire rien du tout..\n",
    "- forcer les exemple à contenir un bout de changement \n",
    "- Probèmes de mémoire, taille du batch etc..\n",
    "- chargement par filepath avec CROP aléatoire + au moins un pixel blanc\n",
    "- but représenter les images entrée et on est bon -> easy garder les liens de fichiers dan s la sortie du data set <3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Entrianement sur vrais données\n",
    "- acquisition des images à l'instant t ?\n",
    "- Sentinel 2 ? stacking ? teledetection ? packages existant - traitement des nuages ? moyennisation ? Sentil 1\n",
    "1) Entrainement sur données réelles + continuer littérature CD et écrire\n",
    "- utiliser le RIL pour extirper des changements ?\n",
    "- peut etre su'avec la BDTOPO on peut detecter les changements ?\n",
    "RIL mise à jour :\n",
    "- RIL réunion pour détecter les changements ?\n",
    "2) Méthode non supervisée ACP pour rigoler en patchs\n",
    "3) Factoriser un peu les codes ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
