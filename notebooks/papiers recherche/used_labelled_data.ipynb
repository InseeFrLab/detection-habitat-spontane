{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0758f67b-7497-4d0f-be0f-a000fb6da6c8",
   "metadata": {},
   "source": [
    "## Utilisation du jeu de données Xview2, qui intitialement présebnte des exemples de territoire avant et après dommage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73560aa1-5db5-4cfb-a536-c84403c33dc6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Je m'écarte ici un peu de l'idée initiale qui était de produire un dataset à l'aide du RIl et de la BDTOPO et je me concentre + sur les datasets labellisés préexistants. J'entraine un modèle de segmentation de suus ou un modèle de déttection d'objet et je vois comment ça réagit sur donnée spleiades. \n",
    "Dans tousd les cas le travail sur le RIL et la BD TOPO est à conserver puisque ces derniers servent de vzlidation !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3a118d9-8fb9-4611-9629-2fdb6a7ced0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install -q -q -q tqdm # progresbar\n",
    "!pip install rasterio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3b10de5-1360-4cda-ac46-9dec8016c9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import s3fs\n",
    "import tarfile\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader,  random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "import rasterio.plot as rp\n",
    "\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},key ='NB6O11616WOKGFCOQCVT', secret = 'ZdAeVa22+jOCsZIaaYB+e0+EH3gyxp+9XaAf++2j', token = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJOQjZPMTE2MTZXT0tHRkNPUUNWVCIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNjc3NzU3NTIyLCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6ImNsZW1lbnQuZ3VpbGxvQGluc2VlLmZyIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImV4cCI6MTY3Nzk2MzkwMiwiZmFtaWx5X25hbWUiOiJHdWlsbG8iLCJnaXZlbl9uYW1lIjoiQ2zDqW1lbnQiLCJncm91cHMiOlsiY2hhbGxlbmdlZGF0YS1lbnMiLCJmdW5hdGhvbiIsInNsdW1zLWRldGVjdGlvbiJdLCJpYXQiOjE2Nzc3NTc1MjMsImlzcyI6Imh0dHBzOi8vYXV0aC5sYWIuc3NwY2xvdWQuZnIvYXV0aC9yZWFsbXMvc3NwY2xvdWQiLCJqdGkiOiIwYzE5ODVlMC04MTc1LTQxZDQtOTM1OS1jNTA1ZmJlMTJhNjAiLCJsb2NhbGUiOiJlbiIsIm5hbWUiOiJDbMOpbWVudCBHdWlsbG8iLCJub25jZSI6IjQ0OGQwNjBhLTk5OTAtNDg0NC1hMTNkLTFkOTlmMGRhZDE4OSIsInBvbGljeSI6InN0c29ubHkiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJjZ3VpbGxvIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNlc3Npb25fc3RhdGUiOiI0YTFkNmNjMC04MzY1LTRlMjQtYWNjOC0zZTBiNjQ2ZGEwMzIiLCJzaWQiOiI0YTFkNmNjMC04MzY1LTRlMjQtYWNjOC0zZTBiNjQ2ZGEwMzIiLCJzdWIiOiIzYjA2ZWZhNC01OWZlLTQzYzgtYTAyYi1hOTRkOWI0YjU0NGUiLCJ0eXAiOiJCZWFyZXIifQ.r_F1qDFfHJmz4hmiv3r3z1pM857ITy2nZIu8HI77VhYT-769AL6C4-QGakWU_L86xgoukTVgxBGSRu5Wglw4GA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa7c3a3-c558-4f44-a1e5-243db46a0a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# je travaille sur une version minimale du dataset xview, le vrai fait 50 giga.. mais ça devrait déjà faire l'affaire pour travailler\n",
    "fs.get('projet-slums-detection/Donnees/data_xBD.tar', 'data_xBD.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ec31159-1d5f-4bf6-a688-67b7d85a65ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"data_xBD.tar\", \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb79d8-72e1-4c6a-a450-c32ed061b1fd",
   "metadata": {},
   "source": [
    "### chargement/observation en place des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a199b3d3-21f9-4533-8af9-a99568a52211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "liste_image  = sorted(os.listdir(\"train/images/\"))\n",
    "liste_label  = sorted(os.listdir(\"train/labels/\")) # boundingbox et polygones !!\n",
    "liste_target  = sorted(os.listdir(\"train/targets/\")) # le masque de segmentation !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76aa5d2c-8907-4ed7-be4c-4d4d445227b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.unique([nom_image.split(\"_\")[0] for nom_image in liste_image])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac233ac-45d9-4e5f-968e-f0eda92cb047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selec_pre_disaster = [nom_image.split(\"_\")[2] == \"pre\" for nom_image in liste_image]\n",
    "selec_tout_sauf_social_fire = [ not nom_image.split(\"_\")[0] in [\"social-fire\",\"palu-tsunami\"] for nom_image in liste_image]\n",
    "\n",
    "selec_train = np.array(selec_pre_disaster) * selec_tout_sauf_social_fire\n",
    "selec_val = np.array(selec_pre_disaster) * list(map(lambda x: not x, selec_tout_sauf_social_fire))\n",
    "\n",
    "\n",
    "train_images_paths = [\"train/images/\" + elt for elt in np.array(liste_image)[selec_train]]\n",
    "train_masks_paths = [\"train/targets/\" + elt for elt in np.array(liste_target)[selec_train]]\n",
    "\n",
    "valid_images_paths = [\"train/images/\" + elt for elt in np.array(liste_image)[selec_val]]\n",
    "valid_masks_paths = [\"train/targets/\" + elt for elt in np.array(liste_target)[selec_val]]\n",
    "\n",
    "\n",
    "sum(selec_pre_disaster)\n",
    "sum(selec_train)\n",
    "sum(selec_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35017c79-30e5-431b-8bb1-ddee599d83f3",
   "metadata": {},
   "source": [
    "## Observation du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43cf49-6ea6-41a3-87cb-4c7477a97748",
   "metadata": {},
   "source": [
    "- Images de dimension 1024-1024 à découper en 4 * 250 pour avoir un diviseur de 2000 (pour les données pleiades) (donc en 4)\n",
    "- Dans la classe data set splitter l'image en 4 et prendre un bout aléatoirement à chaque fois\n",
    "- image à 3 channels, pas de RGB ici..\n",
    "- est ce vraiment la mêlme résolution que pleiade ? résistance à la résolution ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d940dca-39fc-4e38-adc1-fa9fe0210722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open(train_images_paths[350]) # 30 ok\n",
    "img = img.resize((1000,1000))\n",
    "\n",
    "masque = Image.open(train_masks_paths[350])\n",
    "masque = np.array(masque)\n",
    "show_mask = np.zeros((*masque.shape, 3))\n",
    "show_mask[masque == 1, :] = [255,255,255]\n",
    "show_mask = show_mask.astype(np.uint8)\n",
    "\n",
    "# On traçe\n",
    "fig,(ax1,ax2) = plt.subplots(1,2, figsize = (10,10))\n",
    "ax1.imshow(img)\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(show_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2e06c-671b-447c-9348-dc14abe58f57",
   "metadata": {},
   "source": [
    "A mettre dans la classe dataset ! sélection d'un pa(tch aléatoire parmi les 16  possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90abee8a-1c54-413e-adb8-ec1b609f533f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open(train_images_paths[35])\n",
    "\n",
    "img = img.crop((0,0,1000,1000))# je dégomme les derniers pixels..\n",
    "img\n",
    "\n",
    "facteur_div = 250\n",
    "width, height = img.size\n",
    "\n",
    "num_subparts_x = width//facteur_div\n",
    "num_subparts_y =  height//facteur_div\n",
    "\n",
    "# sélection aléatoire d'une aprtie de l'image pour le dataset\n",
    "i = np.random.randint(num_subparts_x)\n",
    "j = np.random.randint(num_subparts_y)\n",
    "\n",
    "print(i,j)\n",
    "\n",
    "left = j * facteur_div\n",
    "right = (j+1) * facteur_div\n",
    "top = i * facteur_div\n",
    "bottom =(i+1)*facteur_div\n",
    "\n",
    "out = img.crop((left,top,right,bottom))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48d7a7-0330-400a-bd48-ea46a2b6fc10",
   "metadata": {},
   "source": [
    "Polygones associés au bati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d252a4-2e20-4111-89b1-7921a01d3c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths,mask_paths, facteur_div = 250):   # initial logic happens like transform\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.facteur_div = 250\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        with Image.open(self.image_paths[idx]) as img :\n",
    "            \n",
    "            img_pour_mean = torch.tensor(np.array(img,dtype = float), dtype =torch.float).permute(2,0,1)\n",
    "            mean, std = img_pour_mean.mean([1,2]), img_pour_mean.std([1,2]) # moyenne sur toutes l'image avant patch sinon on a des cas dégénérés\n",
    "            \n",
    "            width, height = img.size\n",
    "\n",
    "            num_subparts_x = width//self.facteur_div\n",
    "            num_subparts_y =  height//self.facteur_div\n",
    "            # sélection aléatoire d'une aprtie de l'image pour le dataset\n",
    "            i = np.random.randint(num_subparts_x)\n",
    "            j = np.random.randint(num_subparts_y)\n",
    "\n",
    "            left = j * self.facteur_div\n",
    "            right = (j+1) * self.facteur_div\n",
    "            top = i * self.facteur_div\n",
    "            bottom =(i+1)*self.facteur_div\n",
    "\n",
    "            img = img.crop((left,top,right,bottom))\n",
    "            img = img.convert(\"RGB\")\n",
    "            img_pil = img\n",
    "            \n",
    "            \n",
    "            \n",
    "        with Image.open(self.mask_paths[idx]) as masque :\n",
    "            masque = masque.crop((left,top,right,bottom))\n",
    "            masque = np.array(masque)\n",
    "         \n",
    "        \n",
    "        masque = torch.tensor(masque,dtype = torch.long)\n",
    "        #img.convert(\"RGB\")\n",
    "        img = torch.tensor(np.array(img,dtype = float), dtype =torch.float).permute(2,0,1)\n",
    "        \n",
    "        #if any(std == 0):\n",
    "         #   mean = [0,0,0] \n",
    "          #  std = [1,1,1]\n",
    "            \n",
    "        img = transforms.Normalize(mean, std)(img)\n",
    "        ID = str(self.image_paths[idx])\n",
    "     \n",
    "        return {\"image\": img, \"masque\" : masque,\"image_pillow\": np.array(img_pil) , \"id\" : ID} \n",
    "        \n",
    "    def __len__(self):  \n",
    "        return len(self.mask_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb1dc43c-0809-425e-830e-db19fcf12f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_images_paths,train_masks_paths)\n",
    "valid_dataset = CustomDataset(valid_images_paths,valid_masks_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fef0a13f-fe27-4723-a882-a00eea48da6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\"batch_size\" : 17,\n",
    "          \"freq monitoring\" : 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94d2c498-4227-4d1e-88bd-26a0825a97b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#train_size = 2000\n",
    "#val_size = len(all_dataset.mask_paths) - train_size\n",
    "#dans la liste donner la taille du train et la taille deu test\n",
    "#train_dataset, valid_dataset = random_split(all_dataset,[train_size,val_size], generator=torch.Generator().manual_seed(42))\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], \n",
    "                          shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=3, shuffle=True, num_workers=0)\n",
    "\n",
    "next(iter(valid_loader))[\"image\"].shape # parfait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5be83f86-f165-4182-aa8e-766a0de1aa31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() : device= torch.device(\"cuda:0\" )\n",
    "else : device = \"cpu\"\n",
    "\n",
    "print(\"Using {} device\".format(device))\n",
    "if torch.cuda.is_available() :\n",
    "    print(\"nom du GPU :\", torch.cuda.get_device_name(device=None))\n",
    "    print(\"GPU initialisé : \", torch.cuda.is_initialized())\n",
    "\n",
    "    # Load the pretrained model\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False) # 233 Mega\n",
    "# 1 classe !\n",
    "model.classifier[4] = nn.Conv2d(256,2,kernel_size = (1,1),stride =(1,1))\n",
    "\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d4ee7ab2-cfcb-40a0-bb85-86c56566e0cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da9604ed-bd12-4107-86f0-a49bf79c6b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "# Initialize the number of parameters\n",
    "total_params = 0\n",
    "\n",
    "# Loop through the parameters in the model\n",
    "for param in model.parameters():\n",
    "    # Get the size of the parameter tensor\n",
    "    size = param.size()\n",
    "    # Multiply the size of the tensor by the number of elements in it\n",
    "    num_params = torch.prod(torch.tensor(size)).item()\n",
    "    # Add the number of parameters to the total\n",
    "    total_params += num_params\n",
    "\n",
    "# Print the total number of parameters\n",
    "print(\"Total number of parameters: \", total_params) # 60 millions..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d15e1-c64f-477a-94c9-aeb1013755ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# autre idée : pour un batch donné \n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    t= tqdm(train_loader, desc=\"epoch %i\" % (epoch+1),position = 0, leave=True)\n",
    "    epoch_loop = enumerate(t)\n",
    "\n",
    "    for i, data in epoch_loop:\n",
    "        \n",
    "        images = data[\"image\"].to(device)\n",
    "        labels = data[\"masque\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)[\"out\"]\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        del images, labels, output\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % config['freq monitoring'] == 0:  \n",
    "                t.set_description(\"epoch %i, 'mean loss: %.6f'\" % (epoch+1,running_loss/config['freq monitoring']))\n",
    "                t.refresh()\n",
    "                running_loss =0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "70199b88-8791-48e6-9ea2-a8b59bd46826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "#Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0619fbd-7793-4f72-9d01-18096b19931a",
   "metadata": {},
   "source": [
    "Wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f5189-56a4-42ab-ae75-2990cceb9866",
   "metadata": {},
   "source": [
    "## Tests visuels du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "55375398-b57c-4ae7-9258-c7513b91bc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 250*250 = 62500\n",
    "model.eval()\n",
    "indice_dans_batch = 0\n",
    "data_val  = next(iter(valid_loader))\n",
    "#data_val = next(iter(train_loader))\n",
    "data_val[\"image\"].shape\n",
    "output = np.array(model(data_val[\"image\"].to(device))[\"out\"].to(\"cpu\").detach())[indice_dans_batch]\n",
    "output_predictions = output.argmax(0)\n",
    "img_init = Image.fromarray(np.array(data_val[\"image_pillow\"][indice_dans_batch]))\n",
    "\n",
    "masque = output_predictions\n",
    "show_mask = np.zeros((*masque.shape, 3))\n",
    "show_mask[masque == 1, :] = [255,255,255]\n",
    "show_mask = show_mask.astype(np.uint8)\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2, figsize = (10,10))\n",
    "ax1.imshow(img_init)\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(show_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8dd55-d942-48d6-9b22-322932ce6f82",
   "metadata": {},
   "source": [
    "Marche bien sur les images de validation !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484cfd67-bc5b-44c1-aff3-f7563fc34495",
   "metadata": {},
   "source": [
    "## pleiade !!!!! test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b479ac-f328-4ce7-a5e4-f1e0c23ecb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aae8bf50-2ef3-4bc1-ae2e-c30ef0c089bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs \n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},key ='BG8A88QOYOCCC4H3WDD3', secret = 'CuwsRVZZ1zwI7hxh4SKe6eF2ynwjfZCrp4nLf4rx', token = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJCRzhBODhRT1lPQ0NDNEgzV0REMyIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNjc1OTY2ODAwLCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6ImNsZW1lbnQuZ3VpbGxvQGluc2VlLmZyIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImV4cCI6MTY3NjQ1OTEwMSwiZmFtaWx5X25hbWUiOiJHdWlsbG8iLCJnaXZlbl9uYW1lIjoiQ2zDqW1lbnQiLCJncm91cHMiOlsiY2hhbGxlbmdlZGF0YS1lbnMiLCJmdW5hdGhvbiIsInNsdW1zLWRldGVjdGlvbiJdLCJpYXQiOjE2NzU5NjY4MDEsImlzcyI6Imh0dHBzOi8vYXV0aC5sYWIuc3NwY2xvdWQuZnIvYXV0aC9yZWFsbXMvc3NwY2xvdWQiLCJqdGkiOiIxNjZmNTlhNC0wNzcxLTQwYTMtYmQzOS0yZWY3NDIwNjZjOTMiLCJsb2NhbGUiOiJlbiIsIm5hbWUiOiJDbMOpbWVudCBHdWlsbG8iLCJub25jZSI6IjNiY2RhYjFjLTg1ZTAtNGM3Yi05ZjQ5LTIyN2JlNjUyZDU4OCIsInBvbGljeSI6InN0c29ubHkiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJjZ3VpbGxvIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNlc3Npb25fc3RhdGUiOiJkNzUyNTg3Mi0wMWYxLTRiODUtYmYxMy0zOGYzMTRmZGIxZGEiLCJzaWQiOiJkNzUyNTg3Mi0wMWYxLTRiODUtYmYxMy0zOGYzMTRmZGIxZGEiLCJzdWIiOiIzYjA2ZWZhNC01OWZlLTQzYzgtYTAyYi1hOTRkOWI0YjU0NGUiLCJ0eXAiOiJCZWFyZXIifQ.75iqRq2DnaaXXvGfpHVfZi0ulrNZsp7xdi-QZr2TFg4qY0WjlI3CzpyvCBdUnNtSEB5pCtFtMAwzCkdYpZLtPQ')\n",
    "fs.get('projet-slums-detection/Donnees/PLEIADES/Cayenne_200722', 'Cayenne_200722')\n",
    "\n",
    "# DL du contenu d'un dossier avec récursive\n",
    "fs.download(\n",
    "        rpath=\"projet-slums-detection/Donnees/PLEIADES/Cayenne_200722\",\n",
    "        lpath=\"../notebooks/Cayenne_200722\",\n",
    "        recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b457a76-8deb-4dd5-95b1-692abd8ca40e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install rasterio  -q -q -q # marche indépendemment des instalaltions précédentes !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c766c6-2784-4b7b-969a-2b4cc88cf910",
   "metadata": {},
   "source": [
    "Pour ne pas galérer à savoir l'image que je veux représenter je vais toutes les ouvrir en bouclant et je les représente, je les ouvre avac un oviews =16 pour diminuer la taille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9c813e0-53f1-4101-a630-6a3ff52c7e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im_dir = \"Cayenne_200722/16bits/ORT_2022072050325085_U22N/\"\n",
    "liste  = os.listdir(im_dir)\n",
    "list_path_image = np.array([im_dir+l for l in liste])\n",
    "\n",
    "Y = np.array([path_image.split(\"_\")[6] for path_image in list_path_image])\n",
    "order_y = np.argsort(np.array(Y))\n",
    "Y = Y[order_y]\n",
    "list_path_image = list_path_image[order_y]\n",
    "\n",
    "X = [path_image.split(\"_\")[5] for path_image in list_path_image]\n",
    "list_path_image = list_path_image[np.lexsort((Y,X))]\n",
    "list_path_image[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43991499-357f-4676-b59f-8dbfd9665c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_path_image.reshape(15,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79885df1-0ef2-4e3a-8ab0-de11a38074d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "313d3610-1708-417c-a04b-c688bbdcd6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_cols = 16\n",
    "num_rows = 15\n",
    "\n",
    "mat_list_path_image = list_path_image.reshape(15,-1)\n",
    "images = np.empty((num_rows,num_cols), dtype = object)\n",
    "\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        filepath = mat_list_path_image[i,j]\n",
    "        with rasterio.open(filepath) as raster:\n",
    "            oviews = raster.overviews(1) # list of overviews from biggest to smallest\n",
    "            oview = 16 # let's look at the smallest thumbnail\n",
    "\n",
    "            # NOTE this is using a 'decimated read' (http://rasterio.readthedocs.io/en/latest/topics/resampling.html)\n",
    "            B1 = raster.read(1, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "            B2 = raster.read(2, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "            B3 = raster.read(3, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "            B4 = raster.read(4, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "\n",
    "        quantile = 0.97\n",
    "        B1a = rp.adjust_band(np.clip(B1,0,np.quantile(B1,quantile))) # normalisation min-max simple nécessite un clip d'abord étant donné les valeurs extremes\n",
    "        B2a = rp.adjust_band(np.clip(B2,0,np.quantile(B2,quantile)))\n",
    "        B3a = rp.adjust_band(np.clip(B3,0,np.quantile(B3,quantile)))\n",
    "        B4a = rp.adjust_band(np.clip(B4,0,np.quantile(B4,quantile)))\n",
    "\n",
    "        rgb = np.dstack((B1a,B2a,B3a))\n",
    "        rgbvegetal = np.dstack((B4a,B1a,B2a))\n",
    "\n",
    "        images[i,j] = rgb\n",
    "images =np.flip(np.transpose(images),axis=0)\n",
    "mat_list_path_image =np.flip(np.transpose(mat_list_path_image),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a3d5bd-00b9-4d52-b4f4-e9c6c5218efd",
   "metadata": {},
   "source": [
    "Et je représente tout ça dans une grille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d986b378-9a3d-48ad-8feb-40115d3f9ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure and axes\n",
    "fig, axs = plt.subplots(nrows=num_cols, ncols=num_rows, figsize=(10, 10))\n",
    "\n",
    "# Iterate over the images and plot them\n",
    "for i in range(num_cols):\n",
    "    for j in range(num_rows):\n",
    "        axs[i,j].imshow(images[i,j])\n",
    "\n",
    "# Remove any unused axes\n",
    "for i in range(num_cols):\n",
    "    for j in range(num_rows):\n",
    "        axs[i,j].set_axis_off()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fb13e-0500-4e4d-81dd-2b7e9130ceac",
   "metadata": {},
   "source": [
    "## VRAI TEST DES IMAGES PLEIADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c79b9f2-d28e-4ce4-8d37-17042685bb91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "filepath =  mat_list_path_image[9,5] \n",
    "filepath =  mat_list_path_image[7,11]\n",
    "filepath =  mat_list_path_image[2,4] # cool\n",
    "filepath =  mat_list_path_image[4,10] # bof bof\n",
    "filepath =  mat_list_path_image[9,2] # bof\n",
    "filepath =  mat_list_path_image[8,10] # bof\n",
    "filepath =  mat_list_path_image[12,5] # Ok\n",
    "filepath =  mat_list_path_image[12,4] # Ok\n",
    "\n",
    "filepath =  mat_list_path_image[4,6]  # Mon baduel !! : 5 ème ligne, 7 ème colonne !!\n",
    "# interesting# il faut compter en colonnes\n",
    "#filepath = 'Cayenne_200722/16bits/ORT_2022072050325085_U22N/ORT_2022072050325085_0353_0545_U22N_16Bits.jp2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6628777b-4ca9-44b2-962c-ffb6706b3c11",
   "metadata": {},
   "source": [
    "Lecture Image (remplacer par fonction tom Thomas ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "547ea636-9c94-4ca5-beb9-4a6b825e0250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with rasterio.open(filepath) as raster:\n",
    "    oviews = raster.overviews(1) # list of overviews from biggest to smallest\n",
    "    oview = 1 # let's look at the smallest thumbnail\n",
    "\n",
    "    # NOTE this is using a 'decimated read' (http://rasterio.readthedocs.io/en/latest/topics/resampling.html)\n",
    "    B1 = raster.read(1, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "    B2 = raster.read(2, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "    B3 = raster.read(3, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "    B4 = raster.read(4, out_shape=(1, int(raster.height // oview), int(raster.width // oview)))\n",
    "\n",
    "quantile = 0.97\n",
    "B1a = rp.adjust_band(np.clip(B1,0,np.quantile(B1,quantile))) # normalisation min-max simple nécessite un clip d'abord étant donné les valeurs extremes\n",
    "B2a = rp.adjust_band(np.clip(B2,0,np.quantile(B2,quantile)))\n",
    "B3a = rp.adjust_band(np.clip(B3,0,np.quantile(B3,quantile)))\n",
    "B4a = rp.adjust_band(np.clip(B4,0,np.quantile(B4,quantile)))\n",
    "\n",
    "rgb = np.dstack((B1a,B2a,B3a))\n",
    "rgbvegetal = np.dstack((B4a,B1a,B2a))\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(10, 10))\n",
    "#ax.imshow(rgb)\n",
    "#plt.title(\"Représentation de l'image {}\".format(filepath))\n",
    "#plt.xlabel('Pixels Colonnes')\n",
    "#plt.ylabel('Pixels Lignes')\n",
    "#plt.show()\n",
    "\n",
    "im_pleiade = Image.fromarray(np.array(rgb * 255,dtype = np.uint8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec418b2-88a8-4979-8e0f-85536bcff0f9",
   "metadata": {},
   "source": [
    "Pour le mont baduel le modèle va t'il  détecter la zone ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734d8aa-6bff-42bd-be85-553b01842cdb",
   "metadata": {},
   "source": [
    "Application du modèle sur un crop de taille 250 x 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e58e2-7c6f-4534-a754-984bed0a3754",
   "metadata": {},
   "source": [
    "La Normalisation met tout en l'air lorsque on joue avec les données pleiades --'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ff2da5b2-a8c5-4cbd-bd41-b963f981bf89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def segment_image(input_image, model):\n",
    "# Load input image\n",
    "\n",
    "model.eval()\n",
    "#input_image = rgb\n",
    "input_image =  Image.open(filepath)\n",
    "\n",
    "r, g, b, i = input_image.split()\n",
    "input_image = Image.merge(\"RGB\", (r, g, b))\n",
    "\n",
    "# Define tile size and stride\n",
    "tile_size = 250\n",
    "stride = 250\n",
    "\n",
    "# Get input image dimensions\n",
    "width, height = input_image.size\n",
    "\n",
    "# Create empty output image\n",
    "output_image = np.zeros((height, width, 3))\n",
    "\n",
    "# Slice input image into tiles, feed them into segmentation network, and combine results\n",
    "for i in range(0, height - tile_size + 1, stride):\n",
    "    for j in range(0, width - tile_size + 1, stride):\n",
    "        # Extract tile from input image\n",
    "        tile = input_image.crop((j, i, j + tile_size, i + tile_size))\n",
    "        tile = torch.tensor(np.array(tile,dtype = float),dtype = torch.float).permute(2,0,1).to(device)\n",
    "        save = tile\n",
    "        # normalisation or not normalisation ? en théorie pui mais là c'est pas top\n",
    "        #mean, std = tile.mean([1,2]), tile.std([1,2])\n",
    "        #tile = transforms.Normalize(mean, std)(tile)\n",
    "        #print(tile.shape)\n",
    "        \n",
    "        # Feed tile into segmentation network and get output\n",
    "        with torch.no_grad():\n",
    "            out = model(tile.unsqueeze(0))[\"out\"]\n",
    "\n",
    "        # Convert output to numpy array and append to output image\n",
    "        output = np.array(out.to(\"cpu\").detach()).squeeze(0)\n",
    "        output_predictions = output.argmax(0)\n",
    "        masque = output_predictions\n",
    "        show_mask = np.zeros((*masque.shape, 3))\n",
    "        show_mask[masque == 1, :] = [255,255,255]\n",
    "        output = show_mask.astype(np.uint8)\n",
    "        #output = np.transpose(output, (1, 2, 0))\n",
    "        output_image [i:i+tile_size, j:j+tile_size, :] = output\n",
    "        \n",
    "#mean, std = torch_patch.mean([1,2]), torch_patch.std([1,2])\n",
    "#torch_patch = transforms.Normalize(mean, std)(torch_patch).unsqueeze(0) #reshape(1,3,250,250)\n",
    "# je rajoute moyenne etc..\n",
    "\n",
    "\n",
    "# Display input image, tiles, and output image as a grid\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,15))\n",
    "ax[0].imshow(im_pleiade) # avec la normalisation pour l'affichage\n",
    "ax[0].set_title('Input Image')\n",
    "ax[1].imshow(output_image)\n",
    "ax[1].set_title('Output Image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ec900-7566-4a0d-9db9-32418aabc432",
   "metadata": {},
   "source": [
    "## Satistiques sur les couleurs des images\n",
    "- au programme : observation de la distribution des pixels suyrcertaines ilages pleiades et images xView\n",
    "- normalisation pour centrage réduction\n",
    "- regarderr la distribution des ratios inter pixels , les valeurs extremes etc.. faire de la stat\n",
    "- eventuellement sur plusieurs images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "db2e4579-fd96-413c-aadc-39d6c149e650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(np.array(im_pleiade)[:,:,0].ravel(),bins = 50, density = True)\n",
    "plt.hist(np.array(im_pleiade)[:,:,1].ravel(),bins = 50, density = True)\n",
    "plt.hist(np.array(im_pleiade)[:,:,2].ravel(),bins = 50, density = True)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\")\n",
    "\n",
    "plt.show()\n",
    "imxview =np.array(data_val[\"image_pillow\"][2])\n",
    "plt.hist(imxview[:,:,0].ravel(),bins = 50, density = True)\n",
    "plt.hist(imxview[:,:,1].ravel(),bins = 50, density = True)\n",
    "plt.hist(imxview[:,:,2].ravel(),bins = 50, density = True)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "c6226ccf-fa83-4a9b-8445-e14efb659fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.hist(np.array(im_pleiade).ravel(), bins=50, density=True)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.hist(np.array(imxview).ravel(), bins=50, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f0bcb2-7248-4038-beb5-feb8f30fc7e9",
   "metadata": {},
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "b58951c5-37d7-4105-aecc-ac310d565b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "- MLFlow + ligthning torch\n",
    "- Idée même si on ne détecte pas tout faire une polygonisation de nos pixels détectés et des buffers ou une approche graphe pour dessiner des zones de detection (ex 3 points =triangles)\n",
    "(travail sur les masques produits finalement)\n",
    "- il nous faudrait un cas exemple d'un teritoire sur 2 années différentes pour voir si ça marche\n",
    "- apurement du jeu de données , trop de patch sans logements.. à enlever et c'est faisable (en filtrant au préalable sur les labels)\n",
    "- petite présnetation / bilan d'étape\n",
    "- dessiner au brouillon l'architecture du projet avec les entrainemenbt avec la volonté detre agnostique au type de modèle\n",
    "- validation en utilisant les labels RIL !! (les utiliser aussi pour l'zentraînement)\n",
    "\n",
    "- transferabilité résolution / couleurs pourquoi kla normaklisation kill the game ?\n",
    "- faire un taf en 2 temps test buildoing dans la zone puis segmentation\n",
    "- to do : utiliser les bounding box pour faire autre chose que de la segmentation\n",
    "- faire un énorme schéma\n",
    "- équilibrer jeu de donnée avec les 0 et 1. prendre que des exemple ou il y a des 1\n",
    "- Rappatrier un peu de biblio suivant les différentes étapes\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "47ace637-ca7c-485d-abb6-a3dd81ac2b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "- Présentation générale\n",
    "- Algo avec BBOX\n",
    "- augmenter le nooùmbre d'images d'entraînement xview dataset\n",
    "- normalisation aps top sur Guyane, pb de transférabilité\n",
    "- Semble détecter des logements facilement quand il y 'en a beaucoup autour (transférabilité des résolutions?)\n",
    "- surcouche dans le modèle xview pour dégager lme [\"out\"] \n",
    "- mettre au propre les entraînements avec ligthning torch\n",
    "- regarder torchgeo\n",
    "- ml flow pour l'enregistrement des modèles\n",
    "- INTEGRER TOUT CA DANS LE FRAMEWORK DES GARS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
