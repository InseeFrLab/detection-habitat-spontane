{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import shutil\n",
    "import os\n",
    "import s3fs\n",
    "import fs\n",
    "from tqdm import tqdm\n",
    "import hvac\n",
    "from minio import Minio\n",
    "from utils.satellite_image import SatelliteImage\n",
    "from osgeo import gdal\n",
    "import geemap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authentification à Google Earth Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service_account = (\n",
    "#     \"slums-detection-sa@ee-insee-sentinel.iam.gserviceaccount.com\"\n",
    "# )\n",
    "# credentials = ee.ServiceAccountCredentials(\n",
    "#     service_account, \"GCP_credentials.json\"\n",
    "# )\n",
    "\n",
    "# # Initialize the library.\n",
    "# ee.Initialize(credentials)\n",
    "\n",
    "import ee\n",
    "\n",
    "# Trigger the authentication flow.\n",
    "ee.Authenticate()\n",
    "\n",
    "# Initialize the library.\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (\n",
    "        ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte(\"CLOUDY_PIXEL_PERCENTAGE\", CLOUD_FILTER))\n",
    "    )\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (\n",
    "        ee.ImageCollection(\"COPERNICUS/S2_CLOUD_PROBABILITY\")\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "    )\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
    "    return ee.ImageCollection(\n",
    "        ee.Join.saveFirst(\"s2cloudless\").apply(\n",
    "            **{\n",
    "                \"primary\": s2_sr_col,\n",
    "                \"secondary\": s2_cloudless_col,\n",
    "                \"condition\": ee.Filter.equals(\n",
    "                    **{\n",
    "                        \"leftField\": \"system:index\",\n",
    "                        \"rightField\": \"system:index\",\n",
    "                    }\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get(\"s2cloudless\")).select(\"probability\")\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename(\"clouds\")\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select(\"SCL\").neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = (\n",
    "        img.select(\"B8\")\n",
    "        .lt(NIR_DRK_THRESH * SR_BAND_SCALE)\n",
    "        .multiply(not_water)\n",
    "        .rename(\"dark_pixels\")\n",
    "    )\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(\n",
    "        ee.Number(img.get(\"MEAN_SOLAR_AZIMUTH_ANGLE\"))\n",
    "    )\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (\n",
    "        img.select(\"clouds\")\n",
    "        .directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST * 10)\n",
    "        .reproject(**{\"crs\": img.select(0).projection(), \"scale\": 100})\n",
    "        .select(\"distance\")\n",
    "        .mask()\n",
    "        .rename(\"cloud_transform\")\n",
    "    )\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename(\"shadows\")\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cld_shdw_mask(img):\n",
    "    # Add cloud component bands.\n",
    "    img_cloud = add_cloud_bands(img)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = (\n",
    "        img_cloud_shadow.select(\"clouds\")\n",
    "        .add(img_cloud_shadow.select(\"shadows\"))\n",
    "        .gt(0)\n",
    "    )\n",
    "\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (\n",
    "        is_cld_shdw.focalMin(2)\n",
    "        .focalMax(BUFFER * 2 / 20)\n",
    "        .reproject(**{\"crs\": img.select([0]).projection(), \"scale\": 20})\n",
    "        .rename(\"cloudmask\")\n",
    "    )\n",
    "\n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    return img_cloud_shadow.addBands(is_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cld_shdw_mask(img):\n",
    "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
    "    not_cld_shdw = img.select(\"cloudmask\").Not()\n",
    "\n",
    "    # Subset reflectance bands and update their masks, return the result.\n",
    "    return img.select(\"B.*\").updateMask(not_cld_shdw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Téléchargement en local puis mise en ligne de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_s2_no_cloud(\n",
    "    DOM,\n",
    "    AOIs,\n",
    "    EPSGs,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    cloud_filter,\n",
    "    cloud_prb_thresh,\n",
    "    nir_drk_thresh,\n",
    "    cld_prj_dist,\n",
    "    buffer,\n",
    "):\n",
    "        \n",
    "    AOI = ee.Geometry.BBox(**AOIs[DOM])\n",
    "    s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "    s2_sr_median = (\n",
    "        s2_sr_cld_col.map(add_cld_shdw_mask).map(apply_cld_shdw_mask).median()\n",
    "    )\n",
    "\n",
    "    fishnet = geemap.fishnet(AOI, rows=4, cols=4, delta=0.5)\n",
    "    geemap.download_ee_image_tiles(\n",
    "        s2_sr_median,\n",
    "        fishnet,\n",
    "        f'{DOM}_{start_date[0:4]}/',\n",
    "        prefix=\"data_\",\n",
    "        crs=f\"EPSG:{EPSGs[DOM]}\",\n",
    "        scale=10,\n",
    "        num_threads=50,\n",
    "    )\n",
    "\n",
    "    upload_satelliteImages(\n",
    "        f'{DOM}_{start_date[0:4]}',\n",
    "        f'projet-slums-detection/Donnees/SENTINEL2/{DOM.upper()}/TUILES_{start_date[0:4]}',\n",
    "        250)\n",
    "    \n",
    "    shutil.rmtree(f\"{DOM}_{start_date[0:4]}\",ignore_errors=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connexion à MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportToMinio(image,rpath):\n",
    "    client = hvac.Client(\n",
    "            url='https://vault.lab.sspcloud.fr', token=os.environ[\"VAULT_TOKEN\"]\n",
    "        )\n",
    "\n",
    "    secret = os.environ[\"VAULT_MOUNT\"] + os.environ[\"VAULT_TOP_DIR\"] + \"/s3\"\n",
    "    mount_point, secret_path = secret.split(\"/\", 1)\n",
    "    secret_dict = client.secrets.kv.read_secret_version(\n",
    "        path=secret_path, mount_point=mount_point\n",
    "    )\n",
    "\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = secret_dict[\"data\"][\"data\"][\n",
    "        \"ACCESS_KEY_ID\"\n",
    "    ]\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = secret_dict[\"data\"][\"data\"][\n",
    "        \"SECRET_ACCESS_KEY\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        del os.environ['AWS_SESSION_TOKEN']\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    fs = s3fs.S3FileSystem(\n",
    "        client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "        key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    )\n",
    "    \n",
    "    return fs.put(image,rpath,True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mise en ligne de données préalablement téléchargées en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_satelliteImages(\n",
    "    lpath,\n",
    "    rpath,\n",
    "    dim\n",
    "):\n",
    "    images_paths = os.listdir(lpath)\n",
    "\n",
    "    for i in range(len(images_paths)):\n",
    "        images_paths[i] = lpath+'/'+images_paths[i]\n",
    "\n",
    "    list_satelliteImages = [\n",
    "        SatelliteImage.from_raster(\n",
    "            filename,\n",
    "            dep = \"973\",\n",
    "            n_bands = 12\n",
    "        ) for filename in tqdm(images_paths)]\n",
    "\n",
    "    splitted_list_images = [im for sublist in tqdm(list_satelliteImages) for im in sublist.split(dim)]\n",
    "\n",
    "    for i in range(len(splitted_list_images)):\n",
    "        image = splitted_list_images[i]\n",
    "\n",
    "        transf = image.transform\n",
    "        in_ds = gdal.Open(images_paths[1])\n",
    "        proj = in_ds.GetProjection()\n",
    "\n",
    "        array = image.array\n",
    "\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        out_ds = driver.Create(f'image{i}.tif', array.shape[2], array.shape[1], array.shape[0], gdal.GDT_Float64)\n",
    "        out_ds.SetGeoTransform([transf[2],transf[0],transf[1],transf[5],transf[3],transf[4]])\n",
    "        out_ds.SetProjection(proj)\n",
    "\n",
    "        for j in range(array.shape[0]):\n",
    "            out_ds.GetRasterBand(j+1).WriteArray(array[j,:,:])\n",
    "\n",
    "        out_ds = None\n",
    "        \n",
    "        exportToMinio(f'image{i}.tif',rpath)\n",
    "        os.remove(f'image{i}.tif')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtres sur le téléchargement (CRS, emprise, caractéristiques du stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AOIs = {\n",
    "    \"Guadeloupe\": {\n",
    "        \"west\": -61.811124,\n",
    "        \"south\": 15.828534,\n",
    "        \"east\": -60.998518,\n",
    "        \"north\": 16.523944,\n",
    "    },\n",
    "    \"Martinique\": {\n",
    "        \"west\": -61.264617,\n",
    "        \"south\": 14.378599,\n",
    "        \"east\": -60.781573,\n",
    "        \"north\": 14.899453,\n",
    "    },\n",
    "    \"Mayotte\": {\n",
    "        \"west\": 45.013633,\n",
    "        \"south\": -13.006619,\n",
    "        \"east\": 45.308891,\n",
    "        \"north\": -12.633022,\n",
    "    },\n",
    "    \"Guyane\": {\n",
    "        \"west\": -52.883,\n",
    "        \"south\": 4.148,\n",
    "        \"east\": -51.813,\n",
    "        \"north\": 5.426\n",
    "    }\n",
    "}\n",
    "\n",
    "EPSGs = {\"Guadeloupe\": \"4559\", \"Martinique\": \"4559\", \"Mayotte\": \"4471\", \"Guyane\": \"4235\"}\n",
    "\n",
    "START_DATE = \"2022-05-01\"\n",
    "END_DATE = \"2022-09-01\"\n",
    "CLOUD_FILTER = 60\n",
    "CLD_PRB_THRESH = 40\n",
    "NIR_DRK_THRESH = 0.15\n",
    "CLD_PRJ_DIST = 2\n",
    "BUFFER = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Téléchargement en local puis mise en lignes de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_s2_no_cloud(\n",
    "#     \"Guadeloupe\",\n",
    "#     AOIs,\n",
    "#     EPSGs,\n",
    "#     START_DATE,\n",
    "#     END_DATE,\n",
    "#     CLOUD_FILTER,\n",
    "#     CLD_PRB_THRESH,\n",
    "#     NIR_DRK_THRESH,\n",
    "#     CLD_PRJ_DIST,\n",
    "#     BUFFER,\n",
    "# ) \n",
    "\n",
    "# export_s2_no_cloud(\n",
    "#     \"Martinique\",\n",
    "#     AOIs,\n",
    "#     EPSGs,\n",
    "#     START_DATE,\n",
    "#     END_DATE,\n",
    "#     CLOUD_FILTER,\n",
    "#     CLD_PRB_THRESH,\n",
    "#     NIR_DRK_THRESH,\n",
    "#     CLD_PRJ_DIST,\n",
    "#     BUFFER,\n",
    "# )\n",
    "\n",
    "# export_s2_no_cloud(\n",
    "#     \"Mayotte\",\n",
    "#     AOIs,\n",
    "#     EPSGs,\n",
    "#     START_DATE,\n",
    "#     END_DATE,\n",
    "#     CLOUD_FILTER,\n",
    "#     CLD_PRB_THRESH,\n",
    "#     NIR_DRK_THRESH,\n",
    "#     CLD_PRJ_DIST,\n",
    "#     BUFFER,\n",
    "# )\n",
    "\n",
    "# export_s2_no_cloud(\n",
    "#     \"Guyane\",\n",
    "#     AOIs,\n",
    "#     EPSGs,\n",
    "#     START_DATE,\n",
    "#     END_DATE,\n",
    "#     CLOUD_FILTER,\n",
    "#     CLD_PRB_THRESH,\n",
    "#     NIR_DRK_THRESH,\n",
    "#     CLD_PRJ_DIST,\n",
    "#     BUFFER,\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload de données déjà téléchargées pour 2021 et par DROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_satelliteImages(\n",
    "    \"Guadeloupe_2021\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/GUADELOUPE/TUILES_2021',\n",
    "    250)\n",
    "\n",
    "upload_satelliteImages(\n",
    "    \"Martinique_2021\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/MARTINIQUE/TUILES_2021',\n",
    "    250)\n",
    "\n",
    "upload_satelliteImages(\n",
    "    \"Mayotte_2021\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/MAYOTTE/TUILES_2021',\n",
    "    250)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload de données déjà téléchargées pour 2022 et par DROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_satelliteImages(\n",
    "    \"Guadeloupe_2022\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/GUADELOUPE/TUILES_2022',\n",
    "    250)\n",
    "\n",
    "upload_satelliteImages(\n",
    "    \"Martinique_2022\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/MARTINIQUE/TUILES_2022',\n",
    "    250)\n",
    "\n",
    "upload_satelliteImages(\n",
    "    \"Mayotte_2022\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/MAYOTTE/TUILES_2022',\n",
    "    250)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de train avec données Sentinel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import update_storage_access\n",
    "from datetime import datetime\n",
    "from utils.labeler import RILLabeler, BDTOPOLabeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"tile size\": 200,\n",
    "    \"source train\": \"SENTINEL2\",\n",
    "    \"type labeler\": \"RIL\",  # None if source train != PLEIADE\n",
    "    \"buffer size\": 10,  # None if BDTOPO\n",
    "    \"year\": 2022,\n",
    "    \"territory\": \"martinique\",\n",
    "    \"dep\": \"972\",\n",
    "    \"n bands\": 3,\n",
    "    \"n channels train\": 3,\n",
    "}\n",
    "\n",
    "config_train = {\n",
    "    \"lr\": 0.0001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"module\": \"deeplabv3\",\n",
    "    \"batch size\": 2,\n",
    "    \"max epochs\": 100,\n",
    "}\n",
    "\n",
    "# params\n",
    "n_channel_train = config[\"n channels train\"]\n",
    "\n",
    "tile_size = config[\"tile size\"]\n",
    "n_bands = config[\"n bands\"]\n",
    "dep = config[\"dep\"]\n",
    "territory = config[\"territory\"]\n",
    "year = config[\"year\"]\n",
    "buffer_size = config[\"buffer size\"]\n",
    "source_train = config[\"source train\"]\n",
    "type_labeler = config[\"type labeler\"]\n",
    "\n",
    "module = config_train[\"module\"]\n",
    "batch_size = config_train[\"batch size\"]\n",
    "\n",
    "train_directory_name = \"../splitted_data\"\n",
    "\n",
    "update_storage_access()\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://minio.lab.sspcloud.fr\"\n",
    "\n",
    "# DL des données du territoire dont on se sert pour l'entraînement\n",
    "# On peut faire une liste de couples années/territoire également\n",
    "# Plus tard décliner avec change detection etc..\n",
    "if type_labeler == \"RIL\":\n",
    "    date = datetime.strptime(\n",
    "        str(year).split(\"-\")[-1] + \"0101\", \"%Y%m%d\"\n",
    "    )\n",
    "    labeler = RILLabeler(date, dep=dep, buffer_size=buffer_size)\n",
    "\n",
    "if type_labeler == \"BDTOPO\":\n",
    "    date = datetime.strptime(\n",
    "        str(year).split(\"-\")[-1] + \"0101\", \"%Y%m%d\"\n",
    "    )\n",
    "    labeler = BDTOPOLabeler(date, dep=dep)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import get_root_path, get_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_storage_access()\n",
    "root_path = get_root_path()\n",
    "environment = get_environment()\n",
    "\n",
    "bucket = environment[\"bucket\"]\n",
    "path_s3 = environment[\"sources\"][\"SENTINEL2\"][year][territory]\n",
    "path_local = os.path.join(\n",
    "    root_path, environment[\"local-path\"][\"SENTINEL2\"][year][territory]\n",
    ")\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"}\n",
    ")\n",
    "print(\"download \" + territory + \" \" + str(year) + \" in \" + path_local)\n",
    "fs.download(\n",
    "    rpath=f\"{bucket}/{path_s3}\", lpath=f\"{path_local}\", recursive=True\n",
    ")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name = os.listdir(path_local)\n",
    "list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_path = [path_local + \"/\" + name for name in list_name]\n",
    "list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "environment = get_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "output_masks_path = os.path.join(\n",
    "    root_path, environment[\"local-path\"][\"SENTINEL2-LABELS\"][year][territory]\n",
    ")\n",
    "if not os.path.exists(output_masks_path):\n",
    "    os.makedirs(output_masks_path)\n",
    "for path, file_name in zip(list_path, tqdm(list_name)):  # tqdm ici\n",
    "    satellite_image = SatelliteImage.from_raster(\n",
    "        file_path=path, dep=None, date=None, n_bands=n_bands\n",
    "    )\n",
    "    mask = labeler.create_segmentation_label(satellite_image)\n",
    "    np.save(\n",
    "        output_masks_path + \"/\" + Path(file_name).stem + \".npy\", mask\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datas.components.dataset import PleiadeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = os.listdir(output_masks_path)\n",
    "images = os.listdir(path_local)\n",
    "list_path_labels = np.sort(\n",
    "    [output_masks_path + \"/\" + name for name in labels]\n",
    ")\n",
    "list_path_images = np.sort(\n",
    "    [path_local + \"/\"  + name for name in images]\n",
    ")\n",
    "\n",
    "dataset = PleiadeDataset(list_path_images, list_path_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as album\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torch\n",
    "\n",
    "dataset_test = PleiadeDataset(list_path_images[0], list_path_labels[0])\n",
    "image_size = (250, 250)\n",
    "\n",
    "transforms_augmentation = album.Compose(\n",
    "    [\n",
    "        album.Resize(300, 300, always_apply=True),\n",
    "        album.RandomResizedCrop(\n",
    "            *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "        ),\n",
    "        album.HorizontalFlip(),\n",
    "        album.VerticalFlip(),\n",
    "        album.Normalize(mean=(0.5,0.406,0.456,0.485,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5),std=(0.225,0.229,0.225,0.224,0.225,0.225,0.225,0.225,0.225,0.225,0.225,0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_preprocessing = album.Compose(\n",
    "    [\n",
    "        album.Resize(*image_size, always_apply=True),\n",
    "        album.Normalize(mean=(0.5,0.406,0.456,0.485,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5),std=(0.225,0.229,0.225,0.224,0.225,0.225,0.225,0.225,0.225,0.225,0.225,0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Instanciation modèle et paramètres d'entraînement\n",
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\n",
    "    \"lr\": config_train[\"lr\"],\n",
    "    \"momentum\": config_train[\"momentum\"],\n",
    "}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel_train = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from utils.gestion_data import instantiate_module\n",
    "from datas.datamodule import DataModule\n",
    "from models.segmentation_module import SegmentationModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "model = instantiate_module(module, n_channel_train)\n",
    "\n",
    "batch_size = 2\n",
    "data_module = DataModule(\n",
    "    dataset=dataset,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1,\n",
    "    batch_size=batch_size,\n",
    "    dataset_test=dataset_test,\n",
    ")\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"max\"\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"max\", patience=3\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy = \"auto\"\n",
    "list_callbacks = [lr_monitor, checkpoint_callback, early_stop_callback]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=list_callbacks,\n",
    "    max_epochs=config_train[\"max epochs\"],\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2,\n",
    ")\n",
    "\n",
    "trainer.fit(lightning_module, datamodule=data_module)\n",
    "trainer.test(lightning_module, datamodule=data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test more bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentinelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        list_paths_images,\n",
    "        list_paths_labels,\n",
    "        transforms = None,\n",
    "        n_bands: int = 12\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Args:\n",
    "            list_paths_images (List): list of path of the images\n",
    "            list_paths_labels (List): list of paths containing the labels\n",
    "            transforms (Compose) : list of transforms\n",
    "        \"\"\"\n",
    "        self.list_paths_images = list_paths_images\n",
    "        self.list_paths_labels = list_paths_labels\n",
    "        self.transforms = transforms\n",
    "        self.n_bands = n_bands\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            idx (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        pathim = self.list_paths_images[idx]\n",
    "        pathlabel = self.list_paths_labels[idx]\n",
    "\n",
    "        img = SatelliteImage.from_raster(\n",
    "            file_path=pathim, dep=None, date=None, n_bands=self.n_bands\n",
    "        ).array\n",
    "\n",
    "        img = np.transpose(img.astype(float), [1, 2, 0])\n",
    "        label = torch.tensor(np.load(pathlabel))\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image=img, label=label)\n",
    "            img = sample[\"image\"]\n",
    "            label = sample[\"label\"]\n",
    "        else:\n",
    "            img = torch.tensor(img.astype(float))\n",
    "            img = img.permute([2, 0, 1])\n",
    "            label = torch.tensor(label)\n",
    "\n",
    "        img = img.type(torch.float)\n",
    "        label = label.type(torch.LongTensor)\n",
    "        dic = {\"pathimage\": pathim, \"pathlabel\": pathlabel}\n",
    "        return img, label, dic\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_paths_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel_train = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = instantiate_module(module, n_channel_train)\n",
    "dataset = SentinelDataset(list_path_images, list_path_labels)\n",
    "dataset_test = SentinelDataset(list_path_images[0], list_path_labels[0])\n",
    "\n",
    "data_module = DataModule(\n",
    "    dataset=dataset,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1,\n",
    "    batch_size=batch_size,\n",
    "    dataset_test=dataset_test,\n",
    ")\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"max\"\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"max\", patience=3\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy = \"auto\"\n",
    "list_callbacks = [lr_monitor, checkpoint_callback, early_stop_callback]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=list_callbacks,\n",
    "    max_epochs=config_train[\"max epochs\"],\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2,\n",
    ")\n",
    "\n",
    "trainer.fit(lightning_module, datamodule=data_module)\n",
    "trainer.test(lightning_module, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
