{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import shutil\n",
    "import os\n",
    "import s3fs\n",
    "import fs\n",
    "from tqdm import tqdm\n",
    "import hvac\n",
    "from minio import Minio\n",
    "from utils.satellite_image import SatelliteImage\n",
    "from osgeo import gdal\n",
    "import geemap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authentification à Google Earth Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=maduFybkclFc80KEW5R60SPWkhtblt19kdB3PW_lLq0&tc=9WReAjlKgGcHWRldoHRg86NM0qhjzfGQ4RyaJ7R_aXk&cc=5V90zw6pS-yxBw0sj_YNbIV6mnIMVWICtueeH5wHbFU>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=maduFybkclFc80KEW5R60SPWkhtblt19kdB3PW_lLq0&tc=9WReAjlKgGcHWRldoHRg86NM0qhjzfGQ4RyaJ7R_aXk&cc=5V90zw6pS-yxBw0sj_YNbIV6mnIMVWICtueeH5wHbFU</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "# service_account = (\n",
    "#     \"slums-detection-sa@ee-insee-sentinel.iam.gserviceaccount.com\"\n",
    "# )\n",
    "# credentials = ee.ServiceAccountCredentials(\n",
    "#     service_account, \"GCP_credentials.json\"\n",
    "# )\n",
    "\n",
    "# # Initialize the library.\n",
    "# ee.Initialize(credentials)\n",
    "\n",
    "import ee\n",
    "\n",
    "# Trigger the authentication flow.\n",
    "ee.Authenticate()\n",
    "\n",
    "# Initialize the library.\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (\n",
    "        ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte(\"CLOUDY_PIXEL_PERCENTAGE\", CLOUD_FILTER))\n",
    "    )\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (\n",
    "        ee.ImageCollection(\"COPERNICUS/S2_CLOUD_PROBABILITY\")\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "    )\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
    "    return ee.ImageCollection(\n",
    "        ee.Join.saveFirst(\"s2cloudless\").apply(\n",
    "            **{\n",
    "                \"primary\": s2_sr_col,\n",
    "                \"secondary\": s2_cloudless_col,\n",
    "                \"condition\": ee.Filter.equals(\n",
    "                    **{\n",
    "                        \"leftField\": \"system:index\",\n",
    "                        \"rightField\": \"system:index\",\n",
    "                    }\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get(\"s2cloudless\")).select(\"probability\")\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename(\"clouds\")\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select(\"SCL\").neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = (\n",
    "        img.select(\"B8\")\n",
    "        .lt(NIR_DRK_THRESH * SR_BAND_SCALE)\n",
    "        .multiply(not_water)\n",
    "        .rename(\"dark_pixels\")\n",
    "    )\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(\n",
    "        ee.Number(img.get(\"MEAN_SOLAR_AZIMUTH_ANGLE\"))\n",
    "    )\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (\n",
    "        img.select(\"clouds\")\n",
    "        .directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST * 10)\n",
    "        .reproject(**{\"crs\": img.select(0).projection(), \"scale\": 100})\n",
    "        .select(\"distance\")\n",
    "        .mask()\n",
    "        .rename(\"cloud_transform\")\n",
    "    )\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename(\"shadows\")\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cld_shdw_mask(img):\n",
    "    # Add cloud component bands.\n",
    "    img_cloud = add_cloud_bands(img)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = (\n",
    "        img_cloud_shadow.select(\"clouds\")\n",
    "        .add(img_cloud_shadow.select(\"shadows\"))\n",
    "        .gt(0)\n",
    "    )\n",
    "\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (\n",
    "        is_cld_shdw.focalMin(2)\n",
    "        .focalMax(BUFFER * 2 / 20)\n",
    "        .reproject(**{\"crs\": img.select([0]).projection(), \"scale\": 20})\n",
    "        .rename(\"cloudmask\")\n",
    "    )\n",
    "\n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    return img_cloud_shadow.addBands(is_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cld_shdw_mask(img):\n",
    "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
    "    not_cld_shdw = img.select(\"cloudmask\").Not()\n",
    "\n",
    "    # Subset reflectance bands and update their masks, return the result.\n",
    "    return img.select(\"B.*\").updateMask(not_cld_shdw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Téléchargement en local puis mise en ligne de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_s2_no_cloud(\n",
    "    DOM,\n",
    "    AOIs,\n",
    "    EPSGs,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    cloud_filter,\n",
    "    cloud_prb_thresh,\n",
    "    nir_drk_thresh,\n",
    "    cld_prj_dist,\n",
    "    buffer,\n",
    "):\n",
    "        \n",
    "    AOI = ee.Geometry.BBox(**AOIs[DOM])\n",
    "    s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "    s2_sr_median = (\n",
    "        s2_sr_cld_col.map(add_cld_shdw_mask).map(apply_cld_shdw_mask).median()\n",
    "    )\n",
    "\n",
    "    fishnet = geemap.fishnet(AOI, rows=4, cols=4, delta=0.5)\n",
    "    geemap.download_ee_image_tiles(\n",
    "        s2_sr_median,\n",
    "        fishnet,\n",
    "        f'{DOM}_{start_date[0:4]}/',\n",
    "        prefix=\"data_\",\n",
    "        crs=f\"EPSG:{EPSGs[DOM]}\",\n",
    "        scale=10,\n",
    "        num_threads=50,\n",
    "    )\n",
    "\n",
    "    upload_satelliteImages(\n",
    "        f'{DOM}_{start_date[0:4]}',\n",
    "        f'projet-slums-detection/Donnees/SENTINEL2/{DOM.upper()}/TUILES_{start_date[0:4]}',\n",
    "        250)\n",
    "    \n",
    "    shutil.rmtree(f\"{DOM}_{start_date[0:4]}\",ignore_errors=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connexion à MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportToMinio(image,rpath):\n",
    "    client = hvac.Client(\n",
    "            url='https://vault.lab.sspcloud.fr', token=os.environ[\"VAULT_TOKEN\"]\n",
    "        )\n",
    "\n",
    "    secret = os.environ[\"VAULT_MOUNT\"] + os.environ[\"VAULT_TOP_DIR\"] + \"/s3\"\n",
    "    mount_point, secret_path = secret.split(\"/\", 1)\n",
    "    secret_dict = client.secrets.kv.read_secret_version(\n",
    "        path=secret_path, mount_point=mount_point\n",
    "    )\n",
    "\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = secret_dict[\"data\"][\"data\"][\n",
    "        \"ACCESS_KEY_ID\"\n",
    "    ]\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = secret_dict[\"data\"][\"data\"][\n",
    "        \"SECRET_ACCESS_KEY\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        del os.environ['AWS_SESSION_TOKEN']\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    fs = s3fs.S3FileSystem(\n",
    "        client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "        key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    )\n",
    "    \n",
    "    return fs.put(image,rpath,True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mise en ligne de données préalablement téléchargées en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_satelliteImages(\n",
    "    lpath,\n",
    "    rpath,\n",
    "    dim\n",
    "):\n",
    "    images_paths = os.listdir(lpath)\n",
    "\n",
    "    for i in range(len(images_paths)):\n",
    "        images_paths[i] = lpath+'/'+images_paths[i]\n",
    "\n",
    "    list_satelliteImages = [\n",
    "        SatelliteImage.from_raster(\n",
    "            filename,\n",
    "            dep = \"973\",\n",
    "            n_bands = 12\n",
    "        ) for filename in tqdm(images_paths)]\n",
    "\n",
    "    splitted_list_images = [im for sublist in tqdm(list_satelliteImages) for im in sublist.split(dim)]\n",
    "\n",
    "    for i in range(len(splitted_list_images)):\n",
    "        image = splitted_list_images[i]\n",
    "\n",
    "        transf = image.transform\n",
    "        in_ds = gdal.Open(images_paths[1])\n",
    "        proj = in_ds.GetProjection()\n",
    "\n",
    "        array = image.array\n",
    "\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        out_ds = driver.Create(f'image{i}.tif', array.shape[2], array.shape[1], array.shape[0], gdal.GDT_Float64)\n",
    "        out_ds.SetGeoTransform([transf[2],transf[0],transf[1],transf[5],transf[3],transf[4]])\n",
    "        out_ds.SetProjection(proj)\n",
    "\n",
    "        for j in range(array.shape[0]):\n",
    "            out_ds.GetRasterBand(j+1).WriteArray(array[j,:,:])\n",
    "\n",
    "        out_ds = None\n",
    "        \n",
    "        exportToMinio(f'image{i}.tif',rpath)\n",
    "        os.remove(f'image{i}.tif')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtres sur le téléchargement (CRS, emprise, caractéristiques du stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AOIs = {\n",
    "    \"Guadeloupe\": {\n",
    "        \"west\": -61.811124,\n",
    "        \"south\": 15.828534,\n",
    "        \"east\": -60.998518,\n",
    "        \"north\": 16.523944,\n",
    "    },\n",
    "    \"Martinique\": {\n",
    "        \"west\": -61.264617,\n",
    "        \"south\": 14.378599,\n",
    "        \"east\": -60.781573,\n",
    "        \"north\": 14.899453,\n",
    "    },\n",
    "    \"Mayotte\": {\n",
    "        \"west\": 45.013633,\n",
    "        \"south\": -13.006619,\n",
    "        \"east\": 45.308891,\n",
    "        \"north\": -12.633022,\n",
    "    },\n",
    "    \"Guyane\": {\n",
    "        \"west\": -52.883,\n",
    "        \"south\": 4.148,\n",
    "        \"east\": -51.813,\n",
    "        \"north\": 5.426\n",
    "    }\n",
    "}\n",
    "\n",
    "EPSGs = {\"Guadeloupe\": \"4559\", \"Martinique\": \"4559\", \"Mayotte\": \"4471\", \"Guyane\": \"4235\"}\n",
    "\n",
    "START_DATE = \"2022-05-01\"\n",
    "END_DATE = \"2022-09-01\"\n",
    "CLOUD_FILTER = 60\n",
    "CLD_PRB_THRESH = 40\n",
    "NIR_DRK_THRESH = 0.15\n",
    "CLD_PRJ_DIST = 2\n",
    "BUFFER = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Téléchargement en local puis mise en lignes de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1/4: Mayotte_2022/data_1.tif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b912cc24ff9c404ca2121808baf760df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_1.tif: |          | 0.00/73.8M (raw) [  0.0%] in 00:00 (eta:     ?)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2/4: Mayotte_2022/data_2.tif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0993ab4ed97f4f6dab48190f0291c511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_2.tif: |          | 0.00/73.8M (raw) [  0.0%] in 00:00 (eta:     ?)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 3/4: Mayotte_2022/data_3.tif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4f79fb84e3439182dd3ff325b24394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_3.tif: |          | 0.00/73.9M (raw) [  0.0%] in 00:00 (eta:     ?)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 4/4: Mayotte_2022/data_4.tif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b17deeb69e84d5d842962cb171692a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_4.tif: |          | 0.00/73.8M (raw) [  0.0%] in 00:00 (eta:     ?)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  5.34it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 6311.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# export_s2_no_cloud(\n",
    "#     \"Guadeloupe\",\n",
    "#     AOIs,\n",
    "#     EPSGs,\n",
    "#     START_DATE,\n",
    "#     END_DATE,\n",
    "#     CLOUD_FILTER,\n",
    "#     CLD_PRB_THRESH,\n",
    "#     NIR_DRK_THRESH,\n",
    "#     CLD_PRJ_DIST,\n",
    "#     BUFFER,\n",
    "# ) \n",
    "\n",
    "# export_s2_no_cloud(\n",
    "#     \"Martinique\",\n",
    "#     AOIs,\n",
    "#     EPSGs,\n",
    "#     START_DATE,\n",
    "#     END_DATE,\n",
    "#     CLOUD_FILTER,\n",
    "#     CLD_PRB_THRESH,\n",
    "#     NIR_DRK_THRESH,\n",
    "#     CLD_PRJ_DIST,\n",
    "#     BUFFER,\n",
    "# )\n",
    "\n",
    "# export_s2_no_cloud(\n",
    "#     \"Mayotte\",\n",
    "#     AOIs,\n",
    "#     EPSGs,\n",
    "#     START_DATE,\n",
    "#     END_DATE,\n",
    "#     CLOUD_FILTER,\n",
    "#     CLD_PRB_THRESH,\n",
    "#     NIR_DRK_THRESH,\n",
    "#     CLD_PRJ_DIST,\n",
    "#     BUFFER,\n",
    "# )\n",
    "\n",
    "# export_s2_no_cloud(\n",
    "#     \"Guyane\",\n",
    "#     AOIs,\n",
    "#     EPSGs,\n",
    "#     START_DATE,\n",
    "#     END_DATE,\n",
    "#     CLOUD_FILTER,\n",
    "#     CLD_PRB_THRESH,\n",
    "#     NIR_DRK_THRESH,\n",
    "#     CLD_PRJ_DIST,\n",
    "#     BUFFER,\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload de données déjà téléchargées pour 2021 et par DROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_satelliteImages(\n",
    "    \"Guadeloupe_2021\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/GUADELOUPE/TUILES_2021',\n",
    "    250)\n",
    "\n",
    "upload_satelliteImages(\n",
    "    \"Martinique_2021\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/MARTINIQUE/TUILES_2021',\n",
    "    250)\n",
    "\n",
    "upload_satelliteImages(\n",
    "    \"Mayotte_2021\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/MAYOTTE/TUILES_2021',\n",
    "    250)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload de données déjà téléchargées pour 2022 et par DROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_satelliteImages(\n",
    "    \"Guadeloupe_2022\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/GUADELOUPE/TUILES_2022',\n",
    "    250)\n",
    "\n",
    "upload_satelliteImages(\n",
    "    \"Martinique_2022\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/MARTINIQUE/TUILES_2022',\n",
    "    250)\n",
    "\n",
    "upload_satelliteImages(\n",
    "    \"Mayotte_2022\",\n",
    "    'projet-slums-detection/Donnees/SENTINEL2/MAYOTTE/TUILES_2022',\n",
    "    250)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de train avec données Sentinel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import update_storage_access\n",
    "from datetime import datetime\n",
    "from utils.labeler import RILLabeler, BDTOPOLabeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"tile size\": 200,\n",
    "    \"source train\": \"SENTINEL2\",\n",
    "    \"type labeler\": \"RIL\",  # None if source train != PLEIADE\n",
    "    \"buffer size\": 10,  # None if BDTOPO\n",
    "    \"year\": 2022,\n",
    "    \"territory\": \"martinique\",\n",
    "    \"dep\": \"972\",\n",
    "    \"n bands\": 3,\n",
    "    \"n channels train\": 3,\n",
    "}\n",
    "\n",
    "config_train = {\n",
    "    \"lr\": 0.0001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"module\": \"deeplabv3\",\n",
    "    \"batch size\": 2,\n",
    "    \"max epochs\": 100,\n",
    "}\n",
    "\n",
    "# params\n",
    "n_channel_train = config[\"n channels train\"]\n",
    "\n",
    "tile_size = config[\"tile size\"]\n",
    "n_bands = config[\"n bands\"]\n",
    "dep = config[\"dep\"]\n",
    "territory = config[\"territory\"]\n",
    "year = config[\"year\"]\n",
    "buffer_size = config[\"buffer size\"]\n",
    "source_train = config[\"source train\"]\n",
    "type_labeler = config[\"type labeler\"]\n",
    "\n",
    "module = config_train[\"module\"]\n",
    "batch_size = config_train[\"batch size\"]\n",
    "\n",
    "train_directory_name = \"../splitted_data\"\n",
    "\n",
    "update_storage_access()\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://minio.lab.sspcloud.fr\"\n",
    "\n",
    "# DL des données du territoire dont on se sert pour l'entraînement\n",
    "# On peut faire une liste de couples années/territoire également\n",
    "# Plus tard décliner avec change detection etc..\n",
    "if type_labeler == \"RIL\":\n",
    "    date = datetime.strptime(\n",
    "        str(year).split(\"-\")[-1] + \"0101\", \"%Y%m%d\"\n",
    "    )\n",
    "    labeler = RILLabeler(date, dep=dep, buffer_size=buffer_size)\n",
    "\n",
    "if type_labeler == \"BDTOPO\":\n",
    "    date = datetime.strptime(\n",
    "        str(year).split(\"-\")[-1] + \"0101\", \"%Y%m%d\"\n",
    "    )\n",
    "    labeler = BDTOPOLabeler(date, dep=dep)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import get_root_path, get_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download martinique 2022 in /home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022\n"
     ]
    }
   ],
   "source": [
    "update_storage_access()\n",
    "root_path = get_root_path()\n",
    "environment = get_environment()\n",
    "\n",
    "bucket = environment[\"bucket\"]\n",
    "path_s3 = environment[\"sources\"][\"SENTINEL2\"][year][territory]\n",
    "path_local = os.path.join(\n",
    "    root_path, environment[\"local-path\"][\"SENTINEL2\"][year][territory]\n",
    ")\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"}\n",
    ")\n",
    "print(\"download \" + territory + \" \" + str(year) + \" in \" + path_local)\n",
    "fs.download(\n",
    "    rpath=f\"{bucket}/{path_s3}\", lpath=f\"{path_local}\", recursive=True\n",
    ")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image227.tif',\n",
       " 'image433.tif',\n",
       " 'image140.tif',\n",
       " 'image312.tif',\n",
       " 'image340.tif',\n",
       " 'image164.tif',\n",
       " 'image489.tif',\n",
       " 'image260.tif',\n",
       " 'image43.tif',\n",
       " 'image515.tif',\n",
       " 'image474.tif',\n",
       " 'image100.tif',\n",
       " 'image364.tif',\n",
       " 'image475.tif',\n",
       " 'image418.tif',\n",
       " 'image208.tif',\n",
       " 'image376.tif',\n",
       " 'image335.tif',\n",
       " 'image185.tif',\n",
       " 'image46.tif',\n",
       " 'image98.tif',\n",
       " 'image160.tif',\n",
       " 'image467.tif',\n",
       " 'image544.tif',\n",
       " 'image103.tif',\n",
       " 'image388.tif',\n",
       " 'image121.tif',\n",
       " 'image19.tif',\n",
       " 'image446.tif',\n",
       " 'image555.tif',\n",
       " 'image175.tif',\n",
       " 'image210.tif',\n",
       " 'image215.tif',\n",
       " 'image152.tif',\n",
       " 'image120.tif',\n",
       " 'image292.tif',\n",
       " 'image21.tif',\n",
       " 'image302.tif',\n",
       " 'image134.tif',\n",
       " 'image416.tif',\n",
       " 'image144.tif',\n",
       " 'image69.tif',\n",
       " 'image485.tif',\n",
       " 'image266.tif',\n",
       " 'image254.tif',\n",
       " 'image567.tif',\n",
       " 'image315.tif',\n",
       " 'image6.tif',\n",
       " 'image241.tif',\n",
       " 'image181.tif',\n",
       " 'image569.tif',\n",
       " 'image347.tif',\n",
       " 'image478.tif',\n",
       " 'image311.tif',\n",
       " 'image178.tif',\n",
       " 'image37.tif',\n",
       " 'image14.tif',\n",
       " 'image307.tif',\n",
       " 'image559.tif',\n",
       " 'image59.tif',\n",
       " 'image395.tif',\n",
       " 'image525.tif',\n",
       " 'image105.tif',\n",
       " 'image36.tif',\n",
       " 'image278.tif',\n",
       " 'image182.tif',\n",
       " 'image404.tif',\n",
       " 'image165.tif',\n",
       " 'image471.tif',\n",
       " 'image508.tif',\n",
       " 'image145.tif',\n",
       " 'image325.tif',\n",
       " 'image535.tif',\n",
       " 'image438.tif',\n",
       " 'image406.tif',\n",
       " 'image533.tif',\n",
       " 'image305.tif',\n",
       " 'image171.tif',\n",
       " 'image67.tif',\n",
       " 'image77.tif',\n",
       " 'image323.tif',\n",
       " 'image558.tif',\n",
       " 'image174.tif',\n",
       " 'image70.tif',\n",
       " 'image168.tif',\n",
       " 'image40.tif',\n",
       " 'image253.tif',\n",
       " 'image63.tif',\n",
       " 'image295.tif',\n",
       " 'image106.tif',\n",
       " 'image349.tif',\n",
       " 'image491.tif',\n",
       " 'image71.tif',\n",
       " 'image427.tif',\n",
       " 'image321.tif',\n",
       " 'image403.tif',\n",
       " 'image368.tif',\n",
       " 'image244.tif',\n",
       " 'image445.tif',\n",
       " 'image432.tif',\n",
       " 'image184.tif',\n",
       " 'image391.tif',\n",
       " 'image532.tif',\n",
       " 'image286.tif',\n",
       " 'image55.tif',\n",
       " 'image186.tif',\n",
       " 'image497.tif',\n",
       " 'image293.tif',\n",
       " 'image464.tif',\n",
       " 'image282.tif',\n",
       " 'image256.tif',\n",
       " 'image341.tif',\n",
       " 'image191.tif',\n",
       " 'image357.tif',\n",
       " 'image450.tif',\n",
       " 'image17.tif',\n",
       " 'image243.tif',\n",
       " 'image9.tif',\n",
       " 'image495.tif',\n",
       " 'image437.tif',\n",
       " 'image102.tif',\n",
       " 'image198.tif',\n",
       " 'image169.tif',\n",
       " 'image530.tif',\n",
       " 'image220.tif',\n",
       " 'image232.tif',\n",
       " 'image294.tif',\n",
       " 'image510.tif',\n",
       " 'image374.tif',\n",
       " 'image380.tif',\n",
       " 'image230.tif',\n",
       " 'image91.tif',\n",
       " 'image564.tif',\n",
       " 'image209.tif',\n",
       " 'image133.tif',\n",
       " 'image289.tif',\n",
       " 'image319.tif',\n",
       " 'image24.tif',\n",
       " 'image405.tif',\n",
       " 'image469.tif',\n",
       " 'image20.tif',\n",
       " 'image326.tif',\n",
       " 'image440.tif',\n",
       " 'image50.tif',\n",
       " 'image540.tif',\n",
       " 'image417.tif',\n",
       " 'image425.tif',\n",
       " 'image385.tif',\n",
       " 'image271.tif',\n",
       " 'image151.tif',\n",
       " 'image372.tif',\n",
       " 'image324.tif',\n",
       " 'image124.tif',\n",
       " 'image136.tif',\n",
       " 'image363.tif',\n",
       " 'image367.tif',\n",
       " 'image84.tif',\n",
       " 'image226.tif',\n",
       " 'image322.tif',\n",
       " 'image337.tif',\n",
       " 'image117.tif',\n",
       " 'image351.tif',\n",
       " 'image308.tif',\n",
       " 'image18.tif',\n",
       " 'image401.tif',\n",
       " 'image477.tif',\n",
       " 'image15.tif',\n",
       " 'image93.tif',\n",
       " 'image435.tif',\n",
       " 'image413.tif',\n",
       " 'image176.tif',\n",
       " 'image424.tif',\n",
       " 'image218.tif',\n",
       " 'image187.tif',\n",
       " 'image296.tif',\n",
       " 'image44.tif',\n",
       " 'image22.tif',\n",
       " 'image451.tif',\n",
       " 'image51.tif',\n",
       " 'image263.tif',\n",
       " 'image568.tif',\n",
       " 'image487.tif',\n",
       " 'image110.tif',\n",
       " 'image47.tif',\n",
       " 'image387.tif',\n",
       " 'image65.tif',\n",
       " 'image199.tif',\n",
       " 'image273.tif',\n",
       " 'image383.tif',\n",
       " 'image269.tif',\n",
       " 'image290.tif',\n",
       " 'image343.tif',\n",
       " 'image217.tif',\n",
       " 'image541.tif',\n",
       " 'image201.tif',\n",
       " 'image402.tif',\n",
       " 'image429.tif',\n",
       " 'image62.tif',\n",
       " 'image285.tif',\n",
       " 'image113.tif',\n",
       " 'image148.tif',\n",
       " 'image83.tif',\n",
       " 'image177.tif',\n",
       " 'image229.tif',\n",
       " 'image142.tif',\n",
       " 'image207.tif',\n",
       " 'image125.tif',\n",
       " 'image502.tif',\n",
       " 'image362.tif',\n",
       " 'image86.tif',\n",
       " 'image0.tif',\n",
       " 'image330.tif',\n",
       " 'image26.tif',\n",
       " 'image414.tif',\n",
       " 'image517.tif',\n",
       " 'image57.tif',\n",
       " 'image454.tif',\n",
       " 'image147.tif',\n",
       " 'image542.tif',\n",
       " 'image45.tif',\n",
       " 'image382.tif',\n",
       " 'image107.tif',\n",
       " 'image104.tif',\n",
       " 'image41.tif',\n",
       " 'image194.tif',\n",
       " 'image439.tif',\n",
       " 'image116.tif',\n",
       " 'image115.tif',\n",
       " 'image193.tif',\n",
       " 'image270.tif',\n",
       " 'image371.tif',\n",
       " 'image462.tif',\n",
       " 'image126.tif',\n",
       " 'image534.tif',\n",
       " 'image549.tif',\n",
       " 'image158.tif',\n",
       " 'image1.tif',\n",
       " 'image173.tif',\n",
       " 'image481.tif',\n",
       " 'image359.tif',\n",
       " 'image35.tif',\n",
       " 'image249.tif',\n",
       " 'image291.tif',\n",
       " 'image561.tif',\n",
       " 'image99.tif',\n",
       " 'image520.tif',\n",
       " 'image303.tif',\n",
       " 'image300.tif',\n",
       " 'image461.tif',\n",
       " 'image225.tif',\n",
       " 'image318.tif',\n",
       " 'image54.tif',\n",
       " 'image236.tif',\n",
       " 'image419.tif',\n",
       " 'image507.tif',\n",
       " 'image195.tif',\n",
       " 'image412.tif',\n",
       " 'image483.tif',\n",
       " 'image480.tif',\n",
       " 'image131.tif',\n",
       " 'image570.tif',\n",
       " 'image346.tif',\n",
       " 'image58.tif',\n",
       " 'image331.tif',\n",
       " 'image88.tif',\n",
       " 'image490.tif',\n",
       " 'image526.tif',\n",
       " 'image89.tif',\n",
       " 'image87.tif',\n",
       " 'image12.tif',\n",
       " 'image521.tif',\n",
       " 'image188.tif',\n",
       " 'image546.tif',\n",
       " 'image212.tif',\n",
       " 'image329.tif',\n",
       " 'image16.tif',\n",
       " 'image501.tif',\n",
       " 'image500.tif',\n",
       " 'image166.tif',\n",
       " 'image219.tif',\n",
       " 'image370.tif',\n",
       " 'image344.tif',\n",
       " 'image25.tif',\n",
       " 'image258.tif',\n",
       " 'image29.tif',\n",
       " 'image316.tif',\n",
       " 'image529.tif',\n",
       " 'image159.tif',\n",
       " 'image82.tif',\n",
       " 'image170.tif',\n",
       " 'image334.tif',\n",
       " 'image95.tif',\n",
       " 'image233.tif',\n",
       " 'image468.tif',\n",
       " 'image96.tif',\n",
       " 'image338.tif',\n",
       " 'image456.tif',\n",
       " 'image255.tif',\n",
       " 'image562.tif',\n",
       " 'image513.tif',\n",
       " 'image238.tif',\n",
       " 'image72.tif',\n",
       " 'image556.tif',\n",
       " 'image488.tif',\n",
       " 'image163.tif',\n",
       " 'image287.tif',\n",
       " 'image60.tif',\n",
       " 'image281.tif',\n",
       " 'image552.tif',\n",
       " 'image251.tif',\n",
       " 'image231.tif',\n",
       " 'image410.tif',\n",
       " 'image505.tif',\n",
       " 'image205.tif',\n",
       " 'image493.tif',\n",
       " 'image336.tif',\n",
       " 'image179.tif',\n",
       " 'image394.tif',\n",
       " 'image288.tif',\n",
       " 'image252.tif',\n",
       " 'image31.tif',\n",
       " 'image247.tif',\n",
       " 'image422.tif',\n",
       " 'image68.tif',\n",
       " 'image79.tif',\n",
       " 'image304.tif',\n",
       " 'image420.tif',\n",
       " 'image261.tif',\n",
       " 'image519.tif',\n",
       " 'image411.tif',\n",
       " 'image320.tif',\n",
       " 'image443.tif',\n",
       " 'image538.tif',\n",
       " 'image197.tif',\n",
       " 'image2.tif',\n",
       " 'image61.tif',\n",
       " 'image180.tif',\n",
       " 'image339.tif',\n",
       " 'image537.tif',\n",
       " 'image8.tif',\n",
       " 'image408.tif',\n",
       " 'image356.tif',\n",
       " 'image135.tif',\n",
       " 'image265.tif',\n",
       " 'image245.tif',\n",
       " 'image366.tif',\n",
       " 'image206.tif',\n",
       " 'image30.tif',\n",
       " 'image516.tif',\n",
       " 'image407.tif',\n",
       " 'image38.tif',\n",
       " 'image146.tif',\n",
       " 'image216.tif',\n",
       " 'image64.tif',\n",
       " 'image396.tif',\n",
       " 'image237.tif',\n",
       " 'image523.tif',\n",
       " 'image496.tif',\n",
       " 'image430.tif',\n",
       " 'image73.tif',\n",
       " 'image449.tif',\n",
       " 'image299.tif',\n",
       " 'image49.tif',\n",
       " 'image386.tif',\n",
       " 'image348.tif',\n",
       " 'image13.tif',\n",
       " 'image297.tif',\n",
       " 'image524.tif',\n",
       " 'image192.tif',\n",
       " 'image400.tif',\n",
       " 'image28.tif',\n",
       " 'image503.tif',\n",
       " 'image415.tif',\n",
       " 'image196.tif',\n",
       " 'image167.tif',\n",
       " 'image224.tif',\n",
       " 'image114.tif',\n",
       " 'image66.tif',\n",
       " 'image573.tif',\n",
       " 'image355.tif',\n",
       " 'image78.tif',\n",
       " 'image277.tif',\n",
       " 'image384.tif',\n",
       " 'image398.tif',\n",
       " 'image156.tif',\n",
       " 'image509.tif',\n",
       " 'image381.tif',\n",
       " 'image399.tif',\n",
       " 'image155.tif',\n",
       " 'image75.tif',\n",
       " 'image90.tif',\n",
       " 'image455.tif',\n",
       " 'image56.tif',\n",
       " 'image332.tif',\n",
       " 'image202.tif',\n",
       " 'image235.tif',\n",
       " 'image143.tif',\n",
       " 'image409.tif',\n",
       " 'image250.tif',\n",
       " 'image81.tif',\n",
       " 'image574.tif',\n",
       " 'image94.tif',\n",
       " 'image547.tif',\n",
       " 'image448.tif',\n",
       " 'image358.tif',\n",
       " 'image172.tif',\n",
       " 'image274.tif',\n",
       " 'image342.tif',\n",
       " 'image453.tif',\n",
       " 'image543.tif',\n",
       " 'image499.tif',\n",
       " 'image528.tif',\n",
       " 'image228.tif',\n",
       " 'image48.tif',\n",
       " 'image108.tif',\n",
       " 'image132.tif',\n",
       " 'image259.tif',\n",
       " 'image473.tif',\n",
       " 'image33.tif',\n",
       " 'image85.tif',\n",
       " 'image154.tif',\n",
       " 'image34.tif',\n",
       " 'image257.tif',\n",
       " 'image436.tif',\n",
       " 'image213.tif',\n",
       " 'image150.tif',\n",
       " 'image32.tif',\n",
       " 'image204.tif',\n",
       " 'image80.tif',\n",
       " 'image309.tif',\n",
       " 'image571.tif',\n",
       " 'image262.tif',\n",
       " 'image223.tif',\n",
       " 'image3.tif',\n",
       " 'image5.tif',\n",
       " 'image279.tif',\n",
       " 'image442.tif',\n",
       " 'image109.tif',\n",
       " 'image4.tif',\n",
       " 'image390.tif',\n",
       " 'image397.tif',\n",
       " 'image111.tif',\n",
       " 'image275.tif',\n",
       " 'image328.tif',\n",
       " 'image130.tif',\n",
       " 'image511.tif',\n",
       " 'image203.tif',\n",
       " 'image101.tif',\n",
       " 'image459.tif',\n",
       " 'image129.tif',\n",
       " 'image494.tif',\n",
       " 'image200.tif',\n",
       " 'image484.tif',\n",
       " 'image572.tif',\n",
       " 'image268.tif',\n",
       " 'image434.tif',\n",
       " 'image222.tif',\n",
       " 'image512.tif',\n",
       " 'image479.tif',\n",
       " 'image557.tif',\n",
       " 'image280.tif',\n",
       " 'image214.tif',\n",
       " 'image345.tif',\n",
       " 'image441.tif',\n",
       " 'image518.tif',\n",
       " 'image313.tif',\n",
       " 'image162.tif',\n",
       " 'image7.tif',\n",
       " 'image246.tif',\n",
       " 'image52.tif',\n",
       " 'image379.tif',\n",
       " 'image457.tif',\n",
       " 'image119.tif',\n",
       " 'image327.tif',\n",
       " 'image551.tif',\n",
       " 'image23.tif',\n",
       " 'image27.tif',\n",
       " 'image353.tif',\n",
       " 'image53.tif',\n",
       " 'image566.tif',\n",
       " 'image118.tif',\n",
       " 'image554.tif',\n",
       " 'image548.tif',\n",
       " 'image421.tif',\n",
       " 'image189.tif',\n",
       " 'image563.tif',\n",
       " 'image161.tif',\n",
       " 'image377.tif',\n",
       " 'image283.tif',\n",
       " 'image310.tif',\n",
       " 'image575.tif',\n",
       " 'image482.tif',\n",
       " 'image522.tif',\n",
       " 'image11.tif',\n",
       " 'image183.tif',\n",
       " 'image458.tif',\n",
       " 'image153.tif',\n",
       " 'image486.tif',\n",
       " 'image361.tif',\n",
       " 'image536.tif',\n",
       " 'image149.tif',\n",
       " 'image539.tif',\n",
       " 'image92.tif',\n",
       " 'image460.tif',\n",
       " 'image360.tif',\n",
       " 'image531.tif',\n",
       " 'image350.tif',\n",
       " 'image298.tif',\n",
       " 'image333.tif',\n",
       " 'image128.tif',\n",
       " 'image267.tif',\n",
       " 'image264.tif',\n",
       " 'image39.tif',\n",
       " 'image10.tif',\n",
       " 'image498.tif',\n",
       " 'image423.tif',\n",
       " 'image452.tif',\n",
       " 'image354.tif',\n",
       " 'image392.tif',\n",
       " 'image190.tif',\n",
       " 'image301.tif',\n",
       " 'image284.tif',\n",
       " 'image137.tif',\n",
       " 'image138.tif',\n",
       " 'image393.tif',\n",
       " 'image97.tif',\n",
       " 'image369.tif',\n",
       " 'image127.tif',\n",
       " 'image472.tif',\n",
       " 'image431.tif',\n",
       " 'image123.tif',\n",
       " 'image560.tif',\n",
       " 'image272.tif',\n",
       " 'image240.tif',\n",
       " 'image211.tif',\n",
       " 'image466.tif',\n",
       " 'image365.tif',\n",
       " 'image504.tif',\n",
       " 'image465.tif',\n",
       " 'image527.tif',\n",
       " 'image506.tif',\n",
       " 'image122.tif',\n",
       " 'image463.tif',\n",
       " 'image378.tif',\n",
       " 'image426.tif',\n",
       " 'image550.tif',\n",
       " 'image314.tif',\n",
       " 'image76.tif',\n",
       " 'image242.tif',\n",
       " 'image375.tif',\n",
       " 'image221.tif',\n",
       " 'image276.tif',\n",
       " 'image352.tif',\n",
       " 'image428.tif',\n",
       " 'image476.tif',\n",
       " 'image74.tif',\n",
       " 'image373.tif',\n",
       " 'image42.tif',\n",
       " 'image141.tif',\n",
       " 'image565.tif',\n",
       " 'image157.tif',\n",
       " 'image389.tif',\n",
       " 'image248.tif',\n",
       " 'image447.tif',\n",
       " 'image317.tif',\n",
       " 'image239.tif',\n",
       " 'image470.tif',\n",
       " 'image553.tif',\n",
       " 'image492.tif',\n",
       " 'image514.tif',\n",
       " 'image444.tif',\n",
       " 'image545.tif',\n",
       " 'image112.tif',\n",
       " 'image306.tif',\n",
       " 'image139.tif',\n",
       " 'image234.tif']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_name = os.listdir(path_local)\n",
    "list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image227.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image433.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image140.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image312.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image340.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image164.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image489.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image260.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image43.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image515.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image474.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image100.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image364.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image475.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image418.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image208.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image376.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image335.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image185.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image46.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image98.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image160.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image467.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image544.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image103.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image388.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image121.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image19.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image446.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image555.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image175.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image210.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image215.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image152.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image120.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image292.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image21.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image302.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image134.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image416.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image144.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image69.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image485.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image266.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image254.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image567.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image315.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image6.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image241.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image181.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image569.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image347.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image478.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image311.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image178.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image37.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image14.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image307.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image559.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image59.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image395.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image525.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image105.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image36.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image278.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image182.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image404.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image165.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image471.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image508.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image145.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image325.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image535.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image438.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image406.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image533.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image305.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image171.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image67.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image77.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image323.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image558.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image174.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image70.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image168.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image40.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image253.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image63.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image295.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image106.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image349.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image491.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image71.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image427.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image321.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image403.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image368.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image244.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image445.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image432.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image184.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image391.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image532.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image286.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image55.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image186.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image497.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image293.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image464.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image282.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image256.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image341.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image191.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image357.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image450.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image17.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image243.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image9.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image495.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image437.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image102.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image198.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image169.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image530.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image220.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image232.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image294.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image510.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image374.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image380.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image230.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image91.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image564.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image209.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image133.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image289.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image319.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image24.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image405.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image469.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image20.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image326.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image440.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image50.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image540.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image417.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image425.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image385.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image271.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image151.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image372.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image324.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image124.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image136.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image363.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image367.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image84.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image226.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image322.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image337.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image117.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image351.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image308.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image18.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image401.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image477.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image15.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image93.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image435.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image413.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image176.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image424.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image218.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image187.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image296.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image44.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image22.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image451.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image51.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image263.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image568.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image487.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image110.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image47.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image387.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image65.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image199.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image273.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image383.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image269.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image290.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image343.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image217.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image541.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image201.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image402.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image429.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image62.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image285.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image113.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image148.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image83.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image177.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image229.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image142.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image207.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image125.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image502.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image362.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image86.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image0.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image330.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image26.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image414.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image517.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image57.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image454.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image147.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image542.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image45.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image382.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image107.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image104.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image41.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image194.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image439.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image116.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image115.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image193.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image270.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image371.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image462.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image126.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image534.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image549.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image158.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image1.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image173.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image481.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image359.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image35.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image249.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image291.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image561.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image99.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image520.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image303.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image300.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image461.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image225.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image318.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image54.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image236.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image419.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image507.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image195.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image412.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image483.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image480.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image131.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image570.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image346.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image58.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image331.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image88.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image490.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image526.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image89.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image87.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image12.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image521.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image188.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image546.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image212.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image329.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image16.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image501.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image500.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image166.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image219.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image370.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image344.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image25.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image258.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image29.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image316.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image529.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image159.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image82.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image170.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image334.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image95.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image233.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image468.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image96.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image338.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image456.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image255.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image562.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image513.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image238.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image72.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image556.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image488.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image163.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image287.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image60.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image281.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image552.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image251.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image231.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image410.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image505.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image205.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image493.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image336.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image179.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image394.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image288.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image252.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image31.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image247.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image422.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image68.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image79.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image304.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image420.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image261.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image519.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image411.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image320.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image443.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image538.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image197.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image2.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image61.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image180.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image339.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image537.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image8.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image408.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image356.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image135.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image265.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image245.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image366.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image206.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image30.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image516.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image407.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image38.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image146.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image216.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image64.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image396.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image237.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image523.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image496.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image430.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image73.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image449.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image299.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image49.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image386.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image348.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image13.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image297.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image524.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image192.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image400.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image28.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image503.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image415.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image196.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image167.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image224.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image114.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image66.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image573.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image355.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image78.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image277.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image384.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image398.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image156.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image509.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image381.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image399.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image155.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image75.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image90.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image455.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image56.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image332.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image202.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image235.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image143.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image409.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image250.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image81.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image574.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image94.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image547.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image448.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image358.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image172.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image274.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image342.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image453.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image543.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image499.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image528.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image228.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image48.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image108.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image132.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image259.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image473.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image33.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image85.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image154.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image34.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image257.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image436.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image213.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image150.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image32.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image204.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image80.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image309.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image571.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image262.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image223.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image3.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image5.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image279.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image442.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image109.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image4.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image390.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image397.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image111.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image275.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image328.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image130.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image511.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image203.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image101.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image459.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image129.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image494.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image200.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image484.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image572.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image268.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image434.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image222.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image512.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image479.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image557.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image280.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image214.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image345.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image441.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image518.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image313.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image162.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image7.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image246.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image52.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image379.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image457.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image119.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image327.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image551.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image23.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image27.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image353.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image53.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image566.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image118.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image554.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image548.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image421.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image189.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image563.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image161.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image377.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image283.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image310.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image575.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image482.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image522.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image11.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image183.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image458.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image153.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image486.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image361.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image536.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image149.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image539.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image92.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image460.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image360.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image531.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image350.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image298.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image333.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image128.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image267.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image264.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image39.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image10.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image498.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image423.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image452.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image354.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image392.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image190.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image301.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image284.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image137.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image138.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image393.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image97.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image369.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image127.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image472.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image431.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image123.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image560.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image272.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image240.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image211.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image466.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image365.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image504.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image465.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image527.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image506.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image122.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image463.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image378.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image426.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image550.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image314.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image76.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image242.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image375.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image221.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image276.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image352.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image428.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image476.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image74.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image373.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image42.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image141.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image565.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image157.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image389.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image248.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image447.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image317.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image239.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image470.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image553.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image492.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image514.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image444.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image545.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image112.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image306.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image139.tif',\n",
       " '/home/onyxia/work/detection-bidonvilles/notebooks/../data/SENTINEL2/MARTINIQUE/TUILES_2022/image234.tif']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_path = [path_local + \"/\" + name for name in list_name]\n",
    "list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "environment = get_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 575/576 [00:43<00:00, 13.31it/s]\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "output_masks_path = os.path.join(\n",
    "    root_path, environment[\"local-path\"][\"SENTINEL2-LABELS\"][year][territory]\n",
    ")\n",
    "if not os.path.exists(output_masks_path):\n",
    "    os.makedirs(output_masks_path)\n",
    "for path, file_name in zip(list_path, tqdm(list_name)):  # tqdm ici\n",
    "    satellite_image = SatelliteImage.from_raster(\n",
    "        file_path=path, dep=None, date=None, n_bands=n_bands\n",
    "    )\n",
    "    mask = labeler.create_segmentation_label(satellite_image)\n",
    "    np.save(\n",
    "        output_masks_path + \"/\" + Path(file_name).stem + \".npy\", mask\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datas.components.dataset import PleiadeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = os.listdir(output_masks_path)\n",
    "images = os.listdir(path_local)\n",
    "list_path_labels = np.sort(\n",
    "    [output_masks_path + \"/\" + name for name in labels]\n",
    ")\n",
    "list_path_images = np.sort(\n",
    "    [path_local + \"/\"  + name for name in images]\n",
    ")\n",
    "\n",
    "dataset = PleiadeDataset(list_path_images, list_path_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as album\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torch\n",
    "\n",
    "dataset_test = PleiadeDataset(list_path_images[0], list_path_labels[0])\n",
    "image_size = (250, 250)\n",
    "\n",
    "transforms_augmentation = album.Compose(\n",
    "    [\n",
    "        album.Resize(300, 300, always_apply=True),\n",
    "        album.RandomResizedCrop(\n",
    "            *image_size, scale=(0.7, 1.0), ratio=(0.7, 1)\n",
    "        ),\n",
    "        album.HorizontalFlip(),\n",
    "        album.VerticalFlip(),\n",
    "        album.Normalize(mean=(0.5,0.406,0.456,0.485,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5),std=(0.225,0.229,0.225,0.224,0.225,0.225,0.225,0.225,0.225,0.225,0.225,0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_preprocessing = album.Compose(\n",
    "    [\n",
    "        album.Resize(*image_size, always_apply=True),\n",
    "        album.Normalize(mean=(0.5,0.406,0.456,0.485,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5),std=(0.225,0.229,0.225,0.224,0.225,0.225,0.225,0.225,0.225,0.225,0.225,0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Instanciation modèle et paramètres d'entraînement\n",
    "optimizer = torch.optim.SGD\n",
    "optimizer_params = {\n",
    "    \"lr\": config_train[\"lr\"],\n",
    "    \"momentum\": config_train[\"momentum\"],\n",
    "}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {}\n",
    "scheduler_interval = \"epoch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel_train = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | DeepLabv3Module  | 61.0 M\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "61.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "61.0 M    Total params\n",
      "243.965   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94345713c3544f6a19bbdb3b650749c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f24083d2f484aa3ab71c784d7452e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519ed1d13aa148a7bec8bd1008922deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RasterioIOError",
     "evalue": "Caught RasterioIOError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 308, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 219, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: '/' not recognized as a supported file format.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/onyxia/work/detection-bidonvilles/notebooks/../src/datas/components/dataset.py\", line 114, in __getitem__\n    img = SatelliteImage.from_raster(\n  File \"/home/onyxia/work/detection-bidonvilles/notebooks/../src/utils/satellite_image.py\", line 180, in from_raster\n    with rasterio.open(file_path) as raster:\n  File \"/opt/mamba/lib/python3.10/site-packages/rasterio/env.py\", line 451, in wrapper\n    return f(*args, **kwds)\n  File \"/opt/mamba/lib/python3.10/site-packages/rasterio/__init__.py\", line 304, in open\n    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: '/' not recognized as a supported file format.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 56\u001b[0m\n\u001b[1;32m     47\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     48\u001b[0m     callbacks\u001b[39m=\u001b[39mlist_callbacks,\n\u001b[1;32m     49\u001b[0m     max_epochs\u001b[39m=\u001b[39mconfig_train[\u001b[39m\"\u001b[39m\u001b[39mmax epochs\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     log_every_n_steps\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     55\u001b[0m trainer\u001b[39m.\u001b[39mfit(lightning_module, datamodule\u001b[39m=\u001b[39mdata_module)\n\u001b[0;32m---> 56\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(lightning_module, datamodule\u001b[39m=\u001b[39;49mdata_module)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:706\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    704\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    705\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 706\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    707\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[1;32m    708\u001b[0m )\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:749\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, test_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[1;32m    746\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    747\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    748\u001b[0m )\n\u001b[0;32m--> 749\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    750\u001b[0m \u001b[39m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    751\u001b[0m results \u001b[39m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:971\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39m\"\u001b[39m\u001b[39mrun-stage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    970\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m    973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:108\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m         batch, batch_idx, dataloader_idx \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    109\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    110\u001b[0m         \u001b[39mif\u001b[39;00m previous_dataloader_idx \u001b[39m!=\u001b[39m dataloader_idx:\n\u001b[1;32m    111\u001b[0m             \u001b[39m# the dataloader has changed, notify the logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:136\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    134\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    137\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:150\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_profiler()\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    151\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:276\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    275\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[1;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    278\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:122\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterators[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_idx])\n\u001b[1;32m    123\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx\n\u001b[1;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: Caught RasterioIOError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 308, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 219, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: '/' not recognized as a supported file format.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/onyxia/work/detection-bidonvilles/notebooks/../src/datas/components/dataset.py\", line 114, in __getitem__\n    img = SatelliteImage.from_raster(\n  File \"/home/onyxia/work/detection-bidonvilles/notebooks/../src/utils/satellite_image.py\", line 180, in from_raster\n    with rasterio.open(file_path) as raster:\n  File \"/opt/mamba/lib/python3.10/site-packages/rasterio/env.py\", line 451, in wrapper\n    return f(*args, **kwds)\n  File \"/opt/mamba/lib/python3.10/site-packages/rasterio/__init__.py\", line 304, in open\n    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: '/' not recognized as a supported file format.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from utils.gestion_data import instantiate_module\n",
    "from datas.datamodule import DataModule\n",
    "from models.segmentation_module import SegmentationModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "model = instantiate_module(module, n_channel_train)\n",
    "\n",
    "batch_size = 2\n",
    "data_module = DataModule(\n",
    "    dataset=dataset,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1,\n",
    "    batch_size=batch_size,\n",
    "    dataset_test=dataset_test,\n",
    ")\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"max\"\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"max\", patience=3\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy = \"auto\"\n",
    "list_callbacks = [lr_monitor, checkpoint_callback, early_stop_callback]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=list_callbacks,\n",
    "    max_epochs=config_train[\"max epochs\"],\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2,\n",
    ")\n",
    "\n",
    "trainer.fit(lightning_module, datamodule=data_module)\n",
    "trainer.test(lightning_module, datamodule=data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test more bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentinelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        list_paths_images,\n",
    "        list_paths_labels,\n",
    "        transforms = None,\n",
    "        n_bands: int = 12\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Args:\n",
    "            list_paths_images (List): list of path of the images\n",
    "            list_paths_labels (List): list of paths containing the labels\n",
    "            transforms (Compose) : list of transforms\n",
    "        \"\"\"\n",
    "        self.list_paths_images = list_paths_images\n",
    "        self.list_paths_labels = list_paths_labels\n",
    "        self.transforms = transforms\n",
    "        self.n_bands = n_bands\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            idx (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        pathim = self.list_paths_images[idx]\n",
    "        pathlabel = self.list_paths_labels[idx]\n",
    "\n",
    "        img = SatelliteImage.from_raster(\n",
    "            file_path=pathim, dep=None, date=None, n_bands=self.n_bands\n",
    "        ).array\n",
    "\n",
    "        img = np.transpose(img.astype(float), [1, 2, 0])\n",
    "        label = torch.tensor(np.load(pathlabel))\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image=img, label=label)\n",
    "            img = sample[\"image\"]\n",
    "            label = sample[\"label\"]\n",
    "        else:\n",
    "            img = torch.tensor(img.astype(float))\n",
    "            img = img.permute([2, 0, 1])\n",
    "            label = torch.tensor(label)\n",
    "\n",
    "        img = img.type(torch.float)\n",
    "        label = label.type(torch.LongTensor)\n",
    "        dic = {\"pathimage\": pathim, \"pathlabel\": pathlabel}\n",
    "        return img, label, dic\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_paths_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel_train = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | DeepLabv3Module  | 61.0 M\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "61.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "61.0 M    Total params\n",
      "244.078   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12355d84dda47c2b2d7d627be521d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835f20c288d74b8f95b971da26db3575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 46\u001b[0m\n\u001b[1;32m     36\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m     38\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     39\u001b[0m     callbacks\u001b[39m=\u001b[39mlist_callbacks,\n\u001b[1;32m     40\u001b[0m     max_epochs\u001b[39m=\u001b[39mconfig_train[\u001b[39m\"\u001b[39m\u001b[39mmax epochs\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     log_every_n_steps\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(lightning_module, datamodule\u001b[39m=\u001b[39;49mdata_module)\n\u001b[1;32m     47\u001b[0m trainer\u001b[39m.\u001b[39mtest(lightning_module, datamodule\u001b[39m=\u001b[39mdata_module)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    555\u001b[0m     ckpt_path,\n\u001b[1;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m )\n\u001b[0;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:978\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m    977\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m--> 978\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    979\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    353\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 354\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[1;32m    134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:218\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    217\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], kwargs)\n\u001b[1;32m    219\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:185\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         closure()\n\u001b[1;32m    180\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    187\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:261\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    260\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    262\u001b[0m     trainer,\n\u001b[1;32m    263\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    264\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    265\u001b[0m     batch_idx,\n\u001b[1;32m    266\u001b[0m     optimizer,\n\u001b[1;32m    267\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    268\u001b[0m )\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:142\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    145\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1265\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1232\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1233\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[39m    the optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[39m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1265\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:158\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:224\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 224\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(optimizer, model\u001b[39m=\u001b[39;49mmodel, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:114\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/optim/sgd.py:67\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m---> 67\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m     70\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:101\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     90\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     91\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     93\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     94\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     95\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:308\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m    307\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n\u001b[1;32m    311\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    290\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    291\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:366\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[1;32m    365\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> 366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/work/detection-bidonvilles/notebooks/../src/models/segmentation_module.py:74\u001b[0m, in \u001b[0;36mSegmentationModule.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mTraining step.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mReturns: Tensor\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m images, labels, dic \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> 74\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(images)\n\u001b[1;32m     75\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(output, labels)\n\u001b[1;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m, loss, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/work/detection-bidonvilles/notebooks/../src/models/segmentation_module.py:62\u001b[0m, in \u001b[0;36mSegmentationModule.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m     56\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m    Perform forward-pass.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m        batch (tensor): Batch of images to perform forward-pass.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m    Returns (Tuple[tensor, tensor]): Table, Column prediction.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/detection-bidonvilles/notebooks/../src/models/components/segmentation_models.py:34\u001b[0m, in \u001b[0;36mDeepLabv3Module.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     33\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" \"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torchvision/models/segmentation/_utils.py:27\u001b[0m, in \u001b[0;36m_SimpleSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m result \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m     26\u001b[0m x \u001b[39m=\u001b[39m features[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(x)\n\u001b[1;32m     28\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(x, size\u001b[39m=\u001b[39minput_shape, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m x\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torchvision/models/segmentation/deeplabv3.py:111\u001b[0m, in \u001b[0;36mASPP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m _res \u001b[39m=\u001b[39m []\n\u001b[1;32m    110\u001b[0m \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs:\n\u001b[0;32m--> 111\u001b[0m     _res\u001b[39m.\u001b[39mappend(conv(x))\n\u001b[1;32m    112\u001b[0m res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(_res, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproject(res)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torchvision/models/segmentation/deeplabv3.py:81\u001b[0m, in \u001b[0;36mASPPPooling.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m size \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[1;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     x \u001b[39m=\u001b[39m mod(x)\n\u001b[1;32m     82\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39minterpolate(x, size\u001b[39m=\u001b[39msize, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[39minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m-> 2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m   2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/torch/nn/functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     size_prods \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m size[i \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m   2415\u001b[0m \u001b[39mif\u001b[39;00m size_prods \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])"
     ]
    }
   ],
   "source": [
    "model = instantiate_module(module, n_channel_train)\n",
    "dataset = SentinelDataset(list_path_images, list_path_labels)\n",
    "dataset_test = SentinelDataset(list_path_images[0], list_path_labels[0])\n",
    "\n",
    "data_module = DataModule(\n",
    "    dataset=dataset,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    num_workers=1,\n",
    "    batch_size=batch_size,\n",
    "    dataset_test=dataset_test,\n",
    ")\n",
    "\n",
    "lightning_module = SegmentationModule(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=scheduler_interval,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation_loss\", save_top_k=1, save_last=True, mode=\"max\"\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", mode=\"max\", patience=3\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "strategy = \"auto\"\n",
    "list_callbacks = [lr_monitor, checkpoint_callback, early_stop_callback]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=list_callbacks,\n",
    "    max_epochs=config_train[\"max epochs\"],\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2,\n",
    ")\n",
    "\n",
    "trainer.fit(lightning_module, datamodule=data_module)\n",
    "trainer.test(lightning_module, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
